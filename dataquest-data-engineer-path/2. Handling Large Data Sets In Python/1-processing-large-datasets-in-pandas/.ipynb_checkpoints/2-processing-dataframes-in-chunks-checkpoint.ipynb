{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataframes in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Chunks\n",
    "\n",
    "In the last mission, we explored how to reduce a dataframe's memory footprint by selecting the correct column types. \n",
    "\n",
    "### We need a different strategy for working with data sets that don't fit into memory even after we've optimized types and filtered columns. \n",
    "Instead of trying to load the full data set into memory, we can load and process it in **chunks**. Here's what that workflow looks like:\n",
    "\n",
    "![processing-chunks](https://s3.amazonaws.com/dq-content/processing_chunks_overview.png)\n",
    "\n",
    "At any given time, only a fraction of the total rows are actually available in memory. Soon after we read the next chunk into memory, Python's garbage collection process removes the old chunk from memory. The downside is that we have to write code that can work with just a portion of the data, store the intermediate results, and combine them at the end. While this is straightforward for simple tasks, it can be cumbersome for complex ones. In this mission, we'll explore how to break common tasks down so we can operate on chunks of the data set instead.<br>\n",
    "\n",
    "The `pandas.read_csv()` function makes this workflow easier for us. It has a `chunksize` parameter that we can use to specify the number of rows we want each dataframe chunk to contain.\n",
    "\n",
    "```python\n",
    ">> chunk_iter = pd.read_csv(\"data.csv\", chunksize=10000)\n",
    ">> type(chunk_iter)\n",
    "pandas.io.parsers.TextFileReadert\n",
    "```\n",
    "\n",
    "This function returns an iterator object, which we can use within a loop to process each dataframe chunk as its own dataframe. Each iteration of the loop reads `10000` rows into a dataframe, and assigns them to the iterator variable `chunk`.\n",
    "\n",
    "```python\n",
    ">> for chunk in chunk_iter:\n",
    "..    print(len(chunk))\n",
    "1000\n",
    "1000\n",
    "1000\n",
    "1000\n",
    "...\n",
    "```\n",
    "\n",
    "If you'll recall from an earlier mission, the entire data set consumes approximately 45 megabytes of memory by default. If we only have 1 megabyte of memory available, how do we make sure that each dataframe chunk consumes less than that amount so we don't run out of memory? Unfortunately, there's no foolproof way to make sure each chunk is below a specific memory threshold. The best way is to try a small number of rows to start, calculate each dataframe chunk's memory footprint, and then increase the number of rows until you're below 50% of your threshold (just to be safe).<br>\n",
    "\n",
    "Given that the data set's 34,558 rows consume 45 megabytes of memory, we can expect chunks of `10000` rows to consume around 13 megabytes (10000 rows divided by 34558 total rows, multiplied by 45 megabytes). This means that even using `1000` row chunks is probably too high because each one will consume around 1.3 megabytes of memory (13 megabytes divided by a ratio of 10). Given that we want each chunk to consume less than 50% of our total available memory (which is 1 megabyte for this exercise), we can expect each chunk to consume around 0.3 megabytes of memory (1.3 megabytes divided by a ratio of 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an iterator object that reads in `250`-row chunks from `\"moma.csv\"`.\n",
    "* For each chunk, retrieve the memory footprint in megabytes and append it to the list `memory_footprints`.\n",
    "* Generate and display a histogram of the values in `memory_footprints` using `pyplot.hist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "memory_footprints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   7.,  94.,  37.]),\n",
       " array([ 0.07383919,  0.1022049 ,  0.1305706 ,  0.15893631,  0.18730202,\n",
       "         0.21566772,  0.24403343,  0.27239914,  0.30076485,  0.32913055,\n",
       "         0.35749626]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('../data/moma.csv', chunksize=250)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    mem_fp = chunk.memory_usage(deep=True).sum()/2**20\n",
    "    memory_footprints.append(mem_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149f1828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(memory_footprints);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Across Chunks\n",
    "\n",
    "It looks like all of the dataframe chunks consumed less than 0.35 megabytes of memory, which was close to what we expected. Now that we know how to select the correct chunk size, let's move on to actually breaking down a simple problem. If we wanted to calculate the total number of rows in a single dataframe, we could use the `len()` function. To do this for many dataframe chunks, we'd need to calculate the number of rows in each chunk and add that value to a counter variable. This workflow should be familiar to you because we work with lists in the same way.<br>\n",
    "\n",
    "While this exercise may seem simple, it will help you start thinking in terms of working with chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an iterator object that reads in `250`-row chunks from `\"moma.csv\"`.\n",
    "* For each chunk, retrieve the number of rows and add it to `num_rows`.\n",
    "* Display `num_rows` using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34558\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv(\"../data/moma.csv\", chunksize=250)\n",
    "num_rows = 0\n",
    "for chunk in chunk_iter:\n",
    "    num_rows += len(chunk)\n",
    "    \n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Instead of using the `len()` function once to retrieve the number of rows in the entire data set, we had to retrieve each dataframe chunk's row length and sum them. Breaking a task down, processing the different parts separately, and combining them later on is an important workflow in batch processing. While we aren't taking advantage of the full potential of **[batch processing](https://en.wikipedia.org/wiki/Batch_processing)** when we're processing dataframe chunks (by having many computers work on parts of a task in parallel), we still have to break the task down into smaller ones in the same way.<br>\n",
    "\n",
    "On the last screen, we updated a single value (the `num_rows` integer variable) by adding the number of rows in each chunk.\n",
    "\n",
    "![batch-processing](https://s3.amazonaws.com/dq-content/process_chunks_count.png)\n",
    "\n",
    "Let's look at an example where each chunk generates a collection of values that we need to combine. To generate a distribution of the lifespans of the people involved in the art exhibitions, we can subtract the values in the `ConstituentBeginDate` column from those in the `ConstituentEndDate` column. Subtracting these columns returns a series object for each dataframe chunk.<br>\n",
    "\n",
    "While we could create a separate list and append the values in each series object to that list, we start to **lose out on the performance advantage pandas provides by converting back to native Python objects**. In addition, each time we append many values to a native Python list, Python has to reallocate memory for that list, which consumes CPU time. \n",
    "\n",
    "### Whenever possible, we want to work with pandas objects so we can take advantage of the optimizations under the hood to conserve both CPU and memory.\n",
    "\n",
    "The preferred way to do this in pandas is **to append each series object we want to combine together to a list, then combine them at the end using the [`pandas.concat()` function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)**.\n",
    "\n",
    "```python\n",
    ">> series_list = [pd.Series([1,2]), pd.Series([2,3])]\n",
    ">> pd.concat(series_list)\n",
    "0    1\n",
    "1    2\n",
    "0    2\n",
    "1    3\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "You'll notice that the result preserves the original index values. Thankfully, each value in each of the series objects we'll be generating will have its own unique index value (the row index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a distribution of the lifespans of the constituents (in years).\n",
    "* Create a list object named `lifespans`. We'll append each iteration's series object to this list.\n",
    "* Create an iterator that reads in `250` rows at a time from `\"moma.csv\"`. Specify that we want to cast the `ConstituentBeginDate` and `ConstituentEndDate` columns as float types.\n",
    "* For each chunk, return a series object that represents the lifespans of the constituents, and append this series object to the `lifespans` list.\n",
    "* After you've processed all the chunks, combine the series objects in `lifespans` and assign the resulting series object to `lifespans_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespans = []\n",
    "dtypes = {\n",
    "    'ConstituentBeginDate':'float',\n",
    "    'ConstituentEndDate':'float'\n",
    "}\n",
    "\n",
    "chunk_iter = pd.read_csv('../data/moma.csv', chunksize=250,\n",
    "                        dtype=dtypes)\n",
    "for chunk in chunk_iter:\n",
    "    cons_lifespan = chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate']\n",
    "    lifespans.append(cons_lifespan)\n",
    "    \n",
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "In the last step, each dataframe chunk contained all of the rows, even though we were only using two columns to generate the distribution of constituent lifespans. If we know that we're only going to be using those two columns, **will we observe any performance benefits by reading in only those two columns for each dataframe chunk?**<br>\n",
    "\n",
    "First, let's benchmark how long our implementation from the previous screen takes. To save space, we've rewritten the solution code.\n",
    "\n",
    "```python\n",
    "%%timeit\n",
    "lifespans = []\n",
    "chunk_iter = pd.read_csv(\"moma.csv\", chunksize=250, dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"})\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "lifespans_dist = pd.concat(lifespans)\n",
    "```\n",
    "\n",
    "The output of this benchmark was `1 loop, best of 3: 675 ms per loop`. Note that this can vary greatly depending on the hardware we're using. Now let's compare this to:\n",
    "\n",
    "```python\n",
    "%%timeit\n",
    "lifespans = []\n",
    "chunk_iter = pd.read_csv(\"moma.csv\", chunksize=250, dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"},  usecols=['ConstituentBeginDate', 'ConstituentEndDate'])\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "lifespans_dist = pd.concat(lifespans)\n",
    "```\n",
    "\n",
    "The output of this benchmark was `10 loops, best of 3: 225 ms per loop`. By loading in only the columns we needed, we were able to cut down the overall running time by about two-thirds. We can also increase the chunk size and still keep each chunk's memory footprint below our threshold.<br>\n",
    "\n",
    "Now it's your turn! Experiment with running different benchmarks by varying the chunk size and filtering which columns you read in. When you're done, navigate to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925 ms ± 68.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lifespans = []\n",
    "chunk_iter = pd.read_csv(\"../data/moma.csv\", chunksize=250, \n",
    "                         dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"})\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 ms ± 12.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lifespans = []\n",
    "chunk_iter = pd.read_csv(\"../data/moma.csv\", chunksize=250, \n",
    "                         dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"},  \n",
    "                         usecols=['ConstituentBeginDate', 'ConstituentEndDate'])\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Unique Values\n",
    "\n",
    "If we want to understand the overall gender distribution of the people involved in all of the exhibitions in the data set, we'd typically use the `Series.value_counts()` method on the `Gender` column. When working in batches, however, we need to calculate the unique value counts for each chunk and combine them at the end. Here's what that workflow looks like:\n",
    "\n",
    "![chunking-and-combine-workflow](https://s3.amazonaws.com/dq-content/processing_chunks_value_counts.png)\n",
    "\n",
    "We can use the `pandas.concat()` function to combine all of the chunks at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate the unique value counts for the `Gender` column for each dataframe chunk of `250` rows.\n",
    "* Append each resulting series object to the list `overall_vc`.\n",
    "* Use the `pandas.concat()` function to combine all of the series objects in overall_vc into a single series object, and assign the result to `combined_vc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_vc = []\n",
    "chunk_iter = pd.read_csv('../data/moma.csv',\n",
    "                         chunksize=250,\n",
    "                        usecols = ['Gender'])\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    overall_vc.append(chunk['Gender'].value_counts())\n",
    "    \n",
    "combined_vc = pd.concat(overall_vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male      208\n",
       "Female      9\n",
       "Male      193\n",
       "Female      6\n",
       "Male      195\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female',\n",
       "       'Male', 'Female',\n",
       "       ...\n",
       "       'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male',\n",
       "       'Female', 'Male'],\n",
       "      dtype='object', length=278)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Chunks Using GroupBy\n",
    "\n",
    "While we concatenated all of the series objects into a single series object, it **contains duplicate index values**. That's because the `pandas.concat()` function doesn't combine values with the same index. What we really want to do is **aggregate all of the values associated with each index**:\n",
    "\n",
    "```python\n",
    "Female     2527\n",
    "Male      23268\n",
    "male          1\n",
    "```\n",
    "\n",
    "To sum all of the values associated with an index, we first need to group the values associated with the same index together. We can accomplish this using the [`Series.groupby()` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.groupby.html). Calling this method will return a `pandas.GroupBy` object and display the reference only. We can still observe the groupings by iterating over the object like we would a dictionary and using the [`GroupBy.get_group()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.get_group.html) method.\n",
    "\n",
    "```python\n",
    ">> s4 = s3.groupby(s3.index)\n",
    ">> s4\n",
    "pandas.core.groupby.SeriesGroupBy object at 0x10c9cefd0\n",
    ">> for key, item in s4:\n",
    "..    print(a.get_group(key))\n",
    "\n",
    "Female    2\n",
    "Female    1\n",
    "dtype: int64\n",
    "\n",
    "Male    1\n",
    "Male    2\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "Once we've grouped the values by their corresponding indexes, we can call the `Series.sum()` method to return the final, unique value counts.\n",
    "\n",
    "```python\n",
    ">> s4.sum()\n",
    "Female    3\n",
    "Male      3\n",
    "dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate the unique value counts for the `Gender` column for each dataframe chunk of `250` rows.\n",
    "* Append each resulting series object to the list `overall_vc`.\n",
    "* Use the `pandas.concat()` function to combine all of the series objects in `overall_vc` into a single series object, and assign the result to `combined_vc`.\n",
    "* Use `Series.groupby()` and `Series.sum()` to aggregate the values in `combined_vc` by the unique index values. Assign the resulting series object to `final_vc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv('../data/moma.csv',\n",
    "                        chunksize=250,\n",
    "                        usecols=['Gender'])\n",
    "overall_vc = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    overall_vc.append(chunk['Gender'].value_counts())\n",
    "\n",
    "combined_vc = pd.concat(overall_vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vc = combined_vc.groupby(combined_vc.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female     2527\n",
       "Male      23268\n",
       "male          1\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Intermediate Dataframes\n",
    "\n",
    "Let's explore a slightly more complex example. Now we want to understand how the gender distribution changed with each exhibition, but we have memory constraints relative to the full data set. There are two ways we can approach this problem.<br>\n",
    "\n",
    "One way is to read in just the columns we need to work with as a regular, complete dataframe, then use the GroupBy method. Here's what that would look like:\n",
    "\n",
    "```python\n",
    ">> moma = pd.read_csv(\"moma.csv\", usecols=['ExhibitionID', 'Gender'])\n",
    ">> id_gender_counts = moma['Gender'].groupby(moma['ExhibitionID']).value_counts()\n",
    ">> id_gender_counts\n",
    "\n",
    "ExhibitionID  Gender\n",
    "2.0           Male        1\n",
    "5.0           Male        3\n",
    "6.0           Male       32\n",
    "              Female      2\n",
    "7.0           Male       69\n",
    "9.0           Male        4\n",
    "...\n",
    "```\n",
    "\n",
    "For each unique value in `ExhibitionID`, we have a unique value count for each `Gender`. We can also accomplish the same result through batch processing. To help you practice what you've learned, we're going to ask you to implement a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While processing the data in chunks of `1000` rows, group each unique value in the `Gender` column by each unique value in the `ExhibitionID` column.\n",
    "* Afterwards, combine all of the intermediate series objects using the `pandas.concat()` method.\n",
    "* Use the GroupBy method again to sum the gender counts for each unique index value.\n",
    "* Assign the final series object to `id_gender_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv('../data/moma.csv',\n",
    "                        usecols=['Gender', 'ExhibitionID'],\n",
    "                        chunksize=1000)\n",
    "\n",
    "id_gender_counts_list = []\n",
    "for chunk in chunk_iter:\n",
    "    \n",
    "    id_gender_count = chunk['Gender'].groupby(chunk['ExhibitionID']).value_counts()\n",
    "    id_gender_counts_list.append(id_gender_count)\n",
    "    \n",
    "\n",
    "id_gender_counts = pd.concat(id_gender_counts_list).groupby(pd.concat(id_gender_counts_list).index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, Male)            1\n",
       "(5.0, Male)            3\n",
       "(6.0, Female)          2\n",
       "(6.0, Male)           32\n",
       "(7.0, Male)           69\n",
       "(9.0, Male)            4\n",
       "(10.0, Male)           2\n",
       "(15.0, Male)           1\n",
       "(17.0, Male)           4\n",
       "(19.0, Male)           2\n",
       "(25.0, Male)           3\n",
       "(26.0, Male)           3\n",
       "(29.0, Male)           2\n",
       "(33.0, Male)           5\n",
       "(35.0, Male)           5\n",
       "(79.0, Female)        18\n",
       "(79.0, Male)         120\n",
       "(80.0, Female)         7\n",
       "(80.0, Male)          36\n",
       "(84.0, Male)           2\n",
       "(85.0, Male)           5\n",
       "(89.0, Male)           1\n",
       "(90.0, Male)           2\n",
       "(92.0, Male)           8\n",
       "(94.0, Male)          12\n",
       "(102.0, Female)        1\n",
       "(102.0, Male)          4\n",
       "(103.0, Male)          3\n",
       "(107.0, Male)          2\n",
       "(114.0, Male)          1\n",
       "                    ... \n",
       "(10550.0, Male)       24\n",
       "(10551.0, Female)      6\n",
       "(10551.0, Male)       28\n",
       "(10552.0, Female)      4\n",
       "(10552.0, Male)       21\n",
       "(10553.0, Female)      3\n",
       "(10553.0, Male)       35\n",
       "(10554.0, Female)      4\n",
       "(10554.0, Male)       18\n",
       "(10555.0, Female)      3\n",
       "(10555.0, Male)       21\n",
       "(10556.0, Female)      3\n",
       "(10556.0, Male)       29\n",
       "(10557.0, Female)      1\n",
       "(10557.0, Male)       15\n",
       "(10558.0, Female)      3\n",
       "(10558.0, Male)       20\n",
       "(10559.0, Female)      3\n",
       "(10559.0, Male)       11\n",
       "(10574.0, Male)        1\n",
       "(10600.0, Male)        1\n",
       "(10601.0, Female)     23\n",
       "(10601.0, Male)      177\n",
       "(10684.0, Female)      3\n",
       "(10684.0, Male)       10\n",
       "(10799.0, Female)      8\n",
       "(10799.0, Male)       21\n",
       "(10800.0, Female)      2\n",
       "(10800.0, Male)        7\n",
       "(11267.0, Male)        1\n",
       "Name: Gender, Length: 2214, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_gender_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this mission, we explored how to work with files that are too big to fit in memory using chunking. Next, you'll complete a guided project we designed to help you solidify what you learned about chunking and optimizing dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
