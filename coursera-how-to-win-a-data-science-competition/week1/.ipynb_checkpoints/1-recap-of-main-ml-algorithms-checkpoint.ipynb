{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "### Recap of main ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Families of ML algorithms**\n",
    "* Linear\n",
    "* Tree-based\n",
    "  * [material - explanation of random forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)\n",
    "  * [mateiral - explanation/demonstration of gradient boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "* kNN\n",
    "  * [material - example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n",
    "- seperate objects with a plane with divide places into `2` parts\n",
    "\n",
    "**[Pros]**\n",
    "- Especially good for sparse high dimensional data\n",
    "\n",
    "**[Cons]**\n",
    "- Sometimes different points can't be seperated by a simple approach\n",
    "  * When the points are located in the way being hard to seperate with a direct line.\n",
    "\n",
    "**[Examples]**\n",
    "- Logistic regression, support vector machines\n",
    "- Scikit-learn, vowpal wabbit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based\n",
    "\n",
    "Tree-based methods split splace into multiple boxes\n",
    "\n",
    "**[Pros]**\n",
    "* Powerful - many ranked competitors in kaggle uses this approach.\n",
    "\n",
    "**[Cons]**\n",
    "* Hard to capture linear dependencies since it requires a lot of splits\n",
    "* Prone to overfitting \n",
    "\n",
    "**[Example]**\n",
    "* Random Forest from scikit-learn, XGboost, LightGBM, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the [material about Gradient] Boosting(http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html):\n",
    "\n",
    "![gradient-boosting-explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "\n",
    "### What is Gradient Boosting?\n",
    "\n",
    "To begin with, gradient boosting is an ensembling technique, which means that prediction is done by an ensemble of simpler estimators. While this theoretical framework makes it possible to create an ensemble of various estimators, in practice we almost always use GBDT — gradient boosting over decision trees. This is the case I cover in the demo, but in principle any other estimator could be used instead of a decision tree.<br>\n",
    "\n",
    "The aim of gradient boosting is to create (or \"train\") an ensemble of trees, given that we know how to train a single decision tree. This technique is called boosting because we expect an ensemble to work much better than a single estimator.<br>\n",
    "\n",
    "**[Some interesting things about GB]**\n",
    "* before a tree is added to an ensemble, it's predictions are multiplied by some factor\n",
    "* this factor (called $\\eta$ or learning rate) is an important parameter of GB ( $\\eta = 0.3$ in this demo)\n",
    "* if we set number of trees to 10 and vary the depth\n",
    "  * we notice that as the depth gets higher, the residual gets smaller, but it's also more noisy\n",
    "* discontinuities ('spikes') appear at those points where a decision tree split\n",
    "* the larger the learning rate — the larger the 'step' made by a single decision tree — and the larger the discontinuity\n",
    "* **in practice GBDT is used with small learning rate ( $0.01 < \\eta < 0.1$ ) and large number of trees to get the best results**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN-based methods\n",
    "\n",
    "basic idea : closer objects will likely to have the same labels.\n",
    "\n",
    "**[Pros]**\n",
    "* **Features based on kNN are often very informative!**\n",
    "\n",
    "**[Cons]**\n",
    "* For some data, application of kNN algorithm is not working at all.\n",
    "\n",
    "**[Example]**\n",
    "* knn in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Networks\n",
    "A special class of machine learning models - which deserve a seperate topic.\n",
    "\n",
    "* Feed-forward neural nets are harder to interpret,\n",
    "* but they produce **smooth non-linear decision boundary**.\n",
    "\n",
    "**[Example-workframes]**\n",
    "* `tensorflow`, `keras`, `pytorch`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `No Free Lunch Theorem`\n",
    "\n",
    "* \"There is no method which **outperforms all others for all tasks**\n",
    "* \"For every method **we can construct a task for which this particular method will not be the best**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most powerful methods are Gradient Boosted Decision Trees and Neural Nets, but ...\n",
    "\n",
    "* we should not underestimate Linear Models and kNN, because they sometimes may be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
