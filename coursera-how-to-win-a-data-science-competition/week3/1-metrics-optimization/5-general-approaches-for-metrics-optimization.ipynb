{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Approaches for Metrics Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "* Loss vs. metric\n",
    "* Approaches to metrics optimization in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss vs. metric\n",
    "\n",
    "* **Target metric** is what we want to optimize\n",
    "* **Optimization loss** is what *model optimizes*\n",
    "  * for example, logarithmic loss is widely used as an optization loss\n",
    "  * while the accuracy score is how the solution is eventually evaluated\n",
    "\n",
    "\n",
    "### Sometimes we want to optimize metrics that ...\n",
    "* are really hard or even impossbile to optimize directly\n",
    "* In this case, we usually set the model to optimize a loss that is different to a target metric\n",
    "  * BUT after a model is trained, we use **hacks and heuristics** to negate the discrepancy \n",
    "  * and adjust the model to better fit the target metric\n",
    "* It is completely okay to say target loss as optimization metric\n",
    "  * but we will fix the wording for the clarity now\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for target metric optimization\n",
    "\n",
    "The approaches can be broadly divided into several categories, depending on the metric we need to optimize. Some metrcis can be optimized directly.\n",
    "\n",
    "* **Just run the right model!**\n",
    "  - MSE, Logloss\n",
    "* **Preprocess train and optimize another metric**\n",
    "  - MSPE MAPE, RMSLE ...\n",
    "  - for example, while `MSPE` cannot be optimized directly with XGBoost, we will see later that we can `resample` that train set and optimize `MSE loss` instead, which XGBoost can optimize.\n",
    "* **Optimize another metric, postprocess predictions**\n",
    "  - Accuracy, Kappa\n",
    "* **Write custgom loss function**\n",
    "  - Any, if you can\n",
    "  - for example, quadratic-weighted Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss for XGBoost\n",
    "\n",
    "* **Define an 'objective'**:\n",
    "  - function that computes *first and second order derivatives w.r.t. predictions*\n",
    "  \n",
    "```python\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0 - preds)\n",
    "    return grad, hess \n",
    "```\n",
    "\n",
    "### We only need to implement a single function that \n",
    "* take `predictions` and the `target values`\n",
    "* compute first and second-order derivatives of the loss function with respect to the model's predictions\n",
    "  * For example, here you see one for the Logloss\n",
    "  * Of course, the loss function should be smooth enough and have well-behaved derivatives - \n",
    "    * otherwise XGBoost will drive crazy!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this course, we will consider only a small set of metrics\n",
    "* BUT there are plenty of them in fact!\n",
    "* Some of them, it's really hard to come up with a neat optimization procedure or write a custom loss function\n",
    "* Thankfully there is a method that always works:\n",
    "  * **`EARLY STOPPING`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "![early-stoppping](../img/early-stopping.png)\n",
    "\n",
    "* You set a model to optimize any loss function it can optimize \n",
    "* And you monitor the desired metric on a validation set\n",
    "* And you stop the training when the model starts to fit \n",
    "  * **`according to the desired metric and not according to the metric the model is truly optimizing`**\n",
    "\n",
    "#### Some metrics cannot be even easily evaluated\n",
    "* for example, if the metric is based on a human assessor's opinions, you cannot evaluate it on every iteration\n",
    "  * for such metric, we cannot use early stopping\n",
    "  * but we will never find such metrics in competition!\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* **Loss vs. metric**\n",
    "* **Approaches in general**\n",
    "  - Just run the right model\n",
    "  - Preprocess train and optimize another metric\n",
    "  - Optimize another metric, postprocess predictions\n",
    "  - Write a custom loss function\n",
    "  - Optimize another metric, use early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
