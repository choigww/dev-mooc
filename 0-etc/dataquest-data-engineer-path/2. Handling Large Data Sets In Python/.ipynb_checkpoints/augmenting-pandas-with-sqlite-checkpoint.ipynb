{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting Pandas With SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Pandas with SQLite\n",
    "\n",
    "So far, we've explored a few different ways we can work with medium-sized data sets in pandas. First, we learned how to reduce a dataframe's memory footprint by selecting the optimal data types for each column. Then, we discussed how to work with dataframe chunks and modify our processing logic. In this mission, we'll explore how to augment pandas with SQLite.<br>\n",
    "\n",
    "While pandas stores and works with data in memory, a database tool like SQLite can represent data on disk. This means that while pandas is limited by the amount of available memory (usually a few gigabytes), **SQLite is limited only by the amount of available disk space** (usually hundreds of gigabytes to a few terabytes). This difference is even more pronounced when working with servers in the cloud because the price for extra disk storage is much cheaper than extra memory. With a [maximum supported file size limit](https://www.sqlite.org/limits.html) of 140 terabytes, we can store large data sets in a SQLite database, write SQL queries to extract a subset of the data we want to work with, and use pandas to explore, analyze, and visualize the subset.<br>\n",
    "\n",
    "We'll continue to work with the data set on MOMA Exhibitions from the first two missions in this course, which you can read more about and download at [data.world](https://data.world/moma/exhibitions). First, we'll create a new SQLite database file and load the entire data set into a single SQLite table. Let's work **under the constraint that we have very limited memory**, and that we need to read the data set into chunks and append them to a table in SQLite.<br>\n",
    "\n",
    "One way to do this would be to iterate over each line in a CSV file, manually parse each row, and insert it into SQLite using insert statements. Instead of doing that, let's read chunks of the file in as dataframes and use the `DataFrame.to_sql()` method to append the rows in each dataframe to a SQLite database table. At a minimum, we need to specify the table we want to add the rows to and pass in the SQLite cursor object:\n",
    "\n",
    "```python\n",
    ">> conn = sqlite3.connect('test.db')\n",
    ">> df = pd.DataFrame({'A': [0,1,2], 'B': [3,4,5]})\n",
    ">> df.to_sql('test', conn)\n",
    "```\n",
    "\n",
    "By default, the `DataFrame.to_sql()` method won't append to an existing table; nothing will happen if the table already exists. When exporting chunked dataframes to SQLite, we need to set the `if_exists` parameter to `'append'` to tell the method to append multiple chunks to the same table. Finally, we need to set the `index` parameter to `False` so that it doesn't add the index values from the dataframes to the SQLite database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Connect to the `moma.db` SQLite database.\n",
    "* Create an iterator using `pandas.read_csv()` that will process chunks of `1000 rows` from `moma.csv` at a time. Assign this iterator to `moma_iter`.\n",
    "* Use `moma_iter` to read in chunks of `1000` rows into a dataframe.\n",
    "* Append each dataframe chunk to the `exhibitions` table in `moma.db` without including the index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('moma.db')\n",
    "moma_iter = pd.read_csv('data/moma.csv', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in moma_iter:\n",
    "    \n",
    "    chunk.to_sql('exhibitions', conn, if_exists='append', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Types vs. SQLite Types\n",
    "\n",
    "When we use the `DataFrame.to_sql()` method to add rows to a SQLite database, pandas automatically converts the dataframe's data types to the equivalent SQLite data types. You can find a list of the SQLite data types in the [documentation](https://www.sqlite.org/datatype3.html), which we've recreated below. You'll notice that **SQLite doesn't have special data types for representing datetime, boolean, or categorical values**.\n",
    "\n",
    "Type|Description\n",
    "---|---\n",
    "NULL|The value is a NULL value\n",
    "INTEGER|The value is a signed integer, stored in 1, 2, 3, 4, 6, or 8 bytes, depending on the magnitude of the value\n",
    "REAL|The value is a floating point value, stored as an 8-byte IEEE floating point number\n",
    "TEXT|The value is a text string, stored using the database encoding (UTF-8, UTF-16BE or UTF-16LE)\n",
    "BLOB|The value is a blob of data, stored exactly as it was entered\n",
    "\n",
    "When we export rows from a pandas dataframe to SQLite using the `DataFrame.to_sql()` method, pandas inserts rows by selecting the equivalent SQLite data type.<br>\n",
    "\n",
    "We can use the `pandas.read_sql()` function to query a SQLite database. The function parses the results into a pandas dataframe and returns them in that structure.\n",
    "\n",
    "```python\n",
    ">> conn = sqlite3.connect('test.db')\n",
    ">> df = pd.DataFrame({'A': [0,1,2], 'B': [3,4,5]})\n",
    ">> df.to_sql('test', conn)\n",
    ">> pd.read_sql('select A from test', conn)\n",
    "   A\n",
    "0  0\n",
    "1  1\n",
    "2  2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `pandas.read_sql()` function to query `moma.db` and return the column types for the `exhibitions` table. Assign the resulting dataframe to `results_df`.\n",
    "* Display `results_df` using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cid                    name     type  notnull dflt_value  pk\n",
      "0     0            ExhibitionID  INTEGER        0       None   0\n",
      "1     1        ExhibitionNumber     TEXT        0       None   0\n",
      "2     2         ExhibitionTitle     TEXT        0       None   0\n",
      "3     3  ExhibitionCitationDate     TEXT        0       None   0\n",
      "4     4     ExhibitionBeginDate     TEXT        0       None   0\n",
      "5     5       ExhibitionEndDate     TEXT        0       None   0\n",
      "6     6     ExhibitionSortOrder  INTEGER        0       None   0\n",
      "7     7           ExhibitionURL     TEXT        0       None   0\n",
      "8     8          ExhibitionRole     TEXT        0       None   0\n",
      "9     9           ConstituentID     REAL        0       None   0\n",
      "10   10         ConstituentType     TEXT        0       None   0\n",
      "11   11             DisplayName     TEXT        0       None   0\n",
      "12   12               AlphaSort     TEXT        0       None   0\n",
      "13   13               FirstName     TEXT        0       None   0\n",
      "14   14              MiddleName     TEXT        0       None   0\n",
      "15   15                LastName     TEXT        0       None   0\n",
      "16   16                  Suffix     TEXT        0       None   0\n",
      "17   17             Institution     TEXT        0       None   0\n",
      "18   18             Nationality     TEXT        0       None   0\n",
      "19   19    ConstituentBeginDate     REAL        0       None   0\n",
      "20   20      ConstituentEndDate     REAL        0       None   0\n",
      "21   21               ArtistBio     TEXT        0       None   0\n",
      "22   22                  Gender     TEXT        0       None   0\n",
      "23   23                  VIAFID     REAL        0       None   0\n",
      "24   24              WikidataID     TEXT        0       None   0\n",
      "25   25                  ULANID     REAL        0       None   0\n",
      "26   26          ConstituentURL     TEXT        0       None   0\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "        PRAGMA table_info(exhibitions);\n",
    "'''\n",
    "results_df = pd.read_sql(query, conn)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Appropriate Types\n",
    "\n",
    "The SQLite database consumes **11.1 megabytes on disk**, which closely matches the amount of disk space the original CSV file consumes. Let's see if we can reduce the amount of disk space the SQLite database requires by optimizing the data types of the columns in the `exhibitions` table.<br>\n",
    "\n",
    "If you'll recall from the first mission in this course, we had to clean the data a bit to be able to convert some of the text columns to a numeric type. Unfortunately, data cleaning in SQLite can be cumbersome for many tasks. Because we're reading in values to pandas first anyway, let's focus our type optimization efforts there. **Selecting the correct types in SQLite reduces the disk footprint of the database file, and can make some SQLite operations faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each dataframe chunk, convert the `ExhibitionSortOrder` column to the `int16` data type using the `pandas.to_numeric()` function.\n",
    "* Use the `pandas.read_sql()` function to query `moma.db` and return the column types for the `exhibitions` table. Assign the resulting dataframe to `results_df`.\n",
    "* Display `results_df` using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cid                    name     type  notnull dflt_value  pk\n",
      "0     0            ExhibitionID  INTEGER        0       None   0\n",
      "1     1        ExhibitionNumber     TEXT        0       None   0\n",
      "2     2         ExhibitionTitle     TEXT        0       None   0\n",
      "3     3  ExhibitionCitationDate     TEXT        0       None   0\n",
      "4     4     ExhibitionBeginDate     TEXT        0       None   0\n",
      "5     5       ExhibitionEndDate     TEXT        0       None   0\n",
      "6     6     ExhibitionSortOrder  INTEGER        0       None   0\n",
      "7     7           ExhibitionURL     TEXT        0       None   0\n",
      "8     8          ExhibitionRole     TEXT        0       None   0\n",
      "9     9           ConstituentID     REAL        0       None   0\n",
      "10   10         ConstituentType     TEXT        0       None   0\n",
      "11   11             DisplayName     TEXT        0       None   0\n",
      "12   12               AlphaSort     TEXT        0       None   0\n",
      "13   13               FirstName     TEXT        0       None   0\n",
      "14   14              MiddleName     TEXT        0       None   0\n",
      "15   15                LastName     TEXT        0       None   0\n",
      "16   16                  Suffix     TEXT        0       None   0\n",
      "17   17             Institution     TEXT        0       None   0\n",
      "18   18             Nationality     TEXT        0       None   0\n",
      "19   19    ConstituentBeginDate     REAL        0       None   0\n",
      "20   20      ConstituentEndDate     REAL        0       None   0\n",
      "21   21               ArtistBio     TEXT        0       None   0\n",
      "22   22                  Gender     TEXT        0       None   0\n",
      "23   23                  VIAFID     REAL        0       None   0\n",
      "24   24              WikidataID     TEXT        0       None   0\n",
      "25   25                  ULANID     REAL        0       None   0\n",
      "26   26          ConstituentURL     TEXT        0       None   0\n"
     ]
    }
   ],
   "source": [
    "for chunk in moma_iter:\n",
    "    \n",
    "    #chunk['ExhibitionSortOrder'] = pd.to_numeric(chunk['ExhibitionSortOrder'], downcast='integer')\n",
    "    chunk['ExhibitionSortOrder'] = chunk['ExhibitionSortOrder'].astype('int16')\n",
    "    chunk.to_sql('exhibitions', conn, if_exists='append', index=False)\n",
    "    \n",
    "results_df = pd.read_sql('PRAGMA TABLE_INFO(exhibitions);', conn)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Primarily in SQL\n",
    "\n",
    "Generating a pandas dataframe from our results set unlocks a few different workflows for us. One involves doing most of the computation in SQL, then parsing the results as a dataframe. Another workflow involves doing the data selection with SQL, but the iterative exploration and analysis in pandas. Each workflow has its own set of trade-offs, which we'll explore in greater depth in this mission. We'll start by performing all of the processing in SQL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Query the `exhibitions` table in `moma.db` to return both the unique values in the `ExhibitionID` column and the counts **in descending order by the counts** as a dataframe. Here's how the dataframe should be formatted:\n",
    "\n",
    "\n",
    " |ExhibitionID|counts\n",
    "---|---|---\n",
    "0|NaN|429\n",
    "1|7.0|321\n",
    "\n",
    "* Assign the resulting dataframe to `eid_counts` and display the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExhibitionID</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3838.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3030.0</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3988.0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2600.0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>79.0</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10601.0</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3939.0</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3036.0</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExhibitionID  counts\n",
       "0           NaN     429\n",
       "1           7.0     321\n",
       "2        3838.0     302\n",
       "3        3030.0     284\n",
       "4        3988.0     275\n",
       "5        2600.0     262\n",
       "6          79.0     259\n",
       "7       10601.0     256\n",
       "8        3939.0     254\n",
       "9        3036.0     244"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "        SELECT DISTINCT(ExhibitionID) AS ExhibitionID,\n",
    "                COUNT(*) AS counts\n",
    "        FROM exhibitions\n",
    "        GROUP BY 1\n",
    "        ORDER BY 2 DESC\n",
    "'''\n",
    "\n",
    "eid_counts = pd.read_sql(query, conn)\n",
    "eid_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Primarily in Pandas\n",
    "\n",
    "In the last step, we expressed the entire computation in SQL. Because this was a small task, the results came back relatively quickly. **SQLite will take much longer for larger computations because it's not operating in memory**. We can treat SQLite primarily as an archival data store instead, and only use SQL to return subsets of the data. **If we restrict the size of the subsets and ensure the results can fit into memory as a dataframe, we can move all of our heavier computations to pandas**.<br>\n",
    "\n",
    "Pandas has several advantages over SQLite. \n",
    "* First, pandas has a large suite of functions and methods for performing common operations. \n",
    "* It also has a diverse type system we can use to save space and improve code running speed. \n",
    "* Finally, pandas works in memory and will be much quicker for most tasks.\n",
    "\n",
    "Let's rewrite the task we performed in the last step using a mix of pandas and SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the `exhibitions` table in `moma.db`, return the `ExhibitionID` column as a dataframe.\n",
    "* Calculate the unique value counts of the `ExhibitionID` column in pandas, and assign them to `eid_pandas_counts`. Display the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0        321\n",
       "3838.0     302\n",
       "3030.0     284\n",
       "3988.0     275\n",
       "2600.0     262\n",
       "79.0       259\n",
       "10601.0    256\n",
       "3939.0     254\n",
       "3036.0     244\n",
       "2749.0     225\n",
       "Name: ExhibitionID, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "        SELECT ExhibitionID FROM exhibitions;\n",
    "'''\n",
    "eid_pandas_counts = pd.read_sql(query, conn).iloc[:, 0].value_counts()\n",
    "eid_pandas_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in SQL Results Using Chunks\n",
    "\n",
    "The techniques we've learned so far in this mission are useful whenever the dataframe representing the results of a SQL query fits in memory. \n",
    "\n",
    "### When we're working with a data set that consumes multiple terabytes on disk as a SQLite database file, \n",
    "we may find ourselves wanting to explore a subset of the data in pandas, but lacking sufficient memory to read the subset in as a dataframe. In cases like these, we can read the results in as dataframe chunks and then batch process the chunks, just like we did earlier in this course.\n",
    "\n",
    "```python\n",
    "q = 'select exhibitionid from exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=100)\n",
    "for chunk in chunk_iter:\n",
    "    # Process each chunk.\n",
    "```\n",
    "\n",
    "You'll often find yourself working on a data science team that has databases containing millions of rows. While you can perform most of the computation in SQL itself, pandas (and the Python ecosystem more generally) has richer support for mathematical operations and data visualization. Querying the data in SQL and working with batches of the results set will help you make the most of SQL and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use this step to experiment with different chunk sizes, and observe their effects on runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 ms ± 16.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "q = 'SELECT ExhibitionID FROM exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=100)\n",
    "eid_pandas_counts = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts.append(chunk.iloc[:,0].value_counts())\n",
    "\n",
    "eid_pandas_counts = pd.concat(eid_pandas_counts)\n",
    "eid_pandas_counts_sum = eid_pandas_counts.groupby(eid_pandas_counts.index).sum().sort_values(ascending=False)\n",
    "eid_pandas_counts_sum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 ms ± 8.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "q = 'SELECT ExhibitionID FROM exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=500)\n",
    "eid_pandas_counts = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts.append(chunk.iloc[:,0].value_counts())\n",
    "\n",
    "eid_pandas_counts = pd.concat(eid_pandas_counts)\n",
    "eid_pandas_counts_sum = eid_pandas_counts.groupby(eid_pandas_counts.index).sum().sort_values(ascending=False)\n",
    "eid_pandas_counts_sum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms ± 10 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "q = 'SELECT ExhibitionID FROM exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=1000)\n",
    "eid_pandas_counts = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts.append(chunk.iloc[:,0].value_counts())\n",
    "\n",
    "eid_pandas_counts = pd.concat(eid_pandas_counts)\n",
    "eid_pandas_counts_sum = eid_pandas_counts.groupby(eid_pandas_counts.index).sum().sort_values(ascending=False)\n",
    "eid_pandas_counts_sum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this mission, we explored how to augment our pandas workflow with SQLite to handle larger data sets. We learned that while SQLite can represent larger data sets on disk, it's slower for most computations. In the next mission, we'll learn the basics of building data pipelines to operationalize our data processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
