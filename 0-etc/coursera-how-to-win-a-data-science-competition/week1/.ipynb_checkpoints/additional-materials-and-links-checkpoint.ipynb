{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional materials and links\n",
    "\n",
    "### Overview of methods\n",
    "* [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)\n",
    "* [Overview of k-NN](http://scikit-learn.org/stable/modules/neighbors.html) (sklearn's documentation)\n",
    "* [Overview of Linear Models](http://scikit-learn.org/stable/modules/linear_model.html) (sklearn's documentation)\n",
    "* [Overview of Decision Trees](http://scikit-learn.org/stable/modules/tree.html) (sklearn's documentation)\n",
    "* Overview of algorithms and parameters in [H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)\n",
    "  > \"The time taken to train random forests may sometimes be too huge as you train multiple decision trees. Also, in the case of a categorical variable, the time complexity increases exponentially. For a categorical column with n levels, RF tries split at 2^n -1 points to find the maximal splitting point. **However, with the power of H2O we can now train random forests pretty fast**. You may want to read about H2O at H2O in R explained.\"\n",
    "  * http://manishbarnwal.com/blog/2017/03/28/h2o_with_r/\n",
    "\n",
    "### Additional Tools\n",
    "* [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)\n",
    "* [XGBoost](https://github.com/dmlc/xgboost) repository\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM) repository\n",
    "* [Interactive demo](http://playground.tensorflow.org/) of simple feed-forward Neural Net\n",
    "* Frameworks for Neural Nets: [Keras](https://keras.io/),[PyTorch](http://pytorch.org/),[TensorFlow](https://www.tensorflow.org/),[MXNet](http://mxnet.io/), [Lasagne](http://lasagne.readthedocs.io/)\n",
    "* [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "* [Arbitrary order factorization machines](https://github.com/geffy/tffm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandCloud Computing:\n",
    "[AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)\n",
    "\n",
    "### AWS spot option:\n",
    "* [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)\n",
    "* [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)\n",
    "\n",
    "### Stack and packages:\n",
    "* [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)\n",
    "* [Jupyter Notebook](http://jupyter.org/)\n",
    "* [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)\n",
    "* Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)\n",
    "* Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))\n",
    "* Python distribution with all-included packages: [Anaconda](https://www.continuum.io/what-is-anaconda)\n",
    "* [Blog \"datas-frame\"](https://tomaugspurger.github.io/) (contains posts about effective Pandas usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature preprocessing\n",
    "* [Preprocessing in Sklearn](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "* [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)\n",
    "* [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n",
    "### Feature generation\n",
    "* [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "* [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from text\n",
    "\n",
    "### Bag of words\n",
    "* [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "* [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)\n",
    "\n",
    "### Word2vec\n",
    "* [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)\n",
    "* [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)\n",
    "* [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)\n",
    "\n",
    "### NLP Libraries\n",
    "* [NLTK](http://www.nltk.org/)\n",
    "* [TextBlob](https://github.com/sloria/TextBlob)\n",
    "\n",
    "## Feature extraction from images\n",
    "\n",
    "### Pretrained models\n",
    "* [Using pretrained models in Keras](https://keras.io/applications/)\n",
    "* [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)\n",
    "\n",
    "### Finetuning\n",
    "* [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)\n",
    "* [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
