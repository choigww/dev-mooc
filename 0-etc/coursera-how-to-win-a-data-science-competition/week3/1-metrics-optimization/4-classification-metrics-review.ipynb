{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan for the video\n",
    "* Accuracy\n",
    "* Logarithmic loss\n",
    "* Area under ROC Curve (`AUC`)\n",
    "* (Quadratic weighted) Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "![classification-metrics-notation](../img/classification-metrics-notation.png)\n",
    "\n",
    "* `Hard label` is usually a function of soft labels\n",
    "  * usually `argmax` for multi class takss\n",
    "  * But for binary classification an be thought of as a **thresholding function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy score\n",
    "\n",
    "$$Accuracy = \\frac{1}{N}|\\hat{y_i} = y_i|$$\n",
    "* How frequently our class prediction is correct\n",
    "\n",
    "$$Accuracy = \\frac{1}{N}|\\alpha = y_i|$$\n",
    "* **Best constant**:\n",
    "  - predict the most frequent class\n",
    "  \n",
    "#### Caveat in interpreting the value of the accuracy score\n",
    "* Dataset\n",
    "  - 10 cats\n",
    "  - 90 dogs\n",
    "* **Predict always dog**:\n",
    "  - Accuracy = `.9`\n",
    "  \n",
    "#### Additional note for `accuracy score`\n",
    "* Simple, clean, intuitive.\n",
    "* Hard to optimize\n",
    "* Does not care **how confident the classifier is in the prediction** and **what soft predictions are**\n",
    "  - It cares only about `argmax` hard predictions, not soft predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic loss (`logloss`)\n",
    "\n",
    "A log-loss usually reasons a little bit differntly for binary and multi-classification tasks\n",
    "\n",
    "* **Binary**\n",
    "  * Assuming that `y hat` is a number from 0-1 range\n",
    "  * Show the probability of an object to belong to calss 1 \n",
    "$$LogLoss = -\\frac{1}{N}\\sum^N_{i=1}y_{i}log(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i})$$\n",
    "\n",
    "$$y_{i}\\in\\mathbb{R}$$\n",
    "$$\\hat{y_i}\\in\\mathbb{R}$$\n",
    "\n",
    "\n",
    "* **Multiclass**\n",
    "  * $\\hat{y_i}$ - a vector of size L with 1 as its sum\n",
    "    * The elements are the probabilities to belong to each of the classes\n",
    "\n",
    "$$LogLoss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^L_{l=1}y_{il}log(\\hat{y_{il}})$$\n",
    "$$y_{i}\\in\\mathbb{R}$$\n",
    "$$\\hat{y_i}\\in\\mathbb{R}$$\n",
    "\n",
    "* **In practice**\n",
    "  * Predictions are clipped to be not from 0 from 1\n",
    "  * Predictions clipped from some positive number to 1 minus some small positive number\n",
    "\n",
    "$$LogL oss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^L_{l=1}y_{il}log(min(max(\\hat{y_{il}}, 10^{-15}), 1-10^{-15}))$$\n",
    "\n",
    "\n",
    "### LogLoss strongly penalizes completely wrong answers!\n",
    "* Prefers to make **`a lot of small mistakes`** to **`a few but severer mistakes`**\n",
    "![logloss-analysis](../img/logloss-analysis.png)\n",
    "\n",
    "\n",
    "#### Best constant\n",
    "\n",
    "$$LogLoss = -\\frac{1}{N}\\sum^N_{i=1}y_{i}log(\\alpha)+(1-y_i)log(1-\\alpha)$$\n",
    "\n",
    "- **set $\\alpha_{i}$ to frequency of i-th class.**\n",
    "\n",
    "----\n",
    "\n",
    "Dataset:\n",
    "- 10 cats\n",
    "- 90 dgs \n",
    "- $\\alpha = [0.1, 0.9]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve (AUC ROC)\n",
    "\n",
    "![auc-roc-ex1](../img/auc-roc-ex1.png)\n",
    "![auc-roc-ex2](../img/auc-roc-ex2.png)\n",
    "\n",
    "Recall that ...\n",
    "* to compute accuracy score for a binary task, we usually take soft predictions from our model and apply threshold\n",
    "  * threshold `.5` ---> $Accuracy(|\\hat{y}>0.5|) = \\frac{6}{7}$\n",
    "  * threshold `.7` ---> $Accuracy(|\\hat{y}>0.7|) = 1$\n",
    "  \n",
    "#### This metric (AUC) tries all possible ones and aggregates those scores\n",
    "* **`ONLY FOR BINARY TASKS`**\n",
    "* **Depends only on ordering of the predictions**, not on absolute values\n",
    "\n",
    "#### Several explanations\n",
    "1. Area Under Curve\n",
    "2. Pairs Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation 1 on AUC\n",
    "\n",
    "Positive side = `RED` / negative side = `GREEN`\n",
    "![ex1-on-auc](../img/ex1-on-auc.png)\n",
    "\n",
    "* We will go from left to right, jump from one obj to another\n",
    "* Paint `RED` if right (positive) and if not, paint `GREEN`\n",
    "* The curve we've just build is called `Receiver Operating Curve`\n",
    "\n",
    "![ex1-on-auc2](../img/ex1-on-auc2.png)\n",
    "\n",
    "* `AreaSize = 7`\n",
    "* We need to normalize it by the total plural area of the square\n",
    "  * **AUC = 7/9**\n",
    "  \n",
    "  \n",
    "#### What AUC will be for the data that can be seperated with a threshold? (like our inital example)\n",
    "* AUC will be `1` - the maxium value of AUC\n",
    "* **It does not need a threshold to be specified and it doesn't depend on absolute values**\n",
    "\n",
    "#### If you build such curve for a huge data set in real classifier ...\n",
    "![](../img/auc-huge-data.png)\n",
    "* The curve usually lie above the dashed line (`y = x`) \n",
    "  * which shows how would the curve look like if we made predictions **at random** = `baseline`!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation 2 on AUC\n",
    "\n",
    "Consider all pairs of objects such that\n",
    "* one object is from red class and another one is from green\n",
    "  * **`AUC` is a probability that score for the green one will be higher than the score for the red one**\n",
    "  \n",
    "#### In other words, `AUC` is a fraction of correctly ordered pairs\n",
    "\n",
    "$$AUC = \\frac{\\text{num of correctly ordered pairs}}{\\text{total num of pairs}}\\\\=1-\\frac{\\text{num of incorrectly ordered pairs}}{\\text{total num of pairs}}$$\n",
    "\n",
    "![ex2-auc-img](../img/ex2-auc-img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC best constant\n",
    "\n",
    "* Best constant\n",
    "  - All constants given same score\n",
    "  \n",
    "* Random predictions lead to AUC = `0.5` (`baseline`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa motivation\n",
    "\n",
    "Dataset:\n",
    "* 10 cats\n",
    "* 90 dogs\n",
    "* `Baseline accuracy = 0.9`\n",
    "\n",
    "----\n",
    "\n",
    "$$\\text{my_score} = 1 - \\frac{1 - \\text{accuracy}}{1 - \\text{baseline}}$$\n",
    "\n",
    "#### We can introduce a new metric such that ...\n",
    "* accuracy = 1 ------> my-score = 1\n",
    "* accuracy = 0.9 ------> my_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Cohen's Kappa, we take another value as the baseline (other than `50:50`)\n",
    "\n",
    "Dataset:\n",
    "* 10 cats\n",
    "* 90 dogs\n",
    "* Predict 20 cats and 80 dogs at random \n",
    "  * accuracy ~ 0.74\n",
    "  * `0.2*0.1 + 0.8*0.9 = 0.74`\n",
    "  \n",
    "-----\n",
    "\n",
    "$$\\text{Cohen's Kappa} = 1 - \\frac{1 - \\text{accuracy}}{1 - p_{e}}$$\n",
    "\n",
    "$p_{e}$ - `what accuracy would be on average, if we randomly permute our predictions`\n",
    "\n",
    "$$p_{e} = \\frac{1}{N^2}\\sum_{k}n_{k1}n_{k2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also recall that error is equal to 1 minus accuracy\n",
    "\n",
    "Dataset:\n",
    "* 10 cats\n",
    "* 90 dogs\n",
    "* Predict 20 cats and 80 dogs at random \n",
    "  * accuracy ~ 0.74\n",
    "  * error ~ 0.26\n",
    "  \n",
    "-----\n",
    "$$\\text{Cohen's Kappa} = 1 - \\frac{1 - \\text{error}}{1 - \\text{baseline error}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error\n",
    "\n",
    "Dataset:\n",
    "* 10 cats / 90 dogs / 20 tigers\n",
    "\n",
    "\n",
    "### Say we are more or less okay if we predict dog instead of cat, but it's undesirable to predict cat or dog if it's really a tiger.\n",
    "\n",
    "#### Error weight matrix\n",
    "* Each cell contains the weight for the mistake we might do\n",
    "\n",
    "pred\\true|cat|dog|tiger\n",
    "---|---|---|---\n",
    "**cat**|0|1|10\n",
    "**dog**|1|0|10\n",
    "**tiger**|1|1|0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error and weighted Kappa\n",
    "\n",
    "![confusion-weighted-matrix](../img/confusion-weighted-matrix.png)\n",
    "\n",
    "1. **Multiply these two matrices element-wise**\n",
    "2. **Sum their results**\n",
    "  * This formula needs a proper normalization to make sure the qunatity is between 0 and 1\n",
    "  * But it does not matter for our purpose, as the noramlization constant will anyway cancel\n",
    "$$\\text{weighted error} = \\frac{1}{\\text{const}}\\sum_{i,j}C_{ij}W_{ij}$$\n",
    "\n",
    "<br>\n",
    "$$\\text{Weighted Kappa} = 1 - \\frac{1 - \\text{weighted error}}{1 - \\text{weighted baseline error}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic and Linear Weighted Kappa\n",
    "* The more distant the prediction is from the real label, the more the model gets penalized\n",
    "\n",
    "![quad-lin-weighted-kappa](../img/quad-lin-weighted-kappa.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "* Accuracy\n",
    "* Logloss\n",
    "* AUC (ROC)\n",
    "* (Quadratic weighted) Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
