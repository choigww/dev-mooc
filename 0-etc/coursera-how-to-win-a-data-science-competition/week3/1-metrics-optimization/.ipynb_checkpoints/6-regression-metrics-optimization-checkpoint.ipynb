{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression metrics and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics optimization : our plan\n",
    "**1. Regression**\n",
    "  * **MSE, (R)MSE, R-squared**\n",
    "  * **MAE**\n",
    "  * **(R)MSPE, MAPE**\n",
    "  * **(R)MSLE**\n",
    "2. Classification\n",
    "  * Accuracy\n",
    "  * Logloss\n",
    "  * AUC\n",
    "  * Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE, MSE, R-squared\n",
    "\n",
    "<br>\n",
    "<center>How do you optimize them?</center>\n",
    "<center>Just fit the right model!</center>\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "$$R^2 = 1 - \\frac{MSE}{\\frac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "### Models supporrting MSE optimization\n",
    "\n",
    "#### Tree-based\n",
    "> XGBoost, LightGBM<br>\n",
    "> sklearn.RandomForestRegressor (can split based on MSE)\n",
    "\n",
    "#### Linear Models\n",
    "> sklearn.<>Regression<br>\n",
    "> sklearn.SGDRegressor (using gradient decent to train // very versatile)<br>\n",
    "> Vowpal / Wabbit (quantile loss)\n",
    "\n",
    "#### Neural nets\n",
    "> PyTorch, Keras, TensorFlow, etc.\n",
    "\n",
    "Synonyms: L2 loss (Read the docs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE\n",
    "\n",
    "$$MAE = \\frac{1}{N}\\sum^N_{i=1}|y_i - \\hat{y_i}|$$\n",
    "\n",
    "How to optimize? --- Again, just run the right model!\n",
    "\n",
    "\n",
    "\n",
    "### Models supporrting MSE optimization\n",
    "\n",
    "#### Tree-based\n",
    "* XGB cannot optimize MAE because MAE has zero as a second derivative while LightGBM can\n",
    "  * So you still can use gradient boosting decision trees to this metric\n",
    "* MAF criteria was implemented for RandomForestRegressor from sklearn\n",
    "  * But note that running time will be quite high compared with MSE Corte.\n",
    "\n",
    "> LightGBM<br>\n",
    "> sklearn.RandomForestRegressor (can split based on MSE)\n",
    "\n",
    "#### Linear Models\n",
    "* Unfortunately linear models from sklearn including SG Regressor cannot optimize MAE negatively\n",
    "  * BUT there is a loss function called **`Huber Loss`** implemented in some of the models\n",
    "  * Basically it's very similar to MAE especially when the errors are large\n",
    "> Vowpal / Wabbit (different name --- `quantile loss`)\n",
    "\n",
    "#### Neural nets\n",
    "* As we discussed MAF is not differentiable only when the predictions are equal to target (= rare case)\n",
    "* That is why we may use any model train to put out optimize MAE\n",
    "> PyTorch, Keras, TensorFlow, etc.\n",
    "\n",
    "Synonyms: L1, Median Regression (Read the docs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE : optimal constant\n",
    "\n",
    "![mae-optimal-const](../img/mae-optimal-const.png)\n",
    "\n",
    "You can actually make up your own smooth function that have upload that look like MAE error.\n",
    "* the most famous one is `Huber loss` - it's basically a mix between MSE and MAE.\n",
    "  * MSF is computed when the error is small so we can safely approach zero error\n",
    "  * MAE is computed for large errors given robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSPE and MAPE\n",
    "\n",
    "$$MSPE = \\frac{\\text{100%}}{N}\\sum^N_{i=1}(\\frac{y_i-\\hat{y_i}}{y_i})^2$$\n",
    "\n",
    "$$MAPE = \\frac{\\text{100%}}{N}\\sum^N_{i=1}|\\frac{y_i-\\hat{y_i}}{y_i}|$$\n",
    "\n",
    "<br>\n",
    "<center>How do you optimize them?</center>\n",
    "<center></center>\n",
    "\n",
    "* It's much harder to find the model which can optimize them out of the box\n",
    "  * we can always either implement a custom loss for an integer boost or a neural net\n",
    "  * Or we can optimize different metric and do early stopping\n",
    "  \n",
    "#### But there are several approaches that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSPE (MAPE) as weighted MSE (MAE)\n",
    "\n",
    "This approach is based on the fact that MSPE is a weighted version of MSE and MAPE is a weighted version of MAE.\n",
    "\n",
    "\n",
    "![mspe-as-weighted-mse](../img/mspe-as-weighted-mse.png)\n",
    "\n",
    "* The common denominator just ensures that the weights are summed up to 1 but it is not required\n",
    "* Intuitively, the sample weights are indicating how important the object is for us while training the model\n",
    "\n",
    "#### The smaller the target, the more important the object\n",
    "#### How do we use this knowledge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSPE (MAPE)\n",
    "\n",
    "### Use weights for samples (`sample_weights`)\n",
    "- And use MSE (MAE)\n",
    "  - The model will actaully optimize desired MSPE loss!\n",
    "- *Not every libarary accepts sample weights*\n",
    "  * XGBoost, LightGBM accept\n",
    "  * Neural nets\n",
    "    - Easy to implement if not supported\n",
    "    \n",
    "#### But there is another method which works whenever a libarary can optimize MSE/MAE\n",
    "\n",
    "### Resample the train set\n",
    "- `df.sample(weights=sample_weights)`\n",
    "- And use *any* model that optimizes MSE (MAE) \n",
    "- Train the data resampled with the model\n",
    "- **Usually need to resample many times and average each time you fit the model**\n",
    "  - And then average models' predictions if we will get the score much better and more stable\n",
    "\n",
    "It is important to **set the probabilities for each object to be sampled to the weights we've calculated**. The size of the new data set is up to you.\n",
    "* You can sample twice as many objects as it was in original train set.\n",
    "* Note that we do not need to do anything with the test set\n",
    "\n",
    "#### The results are another way we can optimize MSPE\n",
    "* If the errors are small, we can optimize the predictions in logarithmic scale - we will cover this in the next slide. For details, you can find in the reading materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSLE\n",
    "\n",
    "![rmsle](../img/rmsle-opt.png)\n",
    "\n",
    "Quite easy to optimize becuase of the connection with MSE Loss. All we need to do is ...\n",
    "\n",
    "1. Apply and transform to our taget variables in train set\n",
    "  * $z_i = log(y_{i}+1)$\n",
    "2. Fit a model with MSE loss to transform target \n",
    "3. Get a prediction for a test subject, we first obtain the prediction, $\\hat{z}$ in the logarithmic scale just by calling `model.predict()` or something like that\n",
    "4. Do an inverse transform from logarithmic scale back to the original by exponentiating z hat and subtracting by one\n",
    "  * $\\hat{y_i} = exp(\\hat{z_i}) - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
