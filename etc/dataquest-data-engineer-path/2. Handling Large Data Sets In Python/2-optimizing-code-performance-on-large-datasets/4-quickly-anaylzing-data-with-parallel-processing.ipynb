{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickly Analyzing Data With Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Quotes Data\n",
    "\n",
    "In the past few missions, we learned about I/O bound and CPU bound programs. We also learned about threads, processes, and in which situations to use each. In this mission, we'll bring everything together, and learn how to process a dataset using what we've learned.<br>\n",
    "\n",
    "We'll be using a dataset of movie quotes during this mission. The quotes are taken from the scripts of `1068` movies, and constitute `894014` lines altogether, and take up `56` megabytes of disk space. Although we want these examples in this mission to execute quickly for learning purposes, limiting dataset size, the techniques we'll be using can be scaled up to much larger datasets easily.<br>\n",
    "\n",
    "The movie lines are either [IMDB](https://www.imdb.com/) memorable quotes, or lines surrounding memorable quotes. The number of lines we have for each movie doesn't necessarily correspond to the length of the movie. The dataset originally came from [Cornell](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), but has been modified to be easier to work with.<br>\n",
    "\n",
    "The dataset is `1068` different text files, one for each movie. All of the files are stored in the folder `lines`. Each text file contains all of the lines we have for the movie, separated by a newline character (`\\n`). Here's an example for [The Princess Bride](http://www.imdb.com/title/tt0093779/), which is an excerpt from `theprincessbride.txt`:\n",
    "\n",
    "```\n",
    "Hello. My name is Inigo Montoya, you killed my father, prepare to die.\n",
    "Hello. My name is Inigo Montoya. You killed my father. Prepare to die.\n",
    "Stop saying that!\n",
    "```\n",
    "\n",
    "Each file also has some relatively messy artifacts, such as stray punctuation or other lines. Here's another example, again from `theprincessbride.txt`:\n",
    "\n",
    "```\n",
    "I wasn't finished -- the next thing you lose will be your left eye, followed by your right --\n",
    " -- and then my ears, I understand. Let's get on with it\n",
    " Wrong! Your ears you keep, and I'll tell you why --\n",
    " -- so that every shriek of every child at seeing your hideousness will be yours to cherish -- every babe that weeps at your approach, every woman who cries out, \"Dear God, what is that thing?\" will echo in your perfect ears.\n",
    " --------------------------------------------------------------------------- 119.\n",
    " That is what \"to the pain\" means. It means I leave you in anguish, wallowing in freakish misery forever.\n",
    "```\n",
    "\n",
    "We'll have to figure out a way to deal with the stray punctuation as we progress through the mission. For now, let's just load in a couple of the text files and familiarize ourselves with the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open the `lines/theprincessbride.txt` file, display it, and explore it. What does the structure look like? Do you see any stray or unexpected characters?\n",
    "* Open the `lines/theroadwarrior.txt` file, display it, and explore it. What's different from the other file?\n",
    "* Explore other files (you can list files using the [os.listdir()](https://docs.python.org/3/library/os.html#os.listdir) function) until you're satisified that you've found most of the strange punctuation and other stray, non-dialogue, lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You feeling any better?\n",
      " A little bit.\n",
      " Guess what.\n",
      " What?\n",
      " Your grandfather's here.\n",
      " Mom, can't you tell him that I'm sick?\n",
      " You are sick, that's why he's here.\n",
      " He'll pinch my cheek. I hate that.\n",
      " Maybe he won't.\n",
      " Hey! How's the sickie? Heh?\n",
      " I think I'll leave you two pals.\n",
      " I brought you a special present.\n",
      " What is it?\n",
      " Open it up.\n",
      " A book?\n",
      " That's right. When I was your age, television was called books. And this is a special book. It was the book my father used to read to me when I was sic\n"
     ]
    }
   ],
   "source": [
    "with open('../data/lines/theprincessbride.txt') as f:\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GEORGE MILLER BRIAN HANNANT\n",
      " The vision dims and all that remains are mememories.  They take me back - back to the place where the black pump sucked guzzolene from the earth...\n",
      " And I remember the terrible battle we fought - the day we left that place forever...\n",
      " But, most of all, I remember the courage of a stranger, a road warrior called Max.  To understand who he was you must go back to the last days of the old world ...\n",
      " ...  when, for reasons long forgotten, two mighty warrior nations went\n"
     ]
    }
   ],
   "source": [
    "with open('../data/lines/theroadwarrior.txt') as f:\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/29/1993\n",
      " Whoa, whoa, whoa... what's that?\n",
      " We begin again... let's kick in that new transducer. See if we can double the power output.\n",
      " Whoa, Doc... wait a minute...\n",
      " Okey... dokey!\n",
      " Hiya Peter...\n",
      " Hiya Liz... how're you doing...?\n",
      " How'm I doing what?\n",
      " Peter! Peter! Wait up!\n",
      " Would you, uh... excuse us for a minute?\n",
      " What is it? what...\n",
      " Did the Astro-Physics Journal really accept your paper?\n",
      " Yeah... well... all I got's the data but they've agreed to publish it when it's finished. You know..\n"
     ]
    }
   ],
   "source": [
    "with open('../data/lines/spiderman.txt') as f:\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concurrent Futures Package\n",
    "\n",
    "In the past two missions, we used the [threading](https://docs.python.org/3/library/threading.html) and [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) packages to utilize threads and processes, respectively. While both of these packages are widely used, and give you more low-level control, Python 3 introduced a new package, called **[concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html), that enables you to create pools of threads and workers**. The main advantage of the `concurrent.futures` package is that it gives you a **simple, consistent, interface for both threads and processes**. We'll use it in this mission to make switching between threads and processes easy.<br>\n",
    "\n",
    "We can create pools of threads using the [concurrent.futures.ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) class. Here's an example:\n",
    "\n",
    "```python\n",
    "import concurrent.futures\n",
    "\n",
    "def word_length(word):\n",
    "    return len(word)\n",
    "\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
    "lengths = pool.map(word_length, [\"Hello\", \"are\", \"you\", \"thinking\", \"of\", \"becoming\", \"a\", \"polar\", \"bear\", \"?\"])\n",
    "```\n",
    "\n",
    "Note that the [concurrent.futures.ThreadPoolExecutor.map()](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.map) method returns a [generator](https://wiki.python.org/moin/Generators), and not a list, as we might expect from using `multiprocessing`. We just need to call `list(lengths)` to force the generator to evaluate.<br>\n",
    "\n",
    "We can easily swap between threads and processes -- to switch from a worker pool that uses a thread for each worker to a pool that uses a process for each worker, just swap `ThreadPoolExecutor` for [concurrent.futures.ProcessPoolExecutor](https://docs.python.org/dev/library/concurrent.futures.html#processpoolexecutor):\n",
    "\n",
    "```python\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement a `ProcessPoolExecutor` with `5` workers.\n",
    "* Use the pool to find the square root of each of the numbers in `numbers`. Make sure to use the [math.sqrt()](https://docs.python.org/3/library/math.html#math.sqrt) function.\n",
    "* Assign the results to `roots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "numbers = [1,10,20,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 3.1622776601683795, 4.47213595499958, 7.0710678118654755]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_square_root(num):\n",
    "    return math.sqrt(num)\n",
    "\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "roots = list(pool.map(calc_square_root, numbers))\n",
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In Files\n",
    "\n",
    "Now that we understand how to use the `concurrent.futures` package, we can start analyzing our data. To get started, we'll:\n",
    "* Create a thread pool.\n",
    "* Write a function to count the number of lines in each file.\n",
    "* Execute the function across all of the filenames.\n",
    "* Turn the result into a dictionary, where the keys are the names of the files, and the values are the number of lines in each file.\n",
    "\n",
    "You'll recall from the last mission that threads are better in I/O bound situations, and this is such a case, since we're reading in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an instance of `ThreadPoolExecutor` with `5` workers. Feel free to experiment with worker count if you want to see how it affects performance.\n",
    "* Create a function that takes a file name as input, opens the file, then counts the numbers of lines in the file.\n",
    "  * It should return the number of lines.\n",
    "* Use the `map` method to execute the function across all of the files in the `lines` directory.\n",
    "  * Assign the result to `lengths`.\n",
    "* Turn `lengths` into the dictionary `movie_lengths`, where the keys are filenames, and the values are the number of lines in the file.\n",
    "  * The file names should not have the folder name -- they should look like `spiderman.txt`.\n",
    "* Find the key in the dictionary that corresponds to the most lines. Assign the result to `most_lines`.\n",
    "* Print out `most_lines`, and the key in the dictionary that corresponds to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiderman.txt', 'theroadwarrior.txt', 'theprincessbride.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir('../data/lines')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spiderman.txt': 1502, 'theroadwarrior.txt': 185, 'theprincessbride.txt': 786}\n",
      "spiderman.txt\n",
      "1502\n"
     ]
    }
   ],
   "source": [
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "\n",
    "def count_lines(filename):\n",
    "    with open('../data/lines/'+filename) as f:\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "lengths = list(pool.map(count_lines, files))\n",
    "lengths = dict(zip(files, lengths))\n",
    "\n",
    "most_lines = max(lengths.keys(), key=(lambda k: lengths[k]))\n",
    "\n",
    "print(lengths)\n",
    "print(most_lines)\n",
    "print(lengths[most_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The Longest Lines\n",
    "\n",
    "As you saw in the last screen, `spiderman.txt` has the most lines, with `1502`. This may mean that `spiderman.txt` has the most memorable lines of any movie. Now, let's see what movie script in our set has the longest line. We can again use threads for this, since it's primarily I/O bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that takes in a filename, and returns both the length of the longest line, and the text of the longest line.\n",
    "* Execute the function across all the files in `lines`.\n",
    "* Find the movie with the longest line, and assign to `longest_line_movie`.\n",
    "* Find the text of the longest line, and assign to `longest_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_line(filename):\n",
    "    with open('../data/lines/'+filename) as f:\n",
    "        longest_len = 0\n",
    "        longest_line = ''\n",
    "        \n",
    "        for line in f:\n",
    "            leng = len(line)\n",
    "            if leng > longest_len:\n",
    "                longest_len = leng\n",
    "                longest_line = line\n",
    "    return longest_len, longest_line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spiderman.txt': (369,\n",
       "  ' They are in the order Aranae, which is divided into three sub-orders- Mesothelea, Orthognatha, and Labidognatha. All spiders are carnivorous, ravenous eaters who feed on massive quantities of protein, in liquid form, usually the juices of their prey. Arachnids from each of the three groups possess varying strengths which help them in their constant search for food.\\n'),\n",
       " 'theprincessbride.txt': (502,\n",
       "  \" You'd like to think that, wouldn't you? You've beaten my giant, which means you're exceptionally strong. So, you could have put the poison in your own goblet, trusting on your strength to save you. So I can clearly not choose the wine in front of you. But, you've also bested my Spaniard which means you must have studied. And in studying, you must have learned that man is mortal so you would have put the poison as far from yourself as possible, so I can clearly not choose the wine in front of me.\\n\"),\n",
       " 'theroadwarrior.txt': (290,\n",
       "  \" There has been too much violence ... too much pain.  None here is without sin.  But, I have an honourable compromise.  Give me the gasoline and I'll spare your lives.  Just walk away.  I will give you safe passage in the wasteland ...  Walk away and there will be an end to the horror ...\\n\")}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_line_result = list(pool.map(find_longest_line, files))\n",
    "\n",
    "movie_and_longest_line = dict(zip(files, longest_line_result))\n",
    "movie_and_longest_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theprincessbride.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_line_movie = max(movie_and_longest_line.keys(), \n",
    "                         key=(lambda k: movie_and_longest_line[k][0]))\n",
    "longest_line_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" You'd like to think that, wouldn't you? You've beaten my giant, which means you're exceptionally strong. So, you could have put the poison in your own goblet, trusting on your strength to save you. So I can clearly not choose the wine in front of you. But, you've also bested my Spaniard which means you must have studied. And in studying, you must have learned that man is mortal so you would have put the poison as far from yourself as possible, so I can clearly not choose the wine in front of me.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_line = movie_and_longest_line[longest_line_movie][1]\n",
    "longest_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The Most Commonly Used Word\n",
    "\n",
    "Now that we've iterated through the lines, we can go a step deeper and analyze words. Let's define a word as any text that's between two spaces. Here's an example:\n",
    "\n",
    "```\n",
    "Hey!  I'm slowly becoming a polar bear, help me please.\n",
    "```\n",
    "\n",
    "In the above example, `Hey!`, `I'm, slowly`, `becoming`, `a`, `polar`, `bear,`, `help`, `me`, and `please`. are all considered words. We'll deal with the punctuation in a later screen. What we want to do is figure out the most commonly used word in each file.<br>\n",
    "\n",
    "**Because this operation is slightly more CPU intensive, let's switch to using the `ProcessPoolExecutor` class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function to take in a filename, then:\n",
    "  * Read the file.\n",
    "  * Split the file into words.\n",
    "  * Find the most common word.\n",
    "  * Return the most common word.\n",
    "* Execute the function using a process pool with `2` workers.\n",
    "  * Assign the result to `words`.\n",
    "* Assemble the results into a dictionary, where each key is a filename, and each value is the most common word in that file.\n",
    "  * Assign the result to `common_words`.\n",
    "* Try some profiling of this code using processes and threads. Do you notice any patterns? You may need to execute the code multiple times to get a stable time estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word(filename):\n",
    "    \n",
    "    with open('../data/lines/'+filename) as f:\n",
    "        split_list = f.read().split(' ')\n",
    "        word_counts = {}\n",
    "        \n",
    "        for string in split_list:\n",
    "            if string in word_counts:\n",
    "                word_counts[string] += 1\n",
    "            else:\n",
    "                word_counts[string] = 1\n",
    "        \n",
    "        most_common_word = max(word_counts.keys(),\n",
    "                              key=(lambda k: word_counts[k]))\n",
    "        \n",
    "        return most_common_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spiderman.txt': 'I', 'theprincessbride.txt': 'I', 'theroadwarrior.txt': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "words = list(pool.map(most_common_word, files))\n",
    "common_words = dict(zip(files, words))\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012008070945739746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 30.,  30.,  14.,   5.,   4.,   5.,   6.,   3.,   1.,   2.]),\n",
       " array([ 0.01114202,  0.01166692,  0.01219182,  0.01271672,  0.01324162,\n",
       "         0.01376653,  0.01429143,  0.01481633,  0.01534123,  0.01586614,\n",
       "         0.01639104]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhjoktQIA12SGmGgS1IjDHRJasT/AQ/PNb54d6SZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10efacb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    pool = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "    words = list(pool.map(most_common_word, files))\n",
    "    common_words = dict(zip(files, words))\n",
    "    total = time.time() - start\n",
    "    times.append(total)\n",
    "    \n",
    "print(statistics.median(times))\n",
    "plt.hist(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11250615119934082\n"
     ]
    }
   ],
   "source": [
    "# using collections.Counter()\n",
    "\n",
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    from collections import Counter\n",
    "    count = Counter(words)\n",
    "\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "results = []\n",
    "start = time.time()\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"../data/lines/{}\".format(f) for f in os.listdir(\"../data/lines/\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "end = time.time()\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"../data/lines/\", \"\")] = words[i]\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Errors\n",
    "\n",
    "Before we move into working more with the `ProcessPoolExecutor` class, let's discuss how to debug parallel programs. Because of how processes works, sometimes the error that you see returned will be hard to parse. This is because the Python interpreter returns the error from the current process, not the error from the child processes that it created. For example, let's modify our function from the last screen to try to access a nonexistent index, `100`:\n",
    "\n",
    "```python\n",
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    count = Counter(words)\n",
    "\n",
    "    return count.most_common()[0][100]\n",
    "```\n",
    "\n",
    "There are only two items in each tuple returned by `collections.Counter.most_common()`. If we try to run this function in parallel using `ProcessPoolExecutor`, we'll get a cryptic error message:\n",
    "\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "<ipython-input-30-b9c63af62d43> in <module>()\n",
    "     17 filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "     18 words = pool.map(most_common_word, filenames)\n",
    "---> 19 words = list(words)\n",
    "     20 \n",
    "     21 common_words = {}\n",
    "\n",
    "/opt/dsserver/python/lib/python3.4/concurrent/futures/_base.py in result_iterator()\n",
    "    547                 for future in fs:\n",
    "    548                     if timeout is None:\n",
    "--> 549                         yield future.result()\n",
    "    550                     else:\n",
    "    551                         yield future.result(end_time - time.time())\n",
    "\n",
    "/opt/dsserver/python/lib/python3.4/concurrent/futures/_base.py in result(self, timeout)\n",
    "    393                 raise CancelledError()\n",
    "    394             elif self._state == FINISHED:\n",
    "--> 395                 return self.__get_result()\n",
    "    396 \n",
    "    397             self._condition.wait(timeout)\n",
    "\n",
    "/opt/dsserver/python/lib/python3.4/concurrent/futures/_base.py in __get_result(self)\n",
    "    352     def __get_result(self):\n",
    "    353         if self._exception:\n",
    "--> 354             raise self._exception\n",
    "    355         else:\n",
    "    356             return self._result\n",
    "\n",
    "IndexError: tuple index out of range\n",
    "</module></ipython-input-30-b9c63af62d43>\n",
    "```\n",
    "\n",
    "This tell us that the function we called returned an error when it was trying to generate a result. The actual error was in a child process that ran the function, but since the processes don't share memory state, we don't have access to the actual error message from the child process. The easiest way to debug an error like this is to try to call the function on its own, without using `ProcessPoolExecutor`:\n",
    "\n",
    "```python\n",
    "most_common_word('spiderman.txt')\n",
    "```\n",
    "\n",
    "It can also be useful to add print statements into the function, to see the state right before the error happened:\n",
    "\n",
    "```python\n",
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    count = Counter(words)\n",
    "\n",
    "    print(filename)\n",
    "    return count.most_common()[0][100]\n",
    "```\n",
    "\n",
    "If you're using the `multiprocessing` module, you can also turn on logging, so you see various error messages. [Here](https://docs.python.org/3/library/multiprocessing.html#logging) is the documentation on how to debug.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Debug the program below, and see if you can get it to run properly.\n",
    "* Hit \"Next Step\" when you're done experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spiderman.txt': 'I', 'theprincessbride.txt': 'I', 'theroadwarrior.txt': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    count = Counter(words)\n",
    "    \n",
    "    # return count.most_common[0][100] ---> IndexError\n",
    "    # return count.most_common[0][50] --> IndexError\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"../data/lines/{}\".format(f) for f in os.listdir(\"../data/lines\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"../data/lines/\", \"\")] = words[i]\n",
    "    \n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation\n",
    "\n",
    "You may have noticed two screens ago, the most common word was almost always something like `the`, or `I`. Although these are very common words in the English language, they aren't particularly interesting words. We'd like to find words that have some relevance to the specific movie.<br>\n",
    "\n",
    "To fix this, we'll only consider words that are over `5` letters long. This should filter out common but uninformative words.\n",
    "\n",
    "You may have also noticed that some of the most common words were punctuation, like `--`, due to how we defined words. Let's use a regular expression or substitution to remove all of the non-alphanumeric characters. We'll also lowercase the words, so that `Hey` and `hey` aren't seen as different.<br>\n",
    "\n",
    "These operations will take more CPU operations than the last screen, so it's going to be worth it to profile the performance and see if we can optimize our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify your function from before to remove punctuation, and filter out long words:\n",
    "  * Lowercase all of the words.\n",
    "  * Ignore words under 5 characters long.\n",
    "  * Remove any characters that aren't `a-z`, `A-Z`, `_`, or `0-9`, and replace them with a space ().\n",
    "* Execute the code using a process pool with `2` workers.\n",
    "  * Assign the result to `words`.\n",
    "* As before, find the common words, and assign to the `common_words` dictionary.\n",
    "* Profile your code, and see if you can improve performance. Is your answer executing in `O(n)` time? Is your worker count correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    count = Counter(words)\n",
    "    \n",
    "    # return count.most_common[0][100] ---> IndexError\n",
    "    # return count.most_common[0][50] --> IndexError\n",
    "    return count.most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word(filename):\n",
    "    # Fill in the function here\n",
    "\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        words = f.read().lower()\n",
    "        \n",
    "        data = re.sub('\\W+', \" \", words)\n",
    "        words = data.split(' ')\n",
    "        \n",
    "        words = [w for w in words if len(w) >= 5]\n",
    "        count = Counter(words)\n",
    "        \n",
    "        return dict(count)\n",
    "        \n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"../data/lines/{}\".format(f) for f in os.listdir(\"../data/lines\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"../data/lines/\", \"\")] = words[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Word Frequencies\n",
    "\n",
    "Now that we know how to clean up and extract the words, let's see if we can find the top `200` words across all of the movies, and their frequencies.<br>\n",
    "\n",
    "To do this, we'll need to:\n",
    "* Modify our function to return the total counts for each word in each file.\n",
    "* Combine all the counts from all the files.\n",
    "\n",
    "At the end, we'll have a list of all the most common words, along with their counts. Each item in the list should be a tuple containing the word, then the combined frequency across all of the files.<br>\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "```python\n",
    "[\n",
    " ('there', 43859),\n",
    " ('about', 36102),\n",
    " ('right', 29314),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill in the `word_frequencies` function, so that it:\n",
    "  * Remove punctuation, ignores words under `5` letters long, and everything else we did in the last screen.\n",
    "  * Returns the counts of each word in the file.\n",
    "* Execute the function across all of the files with a process pool.\n",
    "* Combine all of the counts so that you get the combined frequencies across all of the files.\n",
    "  * Assign the result to `total_word_counts`.\n",
    "* Find the top 200 keys and values in `total_word_counts`.\n",
    "  * Assign the result to `top_200`.\n",
    "  * Make sure it's in the format shown above, and is sorted properly.\n",
    "  * You may want to use the [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter) class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequencies(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read().lower()\n",
    "    \n",
    "    data = re.sub(\"\\W+\", \" \", data)\n",
    "    words = data.split(\" \")\n",
    "    words = [w for w in words if len(w) >= 5]\n",
    "    count = Counter(words)\n",
    "    \n",
    "    return dict(count)\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"../data/lines/{}\".format(f) for f in os.listdir(\"../data/lines\")]\n",
    "word_counts = pool.map(word_frequencies, filenames)\n",
    "word_counts = list(word_counts)\n",
    "\n",
    "total_word_counts = {}\n",
    "\n",
    "for wc in word_counts:\n",
    "    for k,v in wc.items():\n",
    "        if k not in total_word_counts:\n",
    "            total_word_counts[k] = 0\n",
    "        total_word_counts[k] += v\n",
    "\n",
    "top_200 = Counter(total_word_counts).most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 136), ('peter', 96), ('spider', 79), ('where', 73), ('think', 66)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_200[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this mission, we used threads and processes to work with a large dataset more efficiently. Although we didn't present it this way, you were actually using a paradigm called [MapReduce](https://en.wikipedia.org/wiki/MapReduce). The MapReduce paradigm originated at Google, and is utilized in data processing tools like [Apache Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop) and [Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark). We'll more explicitly cover MapReduce and these tools in a future course, but you already know most of the basics already!<br>\n",
    "\n",
    "The next mission will be a guided project where you practice more with processes, threads, and mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
