{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary.\n",
    "\n",
    "**Pipeline of applying BoW**\n",
    "1. Preprocessing:\n",
    "  * Lowercase, stemming, lemmatization, stopwords\n",
    "2. Ngrams can help to use local context\n",
    "3. Postprocessing: TfiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from texts and images\n",
    "\n",
    "![extract-features-ex-titanic](img/extract-features-ex-titanic.png)\n",
    "\n",
    "![extract-features-commonimgtxt](img/extract-features-commonimgtxt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text -> vector\n",
    "\n",
    "![txt-to-vector](img/txt-to-vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "`sklearn.feature_extraction.text.CountVectorizer`\n",
    "\n",
    "![bow](img/bag-of-words.png)\n",
    "\n",
    "#### Post process\n",
    "We also can post process calculated metrics using some pre-defined methods\n",
    "* to make out why we need post-processing, let's remember some models like kNN, linear regression and neural nets depend on scaling of features.\n",
    "  * **Main goal of post-process** \n",
    "    * `to make sampels more comparable`\n",
    "    * `to boost more important features while decreasing the scale of useless one`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words: TFiDF\n",
    "\n",
    "#### Term frequency\n",
    "\n",
    "```python\n",
    "tf = 1 / x.sum(axis=1)[:, None]\n",
    "x = x * tf\n",
    "```\n",
    "\n",
    "In this way, we will count not occurrences but frequencies of words - thus, texts with different sizes will be more comparable --> **`term frequency transformation`**\n",
    "\n",
    "#### Inverse Document Frequency\n",
    "```python\n",
    "idf = np.log(x.shape[0] / (x > 0).sum(0))\n",
    "x = x * idf\n",
    "```\n",
    "\n",
    "`sklearn.future_extraction.text.TfidVectorizer`\n",
    "\n",
    "<br>\n",
    "To boost more important features, we will make post process our matrix by normalizing data column-wise.\n",
    "* A good idea is to **normalize each feature by the inverse fraction of document which contain the exact word corresponding to this feature**.\n",
    "* In this case, features corresponding to frequent words will be scaled down compared to features corresponding to rarer words. \n",
    "* We can further improve this idea by taking a logarithm of these normalization coefficients.\n",
    "* As results, this will decrease the significance of widespread words in the dataset and do require feature scaling. **This is the purpose of inverse document frequency transformation.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying TFiD Transformation to the previous example\n",
    "## Bag of Words: TF\n",
    "\n",
    "![bow-tf](img/bow-tf.png)\n",
    "* `Occurences` switched to `Frequencies`\n",
    "  * it means that some of variance for each row is now equal to `1`.\n",
    "  \n",
    "  \n",
    "## Bag of Words: TF + iDF\n",
    "\n",
    "![bow-tf-idf](img/bow-tf-idf.png)\n",
    "* Data normalized column-wise.\n",
    "* iDF transformation scaled down the appropriate feature \n",
    "  * **There are plenty of other variants of TfiD which may work better depending on the specific data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "![n-grams](img/n-grams.png)\n",
    "\n",
    "The concept of n-grams\n",
    "* You add not only column corresponding to the word, but also columns corresponding to inconsequent words\n",
    "* This concept can also be applied to sequence of chars\n",
    "  * in cases with low N, we will have a column for each possible combination of N chars.\n",
    "  \n",
    "#### Number of biagrams (`N = 2`) for `28` unique symbols is equal to:\n",
    "* `28 * 28 = 784`\n",
    "\n",
    "#### It can be cheaper to have every possible char Ngram as a feature, instead of having a feature for each unique word from the dataset.\n",
    "* Using char Ngrams also helps our model to handle unseen words - e.g. rarer forms of already used words\n",
    "\n",
    "#### In a scaled CountVectorizer has appropriate parameter for Ngrams - `Ngramrange` to change from word Ngrams to char Ngrams\n",
    "* You may use parameter named `analyzer`\n",
    "```\n",
    "sklearn.feature_extraction.text.CountVectorizer:\n",
    "Ngram_range, analyzer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "1. **Lowercase**\n",
    "2. **Lemmatization**\n",
    "3. **Stemming**\n",
    "4. **Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing : lowercase\n",
    "\n",
    "`very == VERY`\n",
    "`Sunny == sunny`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing: lemmatization and stemming\n",
    "\n",
    "```\n",
    "I had a car == I have car\n",
    "```\n",
    "```\n",
    "We have cars == We have car\n",
    "```\n",
    "\n",
    "#### Stemming\n",
    "a heuristic process that **chops off ending of words** and thus unite duration of related words like democracy, democratic, democratization.\n",
    "* democracy, democratic and democratization -> `democr`\n",
    "* Saw -> `s`\n",
    "\n",
    "#### Lemmatization\n",
    "Using knowledge of vocabulary / morphological analogies of quotes returning democracy for each of the words below.\n",
    "* democracy, democratic and democratization -> `democracy`\n",
    "* Saw -> `see` or `saw` (depending on context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing: stopwords\n",
    "\n",
    "Examples:\n",
    "1. Articles or prepositions\n",
    "2. Very common words\n",
    "\n",
    "NLTK, Natrual Lanuage Toolkit library for python\n",
    "```python\n",
    "# stopwords related parameter in CountVectorizer\n",
    "sklearn.feature_extraction.text.CountVectorizer:\n",
    "max_df # threshold of words frequency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
