{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Project Walkthrough:\n",
    "# Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We spent the last 2 missions cleaning and preparing a dataset that contains data on loans made to members of [Lending Club](https://www.lendingclub.com/). Our eventual goal is to generate features from the data, which can feed into a machine learning algorithm. The algorithm will make predictions about whether or not a loan will be paid off on time, which is contained in the `loan_status` column of the clean dataset.<br>\n",
    "\n",
    "As we prepared the data, we removed columns that had data leakage issues, contained redundant information, or required additional processing to turn into useful features. We cleaned features that had formatting issues, and converted categorical columns to dummy variables.<br>\n",
    "\n",
    "In the last mission, we noticed that there's **a class imbalance** in our target column, `loan_status`. There are about `6` times as many loans that were paid off on time (positive case, label of `1`) than those that weren't (negative case, label of `0`). Imbalances can cause issues with many machine learning algorithms, where they appear to have high accuracy, but actually aren't learning from the training data. Because of its potential to cause issues, we need to keep the class imbalance in mind as we build machine learning models.<br>\n",
    "\n",
    "After all of our data cleaning in the past two missions, we ended up with the csv file called `cleaned_loans_2007.csv`. Let's read this file into a Dataframe and view a summary of the work we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read `cleaned_loans_2007.csv` into a Dataframe named `loans`.\n",
    "* Use the `info()` method and the `print` function to display a summary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38708 entries, 0 to 38707\n",
      "Data columns (total 38 columns):\n",
      "loan_amnt                              38708 non-null float64\n",
      "int_rate                               38708 non-null float64\n",
      "installment                            38708 non-null float64\n",
      "emp_length                             38708 non-null int64\n",
      "annual_inc                             38708 non-null float64\n",
      "loan_status                            38708 non-null int64\n",
      "dti                                    38708 non-null float64\n",
      "delinq_2yrs                            38708 non-null float64\n",
      "inq_last_6mths                         38708 non-null float64\n",
      "open_acc                               38708 non-null float64\n",
      "pub_rec                                38708 non-null float64\n",
      "revol_bal                              38708 non-null float64\n",
      "revol_util                             38708 non-null float64\n",
      "total_acc                              38708 non-null float64\n",
      "home_ownership_MORTGAGE                38708 non-null float64\n",
      "home_ownership_NONE                    38708 non-null float64\n",
      "home_ownership_OTHER                   38708 non-null float64\n",
      "home_ownership_OWN                     38708 non-null float64\n",
      "home_ownership_RENT                    38708 non-null float64\n",
      "verification_status_Not Verified       38708 non-null float64\n",
      "verification_status_Source Verified    38708 non-null float64\n",
      "verification_status_Verified           38708 non-null float64\n",
      "purpose_car                            38708 non-null float64\n",
      "purpose_credit_card                    38708 non-null float64\n",
      "purpose_debt_consolidation             38708 non-null float64\n",
      "purpose_educational                    38708 non-null float64\n",
      "purpose_home_improvement               38708 non-null float64\n",
      "purpose_house                          38708 non-null float64\n",
      "purpose_major_purchase                 38708 non-null float64\n",
      "purpose_medical                        38708 non-null float64\n",
      "purpose_moving                         38708 non-null float64\n",
      "purpose_other                          38708 non-null float64\n",
      "purpose_renewable_energy               38708 non-null float64\n",
      "purpose_small_business                 38708 non-null float64\n",
      "purpose_vacation                       38708 non-null float64\n",
      "purpose_wedding                        38708 non-null float64\n",
      "term_ 36 months                        38708 non-null float64\n",
      "term_ 60 months                        38708 non-null float64\n",
      "dtypes: float64(36), int64(2)\n",
      "memory usage: 11.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('data/cleaned_loans_2007.csv')\n",
    "print(loans.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking an error metric\n",
    "\n",
    "Before we dive into predicting `loan_status` with machine learning, let's go back to our first steps when we started cleaning the Lending Club dataset. You may recall the original question we wanted to answer:\n",
    "\n",
    "* **Can we build a machine learning model that can accurately predict if a borrower will pay off their loan on time or not?**\n",
    "\n",
    "We established that this is a binary classification problem in the first mission of this course, and we converted the `loan_status` column to `0s` and `1s` as a result. Before diving in and selecting an algorithm to apply to the data, we should select an error metric.<br>\n",
    "\n",
    "An error metric will help us figure out when our model is performing well, and when it's performing poorly. To tie error metrics all the way back to the original question we wanted to answer, let's say we're using a machine learning model to predict whether or not we should fund a loan on the Lending Club platform. Our objective in this is to make money -- we want to fund enough loans that are paid off on time to offset our losses from loans that aren't paid off. An error metric will help us determine if our algorithm will make us money or lose us money.<br>\n",
    "\n",
    "In this case, we're primarily concerned with false positives and false negatives. Both of these are different types of misclassifications. With a false positive, we predict that a loan will be paid off on time, but it actually isn't. This costs us money, since we fund loans that lose us money. With a false negative, we predict that a loan won't be paid off on time, but it actually would be paid off on time. This loses us potential money, since we didn't fund a loan that actually would have been paid off.<br>\n",
    "\n",
    "Here's a diagram to simplify the concepts:\n",
    "\n",
    "loan_status_actual|prediction|error type\n",
    "---|---|---\n",
    "0|1|False positive\n",
    "1|1|True positive\n",
    "0|0|True negative\n",
    "1|0|False negative\n",
    "\n",
    "In the `loan_status` and `prediction` columns, a `0` means that the loan wouldn't be paid off on time, and a `1` means that it would.<br>\n",
    "\n",
    "Since we're viewing this problem from the standpoint of a conservative investor, we need to treat false positives differently than false negatives. A conservative investor would want to minimize risk, and avoid false positives as much as possible. They'd be more okay with missing out on opportunities (false negatives) than they would be with funding a risky loan (false positives).<br>\n",
    "\n",
    "Let's calculate false positives and true positives in Python. We can use multiple conditionals, separated by a `&` to select items in a NumPy array that meet certain conditions. For instance, if we had an array called `predictions`, we could select items in `predictions` that equal 1 and where items in `loans[\"loan_status\"]` in the same position also equal 1 using this:\n",
    "\n",
    "```python\n",
    "filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "predictions[tp_filter]\n",
    "```\n",
    "\n",
    "The above code will give us all the items in `predictions` that are true positives -- where we predicted that the loan would be paid off on time, and it was actually paid off on time. By using the `len` function to find the number of items, we can find the number of true positives.<br>\n",
    "\n",
    "Using the diagram above as a reference, it's possible to compute the other `3` quantities we mentioned -- false positives, true negatives, and false negatives.<br>\n",
    "\n",
    "We've generated some predictions automatically, and they are stored in a NumPy array called `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the number of true negatives.\n",
    "  * Find the number of items where predictions is 0, and the corresponding entry in loans[\"loan_status\"] is also 0.\n",
    "  * Assign the result to tn.\n",
    "* Find the number of true positives.\n",
    "  * Find the number of items where predictions is 1, and the corresponding entry in loans[\"loan_status\"] is also 1.\n",
    "  * Assign the result to tp.\n",
    "* Find the number of false negatives.\n",
    "  * Find the number of items where predictions is 0, and the corresponding entry in loans[\"loan_status\"] is 1.\n",
    "  * Assign the result to fn.\n",
    "* Find the number of false positives.\n",
    "  * Find the number of items where predictions is 1, and the corresponding entry in loans[\"loan_status\"] is 0.\n",
    "  * Assign the result to fp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter(pred_num, real_num):\n",
    "    return (predictions==pred_num) & (loans['loan_status']==real_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataquest course implies that \n",
    "# `predictions` array exists in the namespace.\n",
    "# Therefore, in this notebook we will not execute the codes below.\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "\n",
    "We mentioned earlier that there is a significant class imbalance in the `loan_status` column. There are `6` times as many loans that were paid off on time (`1`), than loans that weren't paid off on time (`0`). This causes a major issue when we use accuracy as a metric. This is because due to the class imbalance, a classifier can predict `1` for every row, and still have high accuracy. Here's a diagram that illustrates the concept:\n",
    "\n",
    "loan_status actual|prediction\n",
    "---|---\n",
    "0|1\n",
    "1|1\n",
    "1|1\n",
    "1|1\n",
    "1|1\n",
    "1|1\n",
    "1|1\n",
    "\n",
    "In the above diagram, our predictions are `85.7%` accurate -- we've correctly identified `loan_status` in `85.7%` of cases. However, we've done this by predicting `1` for every row. What this means is that we'll actually lose money. Let's say we loan out `1000` dollars on average to each borrower. Each borrower pays us `10%` interest back. So we make a projected profit of `100` dollars on each loan. In the above diagram, we'd actually lose money:\n",
    "\n",
    "loan_status actual|prediction|profit/loss\n",
    "---|---|---\n",
    "0|1|-1000\n",
    "1|1|100\n",
    "1|1|100\n",
    "1|1|100\n",
    "1|1|100\n",
    "1|1|100\n",
    "1|1|100\n",
    "\n",
    "As you can see, we made `600` dollars in interest from the borrowers that paid us back, but we lost `1000` dollars on the one borrower who never paid us back, so we actually ended up losing `400` dollars overall, even though our model is technically accurate.<br>\n",
    "\n",
    "This is why it's important to always be aware of imbalanced classes in machine learning models, and to adjust your error metric accordingly. In this case, we don't want to use accuracy, and should instead use metrics that tell us the number of false positives and false negatives.<br>\n",
    "\n",
    "This means that we should optimize for:\n",
    "\n",
    "* high [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) (true positive rate)\n",
    "* low [fall-out](https://en.wikipedia.org/wiki/Information_retrieval#Fall-out) (false positive rate)\n",
    "\n",
    "We can calculate false positive rate and true positive rate, using the numbers of true positives, true negatives, false negatives, and false positives.<br>\n",
    "\n",
    "False positive rate is the number of false positives divided by the number of false positives plus the number of true negatives. This divides all the cases where we thought a loan would be paid off but it wasn't by all the loans that weren't paid off:\n",
    "\n",
    "```\n",
    "fpr = fp / (tp + tn)\n",
    "```\n",
    "\n",
    "True positive rate is the number of true positives divided by the number of true positives plus the number of false negatives. This divides all the cases where we thought a loan would be paid off and it was by all the loans that were paid off:\n",
    "\n",
    "```\n",
    "tpr = tp / (tp + fn)\n",
    "```\n",
    "\n",
    "We can write these out as mathematical formula as well:\n",
    "\n",
    "$$\\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives + True Negatives}}$$\n",
    "\n",
    "$$\\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n",
    "\n",
    "Simple english ways to think of each term are:\n",
    "\n",
    "* `False Positive Rate` -- \"what percentage of my `1` predictions are incorrect?\"\n",
    "  * In this case, \"what percentage of the loans that I fund would not be repaid?\"\n",
    "* `True Positive Rate` -- \"what percentage of all the possible `1` predictions am I making?\"\n",
    "  * In this case, \"what percentage of loans that could be funded would I fund?\"\n",
    "\n",
    "Generally, if we want to reduce false positive rate, true positive rate will also go down. This is because if we want to reduce the risk of false positives, we wouldn't think about funding riskier loans in the first place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the false positive rate for `predictions`.\n",
    "  * Compute the number of false positives, then dived by the number of false positives plus the number of true negatives.\n",
    "  * Assign to `fpr`.\n",
    "* Compute the true positive rate for `predictions`.\n",
    "  * Compute the number of true positives, then dived by the number of true positives plus the number of false negatives.\n",
    "  * Assign to `tpr`.\n",
    "* Print out `fpr` and `tpr` to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# Predict that all loans will be paid off on time.\n",
    "predictions = pd.Series(numpy.ones(loans.shape[0]))\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()\n",
    "\n",
    "fpr = fp / (fp+tn)\n",
    "tpr = tp / (tp+fn)\n",
    "\n",
    "print(fpr)\n",
    "print(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In the last screen, you may have noticed that both `fpr` and `tpr` were `1`. This is because we predicted `1` for each row. This means that we correctly identified all of the good loans (true positive rate), but we also incorrectly identified all of the bad loans (false positive rate). Now that we've setup error metrics, we can move on to making predictions using a machine learning algorithm.<br>\n",
    "\n",
    "As we saw in the first screen of the mission, our cleaned dataset contains `41` columns, all of which are either the `int64` or the `float64` data type. There aren't any null values in any of the columns. This means that we can now apply any machine learning algorithm to our dataset. Most algorithms can't deal with non-numeric or missing values, which is why we had to do so much data cleaning.<br>\n",
    "\n",
    "In order to fit the machine learning models, we'll use the Scikit-learn library. Although we've built our own implementations of algorithms in earlier missions, it's easier and faster to use algorithms that someone else has already written and tuned for high performance.<br>\n",
    "\n",
    "A good first algorithm to apply to binary classification problems is [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), for the following reasons:\n",
    "\n",
    "* it's quick to train and we can iterate more quickly,\n",
    "* it's less prone to overfitting than more complex models like decision trees,\n",
    "* it's easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a Dataframe named `features` that contains just the feature columns.\n",
    "  * Remove the `loan_status` column.\n",
    "* Create a Series named `target` that contains just the target column (`loan_status`).\n",
    "* Use the [fit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) method of `lr` to fit a logistic regression to `features` and `target`.\n",
    "* Use the [predict](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) method of `lr` to make predictions on `features`. Assign the predictions to `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "features = loans.drop(['loan_status'], axis=1)\n",
    "target = loans['loan_status']\n",
    "\n",
    "lr.fit(features, target)\n",
    "predictions = lr.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "While we generated predictions in the last screen, those predictions were overfit. They were overfit because we generated predictions using the same data that we trained our model on. When we use this to evaluate error, we get an unrealistically high depiction of how accurate the algorithm is, because it already \"knows\" the correct answers. This is like asking someone to memorize a bunch of physics equations, then asking them to plug numbers into the equations. They can tell you the right answer, but they can't explain a concept that they haven't already memorized an equation for.<br>\n",
    "\n",
    "In order to get a realistic depiction of the accuracy of the algorithm, we'll need to use cross validation to generate predictions. Cross validation splits the dataset into groups, then makes predictions on each group using the other groups as training data. This ensures that we don't overfit by generating predictions on the same data that we train our algorithm with. We discussed [cross validation](https://en.wikipedia.org/wiki/Cross-validation) in an earlier mission if you'd like a refresher.<br>\n",
    "\n",
    "We can perform cross validation using the [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html#sklearn.cross_validation.cross_val_predict) method of scikit-learn. `cross_val_predict` allows us to pass in a classifier, the features, and the target.<br>\n",
    "\n",
    "We'll create an instance of [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold), which will perform `3` fold cross validation across our dataset. We set `random_state` to `1` to ensure that the folds are always consistent, and we can compare scores between runs. If we don't, each fold will be randomized every time, making it hard to tell if we're improving our model or not.<br>\n",
    "\n",
    "If we pass the instance of [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold) into [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html#sklearn.cross_validation.cross_val_predict), it will then perform `3` fold cross validation to generate unbiased predictions.<br>\n",
    "\n",
    "Once we have cross validated predictions, we can compute true positive rate and false positive rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate cross validated predictions for `features`.\n",
    "  * Call cross_val_predict using `lr, features, and target`.\n",
    "  * Make sure to pass in `kf` to the keyword argument `cv`. This will ensure that our results are consistent between runs.\n",
    "  * Assign the predictions to `predictions`.\n",
    "* Use the Series class to convert `predictions` to a Pandas Series, as we did two screens ago.\n",
    "  * If we don't do this, our FPR and TPR calculations won't work.\n",
    "* Compute true positive rate and false positive rate.\n",
    "  * Assign true positive rate to `tpr`.\n",
    "  * Assign false positive rate to `fpr`.\n",
    "* Print out `fpr` and `tpr` to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99894237452\n",
      "0.99679430098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict, KFold\n",
    "\n",
    "lr = LogisticRegression()\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalizing the classifier\n",
    "\n",
    "As you can see from the last screen, our `fpr` and `tpr` are around what we'd expect if the model was predicting all ones. We can look at the first few rows of `predictions` to confirm:\n",
    "\n",
    "```python\n",
    "0    1\n",
    "1    1\n",
    "2    1\n",
    "3    1\n",
    "4    1\n",
    "5    1\n",
    "6    1\n",
    "7    1\n",
    "8    1\n",
    "9    1\n",
    "```\n",
    "\n",
    "Unfortunately, even through we're not using accuracy as an error metric, the classifier is, and it isn't accounting for the imbalance in the classes. There are a few ways to get a classifier to correct for imbalanced classes. The two main ways are:<br>\n",
    "\n",
    "* Use **oversampling** and **undersampling** to ensure that the classifier gets input that has a balanced number of each class.\n",
    "* Tell the classifier **to penalize misclassifications of the less prevalent class more than the other class**.\n",
    "\n",
    "We'll look into oversampling and undersampling first. They involve taking a sample that contains equal numbers of rows where `loan_status` is `0`, and where `loan_status` is `1`. This way, the classifier is forced to make actual predictions, since predicting all `1s` or all `0s` will only result in `50%` accuracy at most.<br>\n",
    "\n",
    "The downside of this technique is that since it has to preserve an equal ratio, you have to either:\n",
    "\n",
    "* Throw out many rows of data. If we wanted equal numbers of rows where `loan_status` is `0` and where `loan_status` is `1`, one way we could do that is to delete rows where `loan_status` is `1`.\n",
    "* Copy rows multiple times. One way to equalize the `0s` and `1s` is to copy rows where `loan_status` is `0`.\n",
    "* Generate fake data. One way to equalize the 0s and 1s is to generate new rows where `loan_status` is `0`.\n",
    "\n",
    "Unfortunately, none of these techniques are especially easy. The second method we mentioned earlier, telling the classifier to penalize certain rows more, is actually much easier to implement using scikit-learn.<br>\n",
    "\n",
    "### We can do this by setting the class_weight parameter \n",
    "to `balanced` when creating the LogisticRegression instance. \n",
    "* This tells scikit-learn to penalize the misclassification of the minority class during the training process. \n",
    "* The penalty means that the logistic regression classifier pays more attention to correctly classifying rows where `loan_status` is `0`. This lowers accuracy when `loan_status` is `1`, but raises accuracy when `loan_status` is `0`.<br>\n",
    "\n",
    "By setting the `class_weight` parameter to `balanced`, the penalty is set to be inversely proportional to the class frequencies. You can read more about the parameter here. This would mean that for the classifier, correctly classifying a row where `loan_status` is `0` is `6` times more important than correctly classifying a row where `loan_status` is `1`.<br>\n",
    "\n",
    "We can repeat the cross validation procedure we performed in the last screen, but with the `class_weight` parameter set to `balanced`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instance.\n",
    "  * Remember to set `class_weight` to `balanced`.\n",
    "  * Assign the instance to `lr`.\n",
    "* Create a [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold) instance.\n",
    "  * Pass in `features.shape[0]` to do cross validation across all the rows in the training data.\n",
    "  * Set the keyword argument `random_state` to `1`.\n",
    "  * Assign the result to `kf`.\n",
    "* Generate cross validated predictions for `features`.\n",
    "  * Call [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html#sklearn.cross_validation.cross_val_predict) using `lr`, `features`, and `target`.\n",
    "  * Make sure to pass in `kf` to the keyword argument `cv`. This will ensure that our results are consistent between runs.\n",
    "  * Assign the predictions to `predictions`.\n",
    "* Use the Series class to convert `predictions` to a Pandas Series, as we did in the last screen.\n",
    "  * If we don't do this, our FPR and TPR calculations won't work.\n",
    "* Compute true positive rate and false positive rate.\n",
    "  * Assign true positive rate to `tpr`.\n",
    "  * Assign false positive rate to `fpr`.\n",
    "* Print out `fpr` and `tpr` to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.658991327471\n",
      "0.380053428317\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual penalties\n",
    "\n",
    "We significantly improved false positive rate in the last screen by balancing the classes, which reduced true positive rate. Our true positive rate is now around `66%`, and our false positive rate is around `38%`. From a conservative investor's standpoint, it's reassuring that the false positive rate is lower because it means that we'll be able to do a better job at avoiding bad loans than if we funded everything. However, we'd only ever decide to fund `66%` of the total loans (true positive rate), so we'd immediately reject a good amount of loans.<br>\n",
    "\n",
    "We can try to lower the false positive rate further by assigning a harsher penalty for misclassifying the negative class. While setting `class_weight` to balanced will automatically set a penalty based on the number of `1s` and `0s` in the column, **we can also set a manual penalty**. In the last screen, the penalty scikit-learn imposed for misclassifying a `0` would have been around `5.89` (since there are `5.89` times as many `1s` as `0s`).<br>\n",
    "\n",
    "We can also specify a penalty manually if we want to adjust the rates more. To do this, we need to pass in a dictionary of penalty values to the `class_weight` parameter:\n",
    "\n",
    "```python\n",
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "```\n",
    "\n",
    "The above dictionary will impose a penalty of `10` for misclassifying a `0`, and a penalty of `1` for misclassifying a `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the code from the last screen to change the `class_weight` parameter from the string `\"balanced\"` to the dictionary:\n",
    "\n",
    "```python\n",
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "```\n",
    "\n",
    "* Remember to print out the fpr and tpr values at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.229051461034\n",
      "0.0870881567231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "It looks like assigning manual penalties lowered the false positive rate to `9%`, and thus lowered our risk. Note that this comes at the expense of true positive rate. While we have fewer false positives, we're also missing opportunities to fund more loans and potentially make more money. Given that we're approaching this as a conservative investor, this strategy makes sense, but it's worth keeping in mind the tradeoffs.<br>\n",
    "\n",
    "While we could tweak the penalties further, it's best to move to trying a different model right now, for larger potential false positive rate gains. We can always loop back and interate on the penalties more later.<br>\n",
    "\n",
    "Let's try a more complex algorithm, random forest. We learned about random forests in a previous mission, and constructed our own model. Random forests are able to work with nonlinear data, and learn complex conditionals. Logistic regressions are only able to work with linear data. Training a random forest algorithm may enable us to get more accuracy due to columns that correlate nonlinearly with `loan_status`.<br>\n",
    "\n",
    "We can use the [RandomForestClassifer](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class from scikit-learn to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the code from the last screen, and swap out the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for a [RandomForestClassifer](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model.\n",
    "  * Set the value of the keyword argument `random_state` to `1`, so the predictions don't vary due to random chance.\n",
    "  * Set the value of the keyword argument `class_weight` to `balanced`, so we avoid issues with imbalanced classes.\n",
    "* Remember to print out the `fpr` and `tpr` values at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971806726498\n",
      "0.93303650935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced',\n",
    "                           random_state=1)\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(rf, features, target, cv=kf)\n",
    "\n",
    "tn_filter = make_filter(0, 0); tn = tn_filter.sum()\n",
    "tp_filter = make_filter(1, 1); tp = tp_filter.sum()\n",
    "fn_filter = make_filter(0, 1); fn = fn_filter.sum()\n",
    "fp_filter = make_filter(1, 0); fp = fp_filter.sum()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Unfortunately, using a random forest classifier didn't improve our false positive rate. The model is likely weighting too heavily on the `1` class, and still mostly predicting `1s`. We could fix this by applying a harsher penalty for misclassifications of `0s`.<br>\n",
    "\n",
    "Ultimately, our best model had a false positive rate of `9%`, and a true positive rate of `23%`. For a conservative investor, this means that they make money as long as the interest rate is high enough to offset the losses from `9%` of borrowers defaulting, and that the pool of `23%` of borrowers is large enough to make enough interest money to offset the losses.<br>\n",
    "\n",
    "If we had randomly picked loans to fund, borrowers would have defaulted on `14.5%` of them, and our model is better than that, although we're excluding more loans than a random strategy would. Given this, there's still quite a bit of room to improve:\n",
    "\n",
    "* We can tweak the penalties further.\n",
    "* We can try models other than a random forest and logistic regression.\n",
    "* We can use some of the columns we discarded to generate better features.\n",
    "* We can ensemble multiple models to get more accurate predictions.\n",
    "* We can tune the parameters of the algorithm to achieve higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
