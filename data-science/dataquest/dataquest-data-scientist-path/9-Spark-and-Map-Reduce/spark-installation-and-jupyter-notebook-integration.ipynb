{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project:\n",
    "### Spark Installation and Jupyter Notebook Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last mission, we introduced the Spark cluster computing framework and explored some basic PySpark methods, all within the Dataquest interface. In this project, we'll walk through how to set up Spark on your own computer and integrate PySpark with Jupyter Notebook. We can use Spark in two modes:\n",
    "\n",
    "* **Local mode** - The entire Spark application runs on a single machine. **Local mode is what you'll use to prototype Spark code on your own computer**. It's also easier to set up.\n",
    "* **Cluster mode** - The Spark application runs across multiple machines. **Cluster mode is what you'll use when you want to run your Spark application across multiple machines in a cloud environment like `Amazon Web Services`, `Microsoft Azure`, or `Digital Ocean`**.\n",
    "\n",
    "For now, we'll walk through the instructions for installing Spark in local mode on Windows, Mac, and Linux. We'll cover how to install Spark in cluster mode as part of the data engineering track.\n",
    "\n",
    "Here's a diagram describing the high-level components you'll be setting up today:\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/xgRnU89.png](https://dq-content.s3.amazonaws.com/xgRnU89.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Java\n",
    "\n",
    "Spark runs on the Java Virtual Machine, or JVM for short, which comes in the Java SE Development Kit (JDK for short). We recommend installing Java SE Development Kit version 7 or higher, which you can download from Oracle’s website:\n",
    "\n",
    "* http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "\n",
    "As of this writing, Java SE Development Kit 8u111 and 8u112 are the two latest releases of the JDK. Any version after JDK 7 works, so you can download any of the versions on this page. Select the appropriate installation file for your operating system.<br>\n",
    "\n",
    "If you're on Windows or Linux, be sure to choose the correct instruction set architecture (x86 or x64) for your computer. Each computer chip has a specific instruction set architecture that determines the maximum amount of memory it can work with. The two main types are x86 (32 bit) and x64 (64-bit). If you're not sure which one your computer has, you can find out by following [this guide if you're on Windows](http://support.wdc.com/KnowledgeBase/answer.aspx?ID=9405) or [this one if you're on Linux](http://www.howtogeek.com/198615/how-to-check-if-your-linux-system-is-32-bit-or-64-bit/).<br>\n",
    "\n",
    "To verify that the installation worked, launch your command line application (**Command Prompt** for Windows and **Terminal** for Mac and Linux) and run:\n",
    "\n",
    "```python\n",
    "java -version\n",
    "```\n",
    "\n",
    "The output should be similar to:\n",
    "\n",
    "```python\n",
    "java version \"1.7.0_79\"\n",
    "Java(TM) SE Runtime Environment (build 1.7.0_79-b15)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n",
    "```\n",
    "\n",
    "While the exact numbers probably won't match, the key thing to verify is that the version is larger than `1.7`. This number actually represents Version 7. If you're interested, you can read why at [Oracle's website](http://www.oracle.com/technetwork/java/javase/jdk7-naming-418744.html).<br>\n",
    "\n",
    "If running `java -version` returned an error or a different version than the one you just installed, your Java JDK installation most likely wasn't added to your `PATH` properly. Read this post to learn more about how to properly add the Java executable to your `PATH`.<br>\n",
    "\n",
    "Now that we have the JVM set up, let's move on to Spark.\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/HiuPSEj.png](https://dq-content.s3.amazonaws.com/HiuPSEj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Because you've installed JDK, you could technically download the original source code and build Spark on your computer. Building from the source code is the process of generating an executable program for your machine. It involves [many steps](http://stackoverflow.com/a/1622520). While there are some performance benefits to building Spark from source, it takes a while to do, and it's hard to debug if the build fails.<br>\n",
    "\n",
    "We'll download and work with a pre-built version of Spark instead. Navigate to the [Spark downloads page](http://spark.apache.org/downloads.html) and select the following options:\n",
    "\n",
    "* 1.6.2\n",
    "* Pre-built for Hadoop 2.6\n",
    "* Direct Download\n",
    "\n",
    "Next, click the link that appears in Step 4 to download Spark as a `.TGZ` file to your computer. Open your command line application and navigate to the folder you downloaded it to. Unzip the file and move the resulting folder into your home directory. Windows does not have a built in utility that can unzip tgz files - we recommend downloading and using [7-Zip](http://www.7-zip.org/). Once you have unzipped the file, move the resulting folder into your home directory.\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/82TDOgt.png](https://dq-content.s3.amazonaws.com/82TDOgt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Shell\n",
    "\n",
    "In the last mission, you learned that PySpark is a Python library that allows us to interact with Spark objects. The source code for the PySpark library is located in the `python/pyspark` directory, but **the executable version of the library is located in `bin/pyspark`**. To test whether your installation built Spark properly, run the command `bin/pyspark` to start up the PySpark shell. The output should be similar to this:\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/vgMMYkC.png](https://dq-content.s3.amazonaws.com/vgMMYkC.png)\n",
    "\n",
    "While the output is verbose, you can see that the shell automatically initialized the `SparkContext` object and assigned it to the variable sc.<br>\n",
    "\n",
    "You don't have to run `bin/pyspark` from the folder that contains it. Because it's in your home directory, you can use `~/spark-1.6.1-bin-hadoop2.6/bin/pyspark` to launch the PySpark shell from other directories on your machine(Note: replace `1.6.1` with `1.6.2` for newer version users). This way, you can switch to the directory that contains the data you want to use, launch the PySpark shell, and read the data in without having to use its full path. The folder you're in when you launch the PySpark shell will be the local context for working with files in Spark.\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/qCuQs4E.png](https://dq-content.s3.amazonaws.com/qCuQs4E.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook\n",
    "\n",
    "You can make your Jupyter Notebook application aware of Spark in a few different ways. One is to create a configuration file and launch Jupyter Notebook with that configuration. Another is to import PySpark at runtime. We'll focus on the latter approach, so you won't have to restart Jupyter Notebook each time you want to use Spark.<br>\n",
    "\n",
    "First, you'll need to copy the full path to the pre-built Spark folder and set it as a shell environment variable. This way, you can specify Spark's location a single time, and every Python program you write will have access to it. If you move the Spark folder, you can change the path specification once and your code will work just fine.\n",
    "\n",
    "#### Mac / Linux\n",
    "\n",
    "* Use `nano` or another text editor to open your shell environment's configuration file. If you're using the default Terminal application, the file should be in `~/.bash_profile`. If you're using ZSH instead, your configuration file will be in `~/.zshrc`.<br>\n",
    "* Add the following line to the end of the file, replacing `{full path to Spark}` with the actual path to Spark:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=\"{full path to Spark, eg /users/home/jeff/spark-2.0.1-bin-hadoop2.7/}\"\n",
    "```\n",
    "\n",
    "* Exit the text editor and run either `source ~/.bash_profile` or `source ~/.zshrc` so the shell reads in and applies the update you made.\n",
    "\n",
    "\n",
    "#### Windows\n",
    "* If you've never added environment variables, read [this tutorial](http://pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/) before you proceed.\n",
    "* Set the `SPARK_HOME` environment variable to the full path of the Spark folder (e.g. `c:/Users/Jeff/spark-2.0.1-bin-hadoop2.7/`).\n",
    "\n",
    "Next, let's install the [findspark](https://github.com/minrk/findspark) Python library, which looks up the location of PySpark using the environment variable we just set. Use pip to install the findspark library:\n",
    "\n",
    "```bash\n",
    "pip install findspark\n",
    "```\n",
    "\n",
    "Now that we've set up all of the tools we need, let's test the installation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your Installation\n",
    "\n",
    "Download [recent-grads.csv](https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv) to your computer and use the command line to navigate to its location. Start Jupyter Notebook, create a new notebook, and run the following code to test your installation:\n",
    "\n",
    "```python\n",
    "# Find path to PySpark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "​\n",
    "# Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "​\n",
    "# Read `recent-grads.csv` in to an RDD.\n",
    "f = sc.textFile('recent-grads.csv')\n",
    "data = f.map(lambda line: line.split('\\n'))\n",
    "data.take(10)\n",
    "```\n",
    "\n",
    "If you don't get any errors and can see the first 10 lines of `recent-grads.csv`, then you're good to go! You can use Google, StackOverflow, or the members-only Slack community to get help if you need it.\n",
    "\n",
    "![https://dq-content.s3.amazonaws.com/3Ws6xgo.png](https://dq-content.s3.amazonaws.com/3Ws6xgo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
