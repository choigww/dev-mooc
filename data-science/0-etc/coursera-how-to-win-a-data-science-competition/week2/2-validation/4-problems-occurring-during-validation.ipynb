{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems occurring during Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "1. We discussed the concept of validation and overfitting\n",
    "2. We understood how to choose validation strategy\n",
    "3. We leanrned to identify data split made by organizers\n",
    "4. **Validation problems**\n",
    "  - Validation stage\n",
    "    * usually local validation problems are caused by inconsistency of the data\n",
    "    * a widespread example is getting different optimal parameters for different faults\n",
    "    * **In this case we need to make more thorough validation**\n",
    "  - Submission stage\n",
    "    * only when we send our submissions to the platform and find the our validation score and actual score shown in leaderboard are different\n",
    "    * **since we can't mimic the exact train test split on our validation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation stage\n",
    "\n",
    "![holidays](../img/holidays-russia.png)\n",
    "\n",
    "* Consider we need to predict sales in a shop in February.\n",
    "* Say we have target values for the last year\n",
    "* And we will usually take last months in the validation\n",
    "  * This means `January`, but clearly January has much more holidays than February\n",
    "  * and people tend to buy more, which causes target values to be higher in overall\n",
    "  * **and that mean-squared-error of our predictions for January will be greater than for February**\n",
    "  \n",
    "### Does this mean that the model will perform worse for February?\n",
    "* **Probably not** - at least in terms of overfitting\n",
    "\n",
    "### Sometimes this kind of model behavior can be expected\n",
    "* But what if there is no clear reason why scores differ for different folds?\n",
    "* **`Let's identify several common reasons for this and see what we can do about it.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causes of different scores and optimal parameters\n",
    "1. Too little data\n",
    "2. Too diverse and inconsistent data\n",
    "  * **(Quick note)** notice that in this example we can reduce this diversity a bit if we validate on the February from the previous year!\n",
    "  \n",
    "#### We should do extensive validation\n",
    "1. Average scores from different KFold splits (usually K=5 will do)\n",
    "2. Tune model on one KFold splits and evaluate the model on the other KFold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission stage\n",
    "\n",
    "We can observe that:\n",
    "* Leaderboard score is consistently higher/lower that validation score\n",
    "* Leaderboard score is not correlated with validation score at all\n",
    "  * `In the worst case, the higher the validation score is, the lower the leaderboard score becomes.`\n",
    "\n",
    "### `EDA is your friend when it comes down to finding the root of the problems!`\n",
    "\n",
    "### `Remember that the main rule of making a reliable validation is to mimic a train tests pre-made by organizers!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first sort out causes we could observe during validation stage.\n",
    "\n",
    "0. **We may already have quite different scores in Kfold**\n",
    "  - we can calculate the mean and standard deviation of the validation scores and estimate if the leaderboard score is expected. But if this is not the case ...\n",
    "1. **Too little data in public leaderboard**\n",
    "2. **Train and test data are from different distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission stage: different distributions\n",
    "\n",
    "![dist-heights](../img/dist-heights.png)\n",
    "\n",
    "Consider that the train data consists of only women and the test data consists of only men (without labeling).\n",
    "* All the model predictions will around the average height for woman\n",
    "* Our model will have a terrible score on the test data (men)\n",
    "\n",
    "### Simplest way to solve this particular situation in a competition is\n",
    "* to try to **figure out the optimal constant prediction for train and test data**\n",
    "* and **shift your predictions by the difference**\n",
    "* `Mean for train` = calculate from the train data\n",
    "* `Mean for test` = `Leaderboard probing`\n",
    "  * we send two constant submissions\n",
    "  * write down a simple formula\n",
    "  * find out the average target value for the test is equal to 70 inches\n",
    "* Now we get the average value of train and test data - we can get the target values of test data by adding 7 to the train data\n",
    "\n",
    "\n",
    "### Another example\n",
    "The ratios for `men` and `women` target values in train and test data are different.\n",
    "![men-women-ratio](../img/men-women-ratio.png)\n",
    "\n",
    "* If the test data consists mostly of `men`, **force the validation to have the same distribution**\n",
    "* In that case, you ensure that your validatoin will be fair\n",
    "  - This is true for getting raw scores and optimal parameters correctly\n",
    "\n",
    "![men-women-ratio2](../img/men-women-ratio2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission stage - conclusion\n",
    "\n",
    "Causes of validation problems:\n",
    "* too little data in public leaderboard\n",
    "* incorrect train/test split\n",
    "* different distributions in train and test\n",
    "\n",
    "### If you have too little data in public leaderboard, just trust your validation. \n",
    "* If that's not the case, make sure that you did not make model overfit.\n",
    "* Then check if you made correct train/test split as we discussed.\n",
    "* Finally check if you have different distributions in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard Shuffle\n",
    "\n",
    "Say a team ranked 3rd on the private leaderboard but ranked 392nd on the public leaderboard.\n",
    "\n",
    "### Expect LB shuffle becuase of\n",
    "\n",
    "### **Randomness**\n",
    "  * Ex1. easy-to-predict competition\n",
    "    - Scores of competitors were very close; most of them are overfitted.\n",
    "    - Randomness does not make differences to performance much.\n",
    "  * Ex2. hard-to-predict competition\n",
    "    - Financial data in that competition was highly unpredictable\n",
    "    - Randomness does make differences to performance much.\n",
    "  * **`So one could say that the leaderboard shuffle there was among the biggest shuffles on KFold platform`**\n",
    "\n",
    "### **Little amount of data**\n",
    "  * Ex. train set consists of less than 200 gross and test set consists of less than 400 gross.\n",
    "    - As you can see, shuffle here is more than we expected.\n",
    "\n",
    "### **Different public/private distributions**\n",
    "  * Usually for the case with `time-series predictions`\n",
    "  * When we have a time-based split we usually have first few weeks in public leaderboard and next few weeks in private leaderboard\n",
    "  * As people tend to adjust their submission model to public leaderboard and overfit - we can exepct worse results on private leaderboard\n",
    "  * **`Here again, trust your validation and everything will be fine.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* **`If we have big dispersion of scores on validation stage, we should do extensive validation`**\n",
    "  - Average scores from different KFold splits\n",
    "  - Tune model on one split, evaluate on the other\n",
    "* **`If submission's score do not match local validation score, we should`**\n",
    "  - Check if we have too little data in public leaderboard\n",
    "  - Check if we overfitted\n",
    "  - Check if we chose correct splitting strategy\n",
    "  - Check if train/test have different distributions\n",
    "* **`Leaderboard shuffle may occur because of`**\n",
    "  - Randomness\n",
    "  - Little amount of data\n",
    "  - Different public/private distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
