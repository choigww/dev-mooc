{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "1. We will understand the concept of validation and overfitting\n",
    "2. We will identify the number of splits that should be done to establish stable validation\n",
    "3. We will break down most frequent ways to repeat train test split\n",
    "4. We will discuss most often validation problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "![validation-example](img/validation-example.png)\n",
    "![validation-competition](img/validation-competition.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit vs. Overfit\n",
    "\n",
    "![underfit-overfit](img/underfit-overfit.png)\n",
    "\n",
    "### `overfit in general` != `overfit in competition`\n",
    "\n",
    "* In general, we say that the model is `overfitted` if its model performance on the train set is better than on the test set.\n",
    "* But in competition, the model is `overfitted` only in case when performance on the test set will be **worse than we have expected**.\n",
    "  * For example, if Area Under Curve (AUC) for validation set is .9 and AUC for test set is .9 as well, we say it's not `overfitted` in the context of competitions. \n",
    "\n",
    "![cv-errors](img/cv-errors.png)\n",
    "\n",
    "* Training errors always are better than the test error which implies overfitting in general sense\n",
    "  * But does not apply in the context of competitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Validation helps us evaluate a quality of the model\n",
    "2. Validation helps us select model which will perform best on the unseen data\n",
    "3. Underfitting refers to not capturing enough patterns in the data\n",
    "4. Generally, overfitting refers to\n",
    "   - capturing noise\n",
    "   - capturing patterns which do not generalize to test data\n",
    "5. In competitions, overfitting refers to\n",
    "  - low model's quality on test data, which was `unexpected` due to validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
