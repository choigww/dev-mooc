{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b91a74ba-85f4-486e-b5f9-d0898f0626bf",
        "_uuid": "6ac53f18b4f4ec0fc44348cedb5d1c319fa127c0"
      },
      "cell_type": "markdown",
      "source": "### All days of the challange:\n\n* [Day 1: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)\n* [Day 2: Scaling and normalization](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)\n* [Day 3: Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)\n* [Day 4: Character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)\n* [Day 5: Inconsistent Data Entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)\n___\nWelcome to day 4 of the 5-Day Data Challenge! Today, we're going to be working with different character encodings. To get started, click the blue \"Fork Notebook\" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)\n\n> **Your turn!** As we work through this notebook, you'll see some notebook cells (a block of either code or text) that has \"Your Turn!\" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). You can also click in a cell and then click on the right \"play\" arrow to the left of the code. If you want to run all the code in your notebook, you can use the double, \"fast forward\" arrows at the bottom of the notebook editor.\n\nHere's what we're going to do today:\n\n* [Get our environment set up](#Get-our-environment-set-up)\n* [What are encodings?](#What-are-encodings?)\n* [Reading in files with encoding problems](#Reading-in-files-with-encoding-problems)\n* [Saving your files with UTF-8 encoding](#Saving-your-files-with-UTF-8-encoding)\n\nLet's get started!"
    },
    {
      "metadata": {
        "_cell_guid": "5cd5061f-ae30-4837-a53b-690ffd5c5830",
        "_uuid": "9d82bf13584b8e682962fbb96131f2447d741679"
      },
      "cell_type": "markdown",
      "source": "# Get our environment set up\n________\n\nThe first thing we'll need to do is load in the libraries we'll be using. Not our datasets, though: we'll get to those later!\n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!"
    },
    {
      "metadata": {
        "_cell_guid": "135a7804-b5f5-40aa-8657-4a15774e3666",
        "_uuid": "835cbe0834b935fb0fd40c75b9c39454836f4d5f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# helpful character encoding module\nimport chardet\n\n# set seed for reproducibility\nnp.random.seed(0)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "604ac3a4-b1d9-4264-b312-4bbeecdeec00",
        "_uuid": "03ce3b4afe87d98f777172c2c7be066a66a0b237"
      },
      "cell_type": "markdown",
      "source": "Now we're ready to work with some character encodings! (If you like, you can add a code cell here and take this opportunity to take a look at some of the data.)"
    },
    {
      "metadata": {
        "_cell_guid": "52d1b1fb-b71f-4691-9f49-545bf272354d",
        "_uuid": "06093219f80ef491dd51e776a1c0701badaaf67e"
      },
      "cell_type": "markdown",
      "source": "# What are encodings?\n____\n\nCharacter encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding that the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n\næ–‡å—åŒ–ã??\n\nYou might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n\n����������\n\nCharacter encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n\n> UTF-8 is **the** standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble.\n\nIt was pretty hard to deal with encodings in Python 2, but thankfully in Python 3 it's a lot simpler. (Kaggle Kernels only use Python 3.) There are two main data types you'll encounter when working with text in Python 3. One is is the string, which is what text is by default."
    },
    {
      "metadata": {
        "_cell_guid": "579e7b4a-9113-4795-833f-43dfaa7bd223",
        "_uuid": "c93b6c3d188e2174d5060255ea6220f52026d6f2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# check to see what datatype it is\ntype(before)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "str"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e739cbf3e4acea9359dceab3a3017bcb29a4c1b"
      },
      "cell_type": "code",
      "source": "after",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "b'This is the euro symbol: \\xe2\\x82\\xac'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "40283df704bc2ef2441d1048a79fe7fde11c70b3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "411f1c92-beeb-41ae-bf37-689830b18543",
        "_uuid": "3744c3f583a9e2cc71a9dddbd40f875cfe118192"
      },
      "cell_type": "markdown",
      "source": "The other data is the [bytes](https://docs.python.org/3.1/library/functions.html#bytes) data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it's in:"
    },
    {
      "metadata": {
        "_cell_guid": "e2581032-e30d-427a-ade1-e68bd5bbfa26",
        "_uuid": "7abd3230e80d30916c7bb2c89a75268c8d943124",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"utf-8\", errors = \"replace\")\n\n# check the type\ntype(after)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "bytes"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da0d3963e95e9a3c8ec429031190360d00e2c018"
      },
      "cell_type": "code",
      "source": "after",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "b'This is the euro symbol: \\xe2\\x82\\xac'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dad36759f627c5bcb8b99f7bc7b6cdecad7f8f91"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2163421a-27ec-40b7-8064-7a4ddf2ccbb2",
        "_uuid": "561289a2b998601f914ddd548a1f8cc15f6d6452"
      },
      "cell_type": "markdown",
      "source": "If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol  has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
    },
    {
      "metadata": {
        "_cell_guid": "b3aa69d1-7e4a-48a3-b788-a75d71d4dfc4",
        "_uuid": "28337794179e4e4b335983027e60789b4664f0d4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# take a look at what the bytes look like\nafter",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "b'This is the euro symbol: \\xe2\\x82\\xac'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "f56be052-f564-4cea-9aee-e34813a71a3f",
        "_uuid": "4c2e8b76861fb724986a7475cb0979d3bc23276b"
      },
      "cell_type": "markdown",
      "source": "When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
    },
    {
      "metadata": {
        "_cell_guid": "8cc169fb-827e-485a-bb6d-f414a46e6c15",
        "_uuid": "5d904ea4f724652fbad9b786f2c0aa318601b8fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# convert it back to utf-8\nprint(after.decode(\"utf-8\"))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "This is the euro symbol: €\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "ea3bd345-e139-46cf-bf2a-a479887c112b",
        "_uuid": "7ed1ee6a1ae446fc02eb35f01456c9d068fa897d"
      },
      "cell_type": "markdown",
      "source": "However, when we try to use a different encoding to map our bytes into a string,, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n\n> You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work. "
    },
    {
      "metadata": {
        "_cell_guid": "0454daad-f3b4-46bb-986e-e1710e6ec45c",
        "_uuid": "2ae367e4c83d2d1b1a02e288c9ab9d2a409bbddc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# try to decode our bytes with the ascii encoding\nprint(after.decode(\"ascii\"))",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'ascii' codec can't decode byte 0xe2 in position 25: ordinal not in range(128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6329eab61c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try to decode our bytes with the ascii encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 25: ordinal not in range(128)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "7dde2127-bcbe-46a6-8522-66eaef4fde53",
        "_uuid": "c7c9271352fc4564669cf34712096b21fb4b29b7"
      },
      "cell_type": "markdown",
      "source": "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n\nFor example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it *should* have been. That means we may have just made our data unusable!"
    },
    {
      "metadata": {
        "_cell_guid": "abc9901f-4667-4679-a7e1-5589f8cbf7cf",
        "_uuid": "7a54834072c291034ddd3f83292e1a36be01388f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"ascii\", errors = \"replace\")\n\n# convert it back to utf-8\nprint(after.decode(\"ascii\"))\n\n# We've lost the original underlying byte string! It's been \n# replaced with the underlying byte string for the unknown character :(",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "This is the euro symbol: ?\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92c77ba6fe2a81447ad25b2d45a9f5cf07ac8c03"
      },
      "cell_type": "code",
      "source": "before",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "'This is the euro symbol: €'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a36b14815b5df79578ccf95be6269db488afe372"
      },
      "cell_type": "code",
      "source": "after",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "b'This is the euro symbol: ?'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d67fd8f302d58fb18b6dd1e16846bbd3fee78509"
      },
      "cell_type": "code",
      "source": "after.decode(\"ascii\")",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'This is the euro symbol: ?'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "cdc4438f-4e9f-4d06-bbf5-01f2613af790",
        "_uuid": "991c5ca3457deb585ce58bf4ba64d55fe0580ee2"
      },
      "cell_type": "markdown",
      "source": "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8  is when you read in files, which we'll talk about next.\n\nFirst, however, try converting between bytes and strings with different encodings and see what happens. Notice what this does to your text. Would you want this to happen to data you were trying to analyze?"
    },
    {
      "metadata": {
        "_cell_guid": "6a07f260-d0f4-41d1-8511-e45e8a43bf43",
        "_uuid": "1b1f20f32a0647bc65281d3feb1d3124a226ccdd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your turn! Try encoding and decoding different symbols to ASCII and\n# see what happens. I'd recommend $, #, 你好 and नमस्ते but feel free to\n# try other characters. What happens? When would this cause problems?\n\nbefore2 = '你好'\nafter2 = before2.encode('utf', errors = 'replace')\nprint(after2.decode('ascii'))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-141319ecf101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbefore2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'你好'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mafter2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbefore2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "8cdb1777-e518-499c-8941-2c510e1ca785",
        "_uuid": "1970b834a6189e19197ffd4d1ad2c56b1a7c705d"
      },
      "cell_type": "markdown",
      "source": "# Reading in files with encoding problems\n___\n\nMost files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this: "
    },
    {
      "metadata": {
        "_cell_guid": "7f193412-74f3-4b8c-93d3-61997020b922",
        "_uuid": "4833c0ce828c4547d374737f5707401c90ac4597",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")\n\n# Occurring error below\n'''UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte'''",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-534302da9657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try to read in a file not in UTF-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkickstarter_2016\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/kickstarter-projects/ks-projects-201612.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "8e40ef9c-8973-4df8-b307-10a6c592c715",
        "_uuid": "3855e9b70f573aff62a7333001b827b13d349b49"
      },
      "cell_type": "markdown",
      "source": "Notice that we get the same `UnicodeDecodeError` we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually *is* though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. **A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.**\n\nI'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a  large file this can be very slow.) Another reason to just look at the first part of the file is that  we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
    },
    {
      "metadata": {
        "_cell_guid": "86e058c6-e971-4927-a442-0d67fadca013",
        "_uuid": "ef876801a295410c657b2b85ecfef63c8ae0ab09",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# look at the first ten thousand bytes to guess the character encoding\n# using `chardet.detect` method\nwith open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\n\n# check what the character encoding might be\nprint(result)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "eb893685-188a-4de6-9f1e-ab42973135a9",
        "_uuid": "907cafd0d66144e12af2953467021019cd5c2945"
      },
      "cell_type": "markdown",
      "source": "So chardet is 73%  confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:"
    },
    {
      "metadata": {
        "_cell_guid": "25e9e59c-d881-4d91-be0f-4425f5c6583d",
        "_uuid": "5e09150f25216d09065165845414f308d7445ae2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "          ID                                               name   \\\n0  1000002330                    The Songs of Adelaide & Abullah   \n1  1000004038                                     Where is Hank?   \n2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n3  1000011046  Community Film Project: The Art of Neighborhoo...   \n4  1000014025                               Monarch Espresso Bar   \n\n        category  main_category  currency             deadline   goal   \\\n0          Poetry     Publishing       GBP  2015-10-09 11:36:00   1000   \n1  Narrative Film   Film & Video       USD  2013-02-26 00:20:50  45000   \n2           Music          Music       USD  2012-04-16 04:24:11   5000   \n3    Film & Video   Film & Video       USD  2015-08-29 01:00:00  19500   \n4     Restaurants           Food       USD  2016-04-01 13:38:27  50000   \n\n             launched  pledged       state  backers  country  usd pledged   \\\n0  2015-08-11 12:12:28        0      failed        0       GB            0   \n1  2013-01-12 00:20:50      220      failed        3       US          220   \n2  2012-03-17 03:24:11        1      failed        1       US            1   \n3  2015-07-04 08:35:03     1283    canceled       14       US         1283   \n4  2016-02-26 13:38:27    52375  successful      224       US        52375   \n\n  Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16  \n0         NaN         NaN         NaN          NaN  \n1         NaN         NaN         NaN          NaN  \n2         NaN         NaN         NaN          NaN  \n3         NaN         NaN         NaN          NaN  \n4         NaN         NaN         NaN          NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>name</th>\n      <th>category</th>\n      <th>main_category</th>\n      <th>currency</th>\n      <th>deadline</th>\n      <th>goal</th>\n      <th>launched</th>\n      <th>pledged</th>\n      <th>state</th>\n      <th>backers</th>\n      <th>country</th>\n      <th>usd pledged</th>\n      <th>Unnamed: 13</th>\n      <th>Unnamed: 14</th>\n      <th>Unnamed: 15</th>\n      <th>Unnamed: 16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000002330</td>\n      <td>The Songs of Adelaide &amp; Abullah</td>\n      <td>Poetry</td>\n      <td>Publishing</td>\n      <td>GBP</td>\n      <td>2015-10-09 11:36:00</td>\n      <td>1000</td>\n      <td>2015-08-11 12:12:28</td>\n      <td>0</td>\n      <td>failed</td>\n      <td>0</td>\n      <td>GB</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000004038</td>\n      <td>Where is Hank?</td>\n      <td>Narrative Film</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2013-02-26 00:20:50</td>\n      <td>45000</td>\n      <td>2013-01-12 00:20:50</td>\n      <td>220</td>\n      <td>failed</td>\n      <td>3</td>\n      <td>US</td>\n      <td>220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000007540</td>\n      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n      <td>Music</td>\n      <td>Music</td>\n      <td>USD</td>\n      <td>2012-04-16 04:24:11</td>\n      <td>5000</td>\n      <td>2012-03-17 03:24:11</td>\n      <td>1</td>\n      <td>failed</td>\n      <td>1</td>\n      <td>US</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000011046</td>\n      <td>Community Film Project: The Art of Neighborhoo...</td>\n      <td>Film &amp; Video</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2015-08-29 01:00:00</td>\n      <td>19500</td>\n      <td>2015-07-04 08:35:03</td>\n      <td>1283</td>\n      <td>canceled</td>\n      <td>14</td>\n      <td>US</td>\n      <td>1283</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000014025</td>\n      <td>Monarch Espresso Bar</td>\n      <td>Restaurants</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2016-04-01 13:38:27</td>\n      <td>50000</td>\n      <td>2016-02-26 13:38:27</td>\n      <td>52375</td>\n      <td>successful</td>\n      <td>224</td>\n      <td>US</td>\n      <td>52375</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "200d647e-0d92-48b9-b49b-9d48b5f149ba",
        "_uuid": "bbc3c1a70b0d01f314a68b2cf2448cecd8006e89"
      },
      "cell_type": "markdown",
      "source": "Yep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be be fine. \n\n> **What if the encoding chardet guesses isn't right?** Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that."
    },
    {
      "metadata": {
        "_cell_guid": "a52ac0aa-7d47-4442-88e3-d625d7b42934",
        "_uuid": "706e1f985080e9492a47447a34fb9c1203738229",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your Turn! Trying to read in this file gives you an error. Figure out\n# what the correct encoding should be and read in the file. :)\npolice_killings = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\")",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fa9d489ec0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Your Turn! Trying to read in this file gives you an error. Figure out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# what the correct encoding should be and read in the file. :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpolice_killings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 2: invalid start byte"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e56f155dba698b0354cdb5a99fa7026a923104db"
      },
      "cell_type": "code",
      "source": "with open(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", 'rb') as rawdata:\n    first10000 = chardet.detect(rawdata.read(10000))\n    first20000 = chardet.detect(rawdata.read(20000))\n    \nprint(first10000)\nprint(first20000)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c712078c581e10646c353ce23b5e04faea2ef7c4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6d19c8d54d88347663241cea51074e3bfd52094"
      },
      "cell_type": "code",
      "source": "policekilling_us = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", \n                              encoding = 'Windows-1252')\npolicekilling_us.head(5)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "   id                name      date   manner_of_death       armed   age  \\\n0   3          Tim Elliot  02/01/15              shot         gun  53.0   \n1   4    Lewis Lee Lembke  02/01/15              shot         gun  47.0   \n2   5  John Paul Quintero  03/01/15  shot and Tasered     unarmed  23.0   \n3   8     Matthew Hoffman  04/01/15              shot  toy weapon  32.0   \n4   9   Michael Rodriguez  04/01/15              shot    nail gun  39.0   \n\n  gender race           city state  signs_of_mental_illness threat_level  \\\n0      M    A        Shelton    WA                     True       attack   \n1      M    W          Aloha    OR                    False       attack   \n2      M    H        Wichita    KS                    False        other   \n3      M    W  San Francisco    CA                     True       attack   \n4      M    H          Evans    CO                    False       attack   \n\n          flee  body_camera  \n0  Not fleeing        False  \n1  Not fleeing        False  \n2  Not fleeing        False  \n3  Not fleeing        False  \n4  Not fleeing        False  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>date</th>\n      <th>manner_of_death</th>\n      <th>armed</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>race</th>\n      <th>city</th>\n      <th>state</th>\n      <th>signs_of_mental_illness</th>\n      <th>threat_level</th>\n      <th>flee</th>\n      <th>body_camera</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Tim Elliot</td>\n      <td>02/01/15</td>\n      <td>shot</td>\n      <td>gun</td>\n      <td>53.0</td>\n      <td>M</td>\n      <td>A</td>\n      <td>Shelton</td>\n      <td>WA</td>\n      <td>True</td>\n      <td>attack</td>\n      <td>Not fleeing</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Lewis Lee Lembke</td>\n      <td>02/01/15</td>\n      <td>shot</td>\n      <td>gun</td>\n      <td>47.0</td>\n      <td>M</td>\n      <td>W</td>\n      <td>Aloha</td>\n      <td>OR</td>\n      <td>False</td>\n      <td>attack</td>\n      <td>Not fleeing</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>John Paul Quintero</td>\n      <td>03/01/15</td>\n      <td>shot and Tasered</td>\n      <td>unarmed</td>\n      <td>23.0</td>\n      <td>M</td>\n      <td>H</td>\n      <td>Wichita</td>\n      <td>KS</td>\n      <td>False</td>\n      <td>other</td>\n      <td>Not fleeing</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>Matthew Hoffman</td>\n      <td>04/01/15</td>\n      <td>shot</td>\n      <td>toy weapon</td>\n      <td>32.0</td>\n      <td>M</td>\n      <td>W</td>\n      <td>San Francisco</td>\n      <td>CA</td>\n      <td>True</td>\n      <td>attack</td>\n      <td>Not fleeing</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>Michael Rodriguez</td>\n      <td>04/01/15</td>\n      <td>shot</td>\n      <td>nail gun</td>\n      <td>39.0</td>\n      <td>M</td>\n      <td>H</td>\n      <td>Evans</td>\n      <td>CO</td>\n      <td>False</td>\n      <td>attack</td>\n      <td>Not fleeing</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9f6947640647aebdd592970a53e933ebf77d4ffe"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "02f54e31-ec04-425b-9fcd-d85b11742f8e",
        "_uuid": "51e11611fae94a9f704440275905e719ee801a0a"
      },
      "cell_type": "markdown",
      "source": "# Saving your files with UTF-8 encoding\n___\n\nFinally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"
    },
    {
      "metadata": {
        "_cell_guid": "affcfb28-b6a8-426a-b690-0c717073ad09",
        "_uuid": "8f72b89b5ea80a1fc9890c3eac89614757c16b47",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "036fb925-21ec-4b15-8eac-4d29879003f5",
        "_uuid": "5ff9d834741ea3f9a6ed0276dc02b7a59948ae1e"
      },
      "cell_type": "markdown",
      "source": "Pretty easy, huh? :)\n\n> If you haven't saved a file in a kernel before, you need to hit the commit & run button and wait for your notebook to finish running first before you can see or access the file you've saved out. If you don't see it at first, wait a couple minutes and it should show up. The files you save will be in the directory \"../output/\", and you can download them from your notebook."
    },
    {
      "metadata": {
        "_cell_guid": "a56266f2-b957-459b-a2d4-3587b6637e70",
        "_uuid": "b6610da48bc0a43c5934b4970c394e9de596fe97",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your turn! Save out a version of the police_killings dataset with UTF-8 encoding \npolicekilling_us.to_csv('police-killing-us-utf8.csv')",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b4f37fce-4d08-409e-bbbd-6a26c3bbc6ee",
        "_uuid": "52b0af56e3c77db96056e9acd785f8f435f7caf5"
      },
      "cell_type": "markdown",
      "source": "And that's it for today! We didn't do quite as much coding, but  take my word for it: if you don't have the right tools, figuring out what encoding a file is in can be a huge time sink. If you have any questions, be sure to post them in the comments below or [on the forums](https://www.kaggle.com/questions-and-answers). \n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also lets you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\".\n\n# More practice!\n___\n\nCheck out [this dataset of files in different character encodings](https://www.kaggle.com/rtatman/character-encoding-examples). Can you read in all the files with their original encodings and them save them out as UTF-8 files?\n\nIf you have a file that's in UTF-8 but has just a couple of weird-looking characters in it, you can try out the [ftfy module](https://ftfy.readthedocs.io/en/latest/#) and see if it helps. "
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}