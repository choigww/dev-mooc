{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data Set\n",
    "Over the past two missions, we learned about how decision trees are constructed. We used a modified version of ID3, which is a bit simpler than the most common tree building algorithms, C4.5 and CART. The basics are the same, however, so we can apply what we learned about how decision trees work to any tree construction algorithm.<br>\n",
    "\n",
    "In this mission, we'll learn about when to use decision trees, and how to use them most effectively.<br>\n",
    "\n",
    "We'll continue using the 1994 census data on U.S. incomes we worked with in the previous mission. It contains information on marital status, age, type of work, and more. The target column, `high_income`, indicates an income of less than or equal to 50k a year (`0`), or more than 50k a year (`1`).<br>\n",
    "\n",
    "You can download the data from the [University of California, Irvine's website](http://archive.ics.uci.edu/ml/datasets/Adult)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Decision Trees With scikit-learn\n",
    "\n",
    "We can use the [scikit-learn](http://scikit-learn.org/) package to fit a decision tree. The interface is very similar to other algorithms we've fit in the past.<br>\n",
    "\n",
    "We use the [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) class for classification problems, and [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) for regression problems. The `sklearn.tree` package includes both of these classes.<br>\n",
    "\n",
    "In this case, we're predicting a binary outcome, so we'll use a classifier.<br>\n",
    "\n",
    "The first step is to train the classifier on the data. We'll use the `fit` method on a classifier to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit `clf` to the `income` data.\n",
    "\n",
    "* Pass in `income[columns]` so that we only use the named columns as predictors.\n",
    "* The target is the `high_income` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Categorical.from_array is deprecated, use Categorical instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "income = pd.read_csv('data/income.csv')\n",
    "\n",
    "# A list of columns to train with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "for col in columns+['high_income']:\n",
    "    converted = pd.Categorical.from_array(income[col])\n",
    "    income[col] = converted.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt   education  education_num  marital_status  \\\n",
       "0   22          7   77516   Bachelors             12               4   \n",
       "1   33          6   83311   Bachelors             12               2   \n",
       "2   21          4  215646     HS-grad              8               0   \n",
       "3   36          4  234721        11th              6               2   \n",
       "4   11          4  338409   Bachelors             12               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours_per_week  native_country  high_income  \n",
       "0              39              39            0  \n",
       "1              12              39            0  \n",
       "2              39              39            0  \n",
       "3              39              39            0  \n",
       "4              39               5            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to make sure the results are consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# We've already loaded the variable \"income,\" which contains all of the income data\n",
    "clf.fit(income[columns], income['high_income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Train and Test Sets\n",
    "\n",
    "Now that we've fit a model, we can make predictions. We'll want to split our data into training and testing sets first. If we don't, we'll be making predictions on the same data that we train our algorithm with. This leads to overfitting, and will make our error appear lower than it is.<br>\n",
    "\n",
    "While we covered overfitting in more depth in previous missions, here's a quick recap. If you memorize how to perform three specific addition problems (`2+2`, `3+6`, `3+3`), you'll get those specific problems correct every time.<br>\n",
    "\n",
    "On the other hand, if someone asks you what `4+4` is, you won't know how to do it, because you don't know the rules of addition. If you learn the rules of addition, you'll get problems wrong sometimes (because operations like `3443343434+24344343` can be hard to do mentally). Even so, you'll be able to do any problem, and you'll get most of them right.<br>\n",
    "\n",
    "The first example represents overfitting, where you memorize the details of the training set, but can't generalize to new examples you're asked to make predictions on.<br>\n",
    "\n",
    "We can avoid overfitting by always making predictions and evaluating error on data that we haven't trained our algorithm with. This will show us when we're overfitting by giving us a realistic error on data that the algorithm hasn't seen before.<br>\n",
    "\n",
    "We can split the data by shuffling the order of the dataframe, then selecting certain rows to include in the training set, and certain rows to include in the testing set.<br>\n",
    "\n",
    "In this case, we'll make `80%` of our rows training data, and the rest testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the rows in `income` with a position up to `train_max_row` (but not including it) will be part of the training set.\n",
    "\n",
    "* Make a new dataframe called `train` containing all of these rows.\n",
    "* Make a dataframe called `test` containing all of the rows with a position greater than or equal to `train_max_row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed so the shuffle is the same every time\n",
    "np.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using np.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Error with AUC\n",
    "\n",
    "While there are many methods for evaluating error with classification, we'll use AUC, which we've already covered extensively in the machine learning material. [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve) ranges from `0` to `1`, so it's ideal for binary classification. The higher the AUC, the more accurate our predictions.<br>\n",
    "\n",
    "We can compute AUC with the [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) function from `sklearn.metrics`. This function takes in two parameters:\n",
    "* `y_true`: true labels\n",
    "* `y_score`: predicted labels\n",
    "\n",
    "It then calculates and returns the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the AUC between `predictions` and the `high_income` column of `test`, and assign the result to `error`.\n",
    "* Use the `print` function to display `error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693568175543\n"
     ]
    }
   ],
   "source": [
    "error = roc_auc_score(test['high_income'], predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Error on the Training Set\n",
    "\n",
    "The AUC for the predictions on the testing set is about `.694`. Let's compare this against the AUC for predictions on the training set to see if the model is overfitting.<br>\n",
    "\n",
    "**It's normal for the model to predict the training set better than the testing set**. After all, it has full knowledge of that data and the outcomes. \n",
    "* However, if the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print out the AUC score between `predictions` and the `high_income` column of `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947124450144\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train['high_income'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Overfitting\n",
    "\n",
    "Our AUC on the training set was `.947`, and the AUC on the test set was `.694`. There's no hard and fast rule on when overfitting is occurring, but our model is predicting the training set much better than the test set. Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect and fix it.<br>\n",
    "\n",
    "Based on our AUC measurements, it appears that we are in fact overfitting. Let's take a closer look at why decision trees might overfit.<br>\n",
    "\n",
    "In the last mission, we looked at this data:\n",
    "\n",
    "```python\n",
    "high_income    age    marital_status\n",
    "0              20     0\n",
    "0              60     2\n",
    "0              40     1\n",
    "1              25     1\n",
    "1              35     2\n",
    "1              55     1\n",
    "```\n",
    "\n",
    "Here's the full diagram for the decision tree we can build from it:\n",
    "\n",
    "![](img/11.png)\n",
    "\n",
    "This tree predicts all of our values perfectly. It will always get a right answer on the training set, but this is equivalent to memorizing the rules of addition. While we've built our tree in such a way that it can perfectly predict the training set, the way it's constructed doesn't make sense when we take a step back.<br>\n",
    "\n",
    "That's because the tree above is saying that:\n",
    "\n",
    "* If you're under `22.5` years old, you have a low income\n",
    "* If you're `22.5` - `37.5`, you have a high income\n",
    "* If you're `37.5` - `47.5`, you have a low income\n",
    "* If you're `47.5` to `55`, you have a high income\n",
    "* Finally, if you're above `55`, you have a low income\n",
    "\n",
    "These rules are very specific to the training set.<br>\n",
    "\n",
    "Think about the problem with a real-world lens. Does it make sense to predict that someone who's `20` has a low income, someone who's `25` has a high income, and someone who's `40` has a low income? Intuitively, we know that younger people tend to earn less, middle-aged people earn more, and people who have retired earn less.<br>\n",
    "\n",
    "Our tree has created so many age-based splits in an attempt to perfectly predict everyone's income that each split is effectively meaningless.<br>\n",
    "\n",
    "Here's a tree that matches up with our intuition better:\n",
    "\n",
    "![](img/12.png)\n",
    "\n",
    "All we've done is \"pruned\" the tree, and removed some of the lower leaves. We've turned some of the higher-level nodes into leaves instead.<br>\n",
    "\n",
    "The tree above makes more intuitive sense. If you're under `25`, we predict low income. If you're between `25` and `55`, we predict high income (the `.66` rounds up to 1). If you're above `55`, we predict low income.<br>\n",
    "\n",
    "This version actually has lower accuracy on our training set, but will generalize to new examples better because it matches reality more closely.<br>\n",
    "\n",
    "Trees overfit when they have too much depth and make overly complex rules that match the training data, but aren't able to generalize well to new data. This may seem to be a strange principle at first, but the deeper a tree is, the worse it typically performs on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting With a Shallower Tree\n",
    "\n",
    "There are three main ways to combat overfitting:\n",
    "\n",
    "1. \"Prune\" the tree after we build it to remove unnecessary leaves.\n",
    "2. Use ensembling to blend the predictions of many trees.\n",
    "3. Restrict the depth of the tree while we're building it.\n",
    "\n",
    "While we'll explore all of these, we'll look at the third method first.<br>\n",
    "\n",
    "Limiting tree depth during the building process will result in more general rules. This prevents the tree from overfitting.<br>\n",
    "\n",
    "We can restrict tree depth by adding a few parameters when we initialize the `DecisionTreeClassifier` class:\n",
    "\n",
    "* `max_depth` - Globally restricts how deep the tree can go\n",
    "* `min_samples_split` - The minimum number of rows a node should have before it can be split; if this is set to 2, for example, then nodes with 2 rows won't be split, and will become leaves instead\n",
    "* `min_samples_leaf` - The minimum number of rows a leaf must have\n",
    "* `min_weight_fraction_leaf` - The fraction of input rows a leaf must have\n",
    "* `max_leaf_nodes` - The maximum number of total leaves; this will cap the count of leaf nodes as the tree is being built\n",
    "\n",
    "Some of these parameters aren't compatible, however. For example, we can't use `max_depth` and `max_leaf_nodes` together.<br>\n",
    "\n",
    "Now that we know what to tweak, let's improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set `min_samples_split` to `13` when creating the `DecisionTreeClassifier`.\n",
    "* Make predictions on the training set, compute the AUC, and assign it to `train_auc`.\n",
    "* Make predictions on the test set, compute the AUC, and assign it to `test_auc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees model from the last screen\n",
    "clf = DecisionTreeClassifier(random_state=1,\n",
    "                            min_samples_split = 13)\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "pred = clf.predict(train[columns])\n",
    "\n",
    "train_auc = roc_auc_score(train['high_income'], pred)\n",
    "test_auc = roc_auc_score(test['high_income'],\n",
    "                        clf.predict(test[columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking Parameters to Adjust AUC\n",
    "\n",
    "By setting `min_samples_split` to `13`, we managed to boost the test AUC from `.694` to `.700`. The training set AUC decreased from `.947` to `.843`, showing that the model we built was less overfit to the training set than before:\n",
    "\n",
    "settings|train AUC|test AUC\n",
    "---|---|---\n",
    "default|0.947|0.694\n",
    "min_sample_split:13|0.843|0.700\n",
    "\n",
    "Let's play around with parameters some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set `max_depth` to `7` and `min_samples_split` to `13` when creating the `DecisionTreeClassifier`.\n",
    "* Make predictions on the training set, compute the AUC, and assign it to `train_auc`.\n",
    "* Make predictions on the test set, compute the AUC, and assign it to `test_auc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693568175543\n",
      "0.947124450144\n"
     ]
    }
   ],
   "source": [
    "# The first decision trees model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655313848188\n",
      "0.662450804216\n"
     ]
    }
   ],
   "source": [
    "# The first decision trees model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1,\n",
    "                            min_samples_split=100,\n",
    "                            max_depth=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting In Simplistic Trees\n",
    "\n",
    "Our accuracy went down on the last screen, relative to the screen before it:\n",
    "\n",
    "settings|train AUC|test AUC\n",
    "---|---|---\n",
    "default(min_samples_split: 2, max_depth: None)|0.947|0.694\n",
    "min_samples_split: 13|0.843|0.700\n",
    "min_samples_split: 13, max_depth: 7|0.748|0.7744\n",
    "min_samples_split: 100, max_depth: 2|0.662|0.655\n",
    "\n",
    "This is because we're now [underfitting](http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted). Underfitting is what occurs when our model is too simple to explain the relationships between the variables.<br>\n",
    "\n",
    "Let's go back to our tree diagram to explain underfitting.<br>\n",
    "\n",
    "Here's the data:\n",
    "\n",
    "```python\n",
    "high_income    age    marital_status\n",
    "0              20     0\n",
    "0              60     2\n",
    "0              40     1\n",
    "1              25     1\n",
    "1              35     2\n",
    "1              55     1\n",
    "```\n",
    "\n",
    "And here's the \"right fit\" tree. This tree explains the data properly, without overfitting:\n",
    "\n",
    "![](img/13.png)\n",
    "\n",
    "Let's trim this tree even more to show what happens when the model isn't complex enough to explain the data:\n",
    "\n",
    "![](img/14.png)\n",
    "\n",
    "This model will predict that anyone under `37.5` has a high income (`.66` rounds up), and anyone over `37.5` has a low income (`.33` rounds down). It's too simple to model reality, in which people earn less while they're young, more during middle age, and less again after retirement.<br>\n",
    "\n",
    "Therefore, this tree underfits the data and will have lower accuracy than the appropriate version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff\n",
    "\n",
    "By artificially restricting the depth of our tree, we prevent it from creating a model that's complex enough to correctly categorize some of the rows. If we don't perform the artificial restrictions, however, the tree becomes too complex, fits quirks in the data that only exist in the training set, and doesn't generalize to new data.<br>\n",
    "\n",
    "This is known as the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Imagine that we take a random sample of training data and create many models. \n",
    "* If the models' predictions for the same row are far apart from each other, we have high variance. \n",
    "* Imagine this time that we take a random sample of the training data and create many models. If the models' predictions for the same row are close together but far from the actual value, then we have high bias.<br>\n",
    "\n",
    "### High bias can cause underfitting \n",
    "-- if a model is **consistently failing to predict** the correct value, it may be that it's **too simple to model the data** faithfully.<br>\n",
    "\n",
    "### High variance can cause overfitting. \n",
    "If a model varies its predictions significantly based on small changes in the input data, then it's likely fitting itself to quirks in the training data, **rather than making a generalizable model**.<br>\n",
    "\n",
    "We call this the bias-variance tradeoff because decreasing one characteristic will usually increase the other. This is a limitation of all machine learning algorithms. If you'd like to read more about the tradeoff, check out [programmer Scott Fortmann-Roe's post on this topic](http://scott.fortmann-roe.com/docs/BiasVariance.html).<br>\n",
    "\n",
    "Decision trees typically suffer from high variance. The entire structure of a decision tree can change if we make a minor alteration to its training data. **By restricting the depth of the tree, we increase the bias and decrease the variance**. If we restrict the depth too much, we increase bias to the point where it will underfit.<br>\n",
    "\n",
    "You'll generally need to use your intuition and manually tweak parameters to get the \"right\" fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Decision Tree Variance\n",
    "\n",
    "We can induce variance and see what happens with a decision tree. To introduce noise into the data, we'll add a column of random values. A model with high variance (like a decision tree) will pick up on this noise, and overfit to it. This is because models with high variance are very sensitive to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the classifier to the training data.\n",
    "\n",
    "* Make predictions on the training set, compute the AUC, and assign it to `train_auc`.\n",
    "\n",
    "* Make predictions on the test set, compute the AUC, and assign it to `test_auc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691406001394\n",
      "0.975076161435\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Generate a column containing random numbers from 0 to 4\n",
    "income[\"noise\"] = np.random.randint(4, size=income.shape[0])\n",
    "\n",
    "# Adjust \"columns\" to include the noise column\n",
    "columns = [\"noise\", \"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Make new train and test sets\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Leaves to Prevent Overfitting\n",
    "\n",
    "As you can see above, the random noise column causes significant overfitting. Our test set accuracy decreases to `.691`, and our training set accuracy increases to `.975`.<br>\n",
    "\n",
    "One way to prevent overfitting is to block the tree from growing beyond a certain depth (we tried this before). \n",
    "\n",
    "### Another technique is called pruning. \n",
    "\n",
    "Pruning involves \n",
    "* building a **full tree**, \n",
    "* then **removing the leaves that don't add to prediction accuracy**. \n",
    "\n",
    "Pruning prevents a model from becoming overly complex. It can result in a simpler model that has higher accuracy on the testing set.<br>\n",
    "\n",
    "Data scientists use pruning less often than parameter optimization (what we just did) and ensembling. It's still an important technique, though, and we'll cover it in more depth down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing When to Use Decision Trees\n",
    "\n",
    "Let's go over the main advantages and disadvantages of using decision trees. The main advantages of using decision trees is that they're:\n",
    "\n",
    "* Easy to interpret\n",
    "* Relatively fast to fit and make predictions\n",
    "* Able to handle multiple types of data\n",
    "* Able to pick up nonlinearities in data, and usually fairly accurate\n",
    "\n",
    "The main disadvantage of using decision trees is their **tendency to overfit**.<br>\n",
    "\n",
    "Decision trees are a good choice for tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing.<br>\n",
    "\n",
    "The most powerful way to reduce decision tree overfitting is to **create ensembles of trees**. The [random forest](https://en.wikipedia.org/wiki/Random_forest) algorithm is a popular choice for doing this. In cases where prediction accuracy is the most important consideration, random forests usually perform better.<br>\n",
    "\n",
    "In the next mission, we'll explore the random forest algorithm in greater depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
