{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the previous mission, we learned how to read JSON into a Spark DataFrame, as well as some basic techniques for interacting with DataFrames. In this mission, we'll learn how to use Spark's SQL interface to query and interact with the data. We'll continue to work with the 2010 U.S. Census data set in this mission. Later on, we'll add other files to demonstrate how to take advantage of SQL to work with multiple data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a Table\n",
    "\n",
    "Before we can write and run SQL queries, we need to tell Spark to treat the DataFrame as a SQL table. Spark internally maintains a virtual database within the SQLContext object. This object, which we enter as `sqlCtx`, has methods for registering temporary tables.<br>\n",
    "\n",
    "To register a DataFrame as a table, call the [`registerTempTable()` method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable) on that DataFrame object. This method requires one string parameter, `name`, that we use to set the table name for reference in our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the [`registerTempTable()` method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable) to register the DataFrame `df` as a table named `census2010`.\n",
    "* Then, run the SQLContext method `tableNames` to return the list of tables.\n",
    "  * Assign the resulting list to `tables`, and use the `print` function to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"data/census_2010.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SQLContext.tableNames of <pyspark.sql.context.SQLContext object at 0x1037bf4e0>>\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('census2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['census2010']\n"
     ]
    }
   ],
   "source": [
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "Now that we've registered the table within `sqlCtx`, we can start writing and running SQL queries. With Spark SQL, we represent our query as a string and pass it into the `sql()` method within the SQLContext object. The [`sql()` method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql) requires a single parameter, the query string. Spark will return the query results as a DataFrame object. This means you'll have to use `show()` to display the results, due to lazy loading.<br>\n",
    "\n",
    "While SQLite requires that queries end with a semi-colon, Spark SQL will actually throw an error if you include it. Other than this difference in syntax, Spark's flavor of SQL is identical to SQLite, and all the queries you've written for the [SQL course](https://www.dataquest.io/section/databases-sql) will work here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a SQL query that returns the `age` column from the table `census2010`, and use the `show()` method to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'select age from census2010'\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "In the previous mission, we used DataFrame methods to find all of the rows where `age` was greater than `5`. If we only wanted to retrieve data from the `males` and `females` columns where that criteria were true, we'd need to chain additional operations to the Spark DataFrame. To return the results in descending order instead of ascending order, we'd have to chain another method. The DataFrame methods are quick and powerful for simple queries, but chaining them can be cumbersome for more advanced queries.<br>\n",
    "\n",
    "SQL shines at expressing complex logic in a more compact manner. Let's brush up on SQL by writing a query that expresses more specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and run a SQL query that returns:\n",
    "* The `males` and `females` columns (in that order) where `age` > 5 and `age` < 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  males|females|\n",
      "+-------+-------+\n",
      "|2093905|2007781|\n",
      "|2097080|2010281|\n",
      "|2101670|2013771|\n",
      "|2108014|2018603|\n",
      "|2114217|2023289|\n",
      "|2118390|2026352|\n",
      "|2132030|2037286|\n",
      "|2159943|2060100|\n",
      "|2195773|2089651|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''select males, females\n",
    "           from census2010\n",
    "           where age > 5 and age < 15\n",
    "'''\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Functionality\n",
    "\n",
    "Because the results of SQL queries are DataFrame objects, **we can combine the best aspects of both DataFrames and SQL to enhance our workflow**. For example, we can write a SQL query that quickly returns a subset of our data as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a SQL query that returns a DataFrame containing the `males` and `females` columns from the `census2010` table.\n",
    "* Use the [`describe()` method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) to calculate summary statistics for the DataFrame and the `show()` method to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|             males|          females|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|               101|              101|\n",
      "|   mean|1520095.3168316833|1571460.287128713|\n",
      "| stddev|  818587.208016823|748671.0493484351|\n",
      "|    min|              4612|            25673|\n",
      "|    max|           2285990|          2331572|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "        select males, females\n",
    "        from census2010\n",
    "'''\n",
    "\n",
    "sqlCtx.sql(query).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple tables\n",
    "\n",
    "One of the most powerful use cases in SQL is **joining tables**. Spark SQL takes this a step further by enabling you to run join queries across data from multiple file types. Spark will read any of the file types and formats it supports into DataFrame objects and we can register each of these as tables within the SQLContext object to use for querying.<br>\n",
    "\n",
    "As we mentioned briefly in the previous mission, **most data science organizations use a variety of file formats and data storage mechanisms**. Spark SQL was built with the industry use cases in mind and enables data professionals to use one common query language, SQL, to interact with lots of different data sources. We'll explore joins in Spark SQL further, but first let's introduce the other datasets we'll be using:\n",
    "\n",
    "* `census_1980.json` - 1980 U.S. Census data\n",
    "* `census_1990.json` - 1990 U.S. Census data\n",
    "* `census_2000.json` - 2000 U.S. Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read these additional datasets into DataFrame objects and then use the `registerTempTable()` function to register these tables individually within SQLContext:\n",
    "* `census_1980.json` as `census1980`,\n",
    "* `census_1990.json` as `census1990`,\n",
    "* `census_2000.json` as `census2000`.\n",
    "\n",
    "Then use the method `tableNames()` to list the tables within the SQLContext object, assign to `tables`, and finally print `tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous codes to read\n",
    "\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlCtx = SQLContext(sc)\n",
    "#df = sqlCtx.read.json(\"census_2010.json\")\n",
    "#df.registerTempTable('census2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlCtx.read.json('data/census_1980.json')\n",
    "df.registerTempTable('census1980')\n",
    "\n",
    "df = sqlCtx.read.json('data/census_1990.json')\n",
    "df.registerTempTable('census1990')\n",
    "\n",
    "df = sqlCtx.read.json('data/census_2000.json')\n",
    "df.registerTempTable('census2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['census1980', 'census1990', 'census2000', 'census2010']\n"
     ]
    }
   ],
   "source": [
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Now that we have a table for each dataset, we can write join queries to compare values across them. Since we're working with Census data, let's use the `age` column as the joining column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a query that returns a DataFrame with the `total` columns for the tables `census2010` and `census2000` (in that order).\n",
    "* Then, run the query and use the `show()` method to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  total|  total|\n",
      "+-------+-------+\n",
      "|4079669|3733034|\n",
      "|4085341|3825896|\n",
      "|4089295|3904845|\n",
      "|4092221|3970865|\n",
      "|4094802|4024943|\n",
      "|4097728|4068061|\n",
      "|4101686|4101204|\n",
      "|4107361|4125360|\n",
      "|4115441|4141510|\n",
      "|4126617|4150640|\n",
      "|4137506|4152174|\n",
      "|4144742|4145530|\n",
      "|4169316|4139512|\n",
      "|4220043|4138230|\n",
      "|4285424|4137982|\n",
      "|4347028|4133932|\n",
      "|4410804|4130632|\n",
      "|4451147|4111244|\n",
      "|4454165|4068058|\n",
      "|4432260|4011192|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "        select census2010.total, census2000.total\n",
    "        from census2010\n",
    "        inner join census2000\n",
    "        on census2010.age = census2000.age\n",
    "'''\n",
    "sqlCtx.sql(query).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Functions\n",
    "\n",
    "The functions and operators from SQLite that we've used in the past are available for us to use in Spark SQL:\n",
    "\n",
    "* COUNT()\n",
    "* AVG()\n",
    "* SUM()\n",
    "* AND\n",
    "* OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that calculates the sums of the `total` column from each of the tables, in the following order:\n",
    "* `census2010`,\n",
    "* `census2000`,\n",
    "* `census1990`.\n",
    "\n",
    "You'll need to perform two inner joins for this query (all datasets have the same values for `age`, which makes things convenient for joining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|sum(total)|sum(total)|sum(total)|\n",
      "+----------+----------+----------+\n",
      "| 312247116| 284594395| 254506647|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "        select sum(census2010.total),\n",
    "                sum(census2000.total),\n",
    "                sum(census1990.total)\n",
    "        from census2010\n",
    "        inner join census2000 \n",
    "        on census2010.age=census2000.age\n",
    "        inner join census1990\n",
    "        on census2010.age=census1990.age\n",
    "'''\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
