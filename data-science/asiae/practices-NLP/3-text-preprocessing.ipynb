{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informative-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-positive",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "\n",
    "## NLTK word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "metropolitan-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/choigww/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.'\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-operation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stone-possibility",
   "metadata": {},
   "source": [
    "## 신문기사 웹페이지 html parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seven-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vocal-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "curious-accessory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"en\"><head><title>What Drove The AI Renaissance?</title><meta charset=\"utf-8\"><meta http-equiv=\"Content-Language\" content=\"en_US\"><link rel=\"shortcut icon\" href=\"https://i.forbesimg.com/48X48-F.png\"><meta name=\"referrer\" content=\"no-referrer-when-downgrade\"><link rel=\"canonical\" itemprop=\"url\" href=\"https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/\"><link rel=\"amphtml\" href=\"https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/amp/\"><link rel=\"alternate\" type=\"application/rss+xml\" title=\"What Drove The AI Renaissance? - RSS\" href=\"https://www.forbes.com/sites/adrianbridgwater/feed/\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1,user-scalable=yes\"><meta name=\"description\" itemprop=\"description\" content=\"The current renaissance of Artificial Intelligence (AI) with its sister discipline Machine Learning (ML) has led every IT firm worth its s'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extreme-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# css selector\n",
    "eng_news = soup.select('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-uruguay",
   "metadata": {},
   "source": [
    "## 신문기사 웹페이지 p태그 텍스트 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trained-blade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_news[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elect-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(eng_news[3].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "invalid-museum",
   "metadata": {},
   "source": [
    "## WordPunctTokenizer\n",
    "- Tokenize a text into a sequence of alphabetic and\n",
    "non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "czech-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.'\n",
    "word_punct_tokens = WordPunctTokenizer().tokenize(text)\n",
    "print(word_punct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-northeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polish-script",
   "metadata": {},
   "source": [
    "## TreebankWordTokenizer\n",
    "- The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
    "This is the method that is invoked by ``word_tokenize()``.  It assumes that the\n",
    "text has already been segmented into sentences, e.g. using ``sent_tokenize()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hidden-reader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.'\n",
    "word_treebank_tokens = TreebankWordTokenizer().tokenize(text)\n",
    "print(word_treebank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-enlargement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "handy-interaction",
   "metadata": {},
   "source": [
    "## Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "everyday-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/choigww/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aerial-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('.', '.'), ('Please', 'NNP'), ('buy', 'VB'), ('me', 'PRP'), ('two', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "taggedToken = pos_tag(word_tokens)\n",
    "print(taggedToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-ratio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "plastic-cookie",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recorded-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/choigww/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/choigww/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "activated-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Good/JJ)\n",
      "  muffins/NNS\n",
      "  cost/VBP\n",
      "  $/$\n",
      "  3.88/CD\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ./.\n",
      "  Please/NNP\n",
      "  buy/VB\n",
      "  me/PRP\n",
      "  two/CD\n",
      "  of/IN\n",
      "  them/PRP\n",
      "  ./.\n",
      "  Thanks/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "neToken = ne_chunk(taggedToken)\n",
    "print(neToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-major",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-hammer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "played-fourth",
   "metadata": {},
   "source": [
    "## 원형복원\n",
    "\n",
    "nltk document > stem 검색 (영문 stemmer 굉장히 많음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aboriginal-victor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem(word='running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "periodic-entertainment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(word='beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tested-convergence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(word='believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "thirty-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/choigww/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "combined-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "wl.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "uniform-surprise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "operating-hughes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ha'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize('has')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "formal-impact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "binding-local",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT']\n",
    "word = []\n",
    "\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        word.append(tag[0])\n",
    "    \n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-xerox",
   "metadata": {},
   "source": [
    "## Tagger\n",
    "- Komoran\n",
    "- Hannanum\n",
    "- Okt\n",
    "- Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brown-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "\n",
    "komoran_tokens = komoran.morphs(kor_text)\n",
    "print(komoran_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "herbal-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "hannanum_tokens = hannanum.morphs(kor_text)\n",
    "print(hannanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "developmental-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt_tokens = okt.morphs(kor_text)\n",
    "print(okt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "spread-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "kkma_tokens = kkma.morphs(kor_text)\n",
    "print(kkma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "robust-inflation",
   "metadata": {},
   "source": [
    "# 한글 POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "european-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'NNG'), ('이', 'JKS'), ('컴퓨터', 'NNG'), ('와', 'JC'), ('대화', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VV'), ('다는', 'ETM'), ('것', 'NNB'), ('을', 'JKO'), ('깨닫', 'VV'), ('지', 'EC'), ('못하', 'VX'), ('고', 'EC'), ('인간', 'NNG'), ('과', 'JC'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'EC'), ('컴퓨터', 'NNG'), ('는', 'JX'), ('지능', 'NNG'), ('적', 'XSN'), ('이', 'VCP'), ('ㄴ', 'ETM'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'XSV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(komoran.pos(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "binding-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'N'), ('이', 'J'), ('컴퓨터', 'N'), ('와', 'J'), ('대화', 'N'), ('하고', 'J'), ('있', 'P'), ('다는', 'E'), ('것', 'N'), ('을', 'J'), ('깨닫', 'P'), ('지', 'E'), ('못하', 'P'), ('고', 'E'), ('인간', 'N'), ('과', 'J'), ('대화', 'N'), ('를', 'J'), ('계속', 'N'), ('하', 'X'), ('ㄹ', 'E'), ('수', 'N'), ('있', 'P'), ('다면', 'E'), ('컴퓨터', 'N'), ('는', 'J'), ('지능적', 'N'), ('이', 'J'), ('ㄴ', 'E'), ('것', 'N'), ('으로', 'J'), ('간주', 'N'), ('되', 'X'), ('ㄹ', 'E'), ('수', 'N'), ('있', 'P'), ('습니다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.pos(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "varying-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'Noun'), ('이', 'Josa'), ('컴퓨터', 'Noun'), ('와', 'Josa'), ('대화', 'Noun'), ('하고', 'Josa'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('하고', 'Josa'), ('인간', 'Noun'), ('과', 'Josa'), ('대화', 'Noun'), ('를', 'Josa'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Josa'), ('지능', 'Noun'), ('적', 'Suffix'), ('인', 'Josa'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "latin-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'NNG'), ('이', 'JKS'), ('컴퓨터', 'NNG'), ('와', 'JKM'), ('대화', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('있', 'VXV'), ('다는', 'ETD'), ('것', 'NNB'), ('을', 'JKO'), ('깨닫', 'VV'), ('지', 'ECD'), ('못하', 'VX'), ('고', 'ECE'), ('인간', 'NNG'), ('과', 'JKM'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VA'), ('다면', 'ECE'), ('컴퓨터', 'NNG'), ('는', 'JX'), ('지능', 'NNG'), ('적', 'XSN'), ('이', 'VCP'), ('ㄴ', 'ETD'), ('것', 'NNB'), ('으로', 'JKM'), ('간주', 'NNG'), ('되', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('습니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-cloud",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "choice-advertising",
   "metadata": {},
   "source": [
    "### 빈도수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fitting-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "altered-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('인간', 'NNG'), 2), (('컴퓨터', 'NNG'), 2), (('대화', 'NNG'), 2), (('하', 'XSV'), 2), (('고', 'ECE'), 2), (('것', 'NNB'), 2), (('ㄹ', 'ETD'), 2), (('수', 'NNB'), 2), (('이', 'JKS'), 1), (('와', 'JKM'), 1), (('있', 'VXV'), 1), (('다는', 'ETD'), 1), (('을', 'JKO'), 1), (('깨닫', 'VV'), 1), (('지', 'ECD'), 1), (('못하', 'VX'), 1), (('과', 'JKM'), 1), (('를', 'JKO'), 1), (('계속', 'NNG'), 1), (('있', 'VA'), 1), (('다면', 'ECE'), 1), (('는', 'JX'), 1), (('지능', 'NNG'), 1), (('적', 'XSN'), 1), (('이', 'VCP'), 1), (('ㄴ', 'ETD'), 1), (('으로', 'JKM'), 1), (('간주', 'NNG'), 1), (('되', 'XSV'), 1), (('있', 'VV'), 1), (('습니다', 'EFN'), 1), (('.', 'SF'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(kkma.pos(kor_text)).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-denmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "center-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['의', '이', '로', '두고', '들', '를', '은', '과', '수',' 했다']\n",
    "stop_pos = ['Josa', 'Suffix', 'Punctuation', 'Adjective']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "tender-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "for item in okt.pos(kor_text):\n",
    "    if (item[0] not in stop_words) and (item[1] not in stop_pos):\n",
    "        word.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "still-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '컴퓨터', '대화', '것', '깨닫지', '못', '인간', '대화', '계속', '할', '컴퓨터', '지능', '것', '간주', '될']\n"
     ]
    }
   ],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-trash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "thick-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "coordinate-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object bigrams at 0x1ac29cb950>\n",
      "('인간이', '컴퓨터와')\n",
      "('컴퓨터와', '대화하고')\n",
      "('대화하고', '있다는')\n",
      "('있다는', '것을')\n",
      "('것을', '깨닫지')\n",
      "('깨닫지', '못하고')\n",
      "('못하고', '인간과')\n",
      "('인간과', '대화를')\n",
      "('대화를', '계속할')\n",
      "('계속할', '수')\n",
      "('수', '있다면')\n",
      "('있다면', '컴퓨터는')\n",
      "('컴퓨터는', '지능적인')\n",
      "('지능적인', '것으로')\n",
      "('것으로', '간주될')\n",
      "('간주될', '수')\n",
      "('수', '있습니다')\n",
      "('있습니다', '.')\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(kor_text)\n",
    "bigram = bigrams(tokens)\n",
    "print(bigram)\n",
    "\n",
    "for t in bigram:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "protected-occupation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('인간이', '컴퓨터와', '대화하고')\n",
      "('컴퓨터와', '대화하고', '있다는')\n",
      "('대화하고', '있다는', '것을')\n",
      "('있다는', '것을', '깨닫지')\n",
      "('것을', '깨닫지', '못하고')\n",
      "('깨닫지', '못하고', '인간과')\n",
      "('못하고', '인간과', '대화를')\n",
      "('인간과', '대화를', '계속할')\n",
      "('대화를', '계속할', '수')\n",
      "('계속할', '수', '있다면')\n",
      "('수', '있다면', '컴퓨터는')\n",
      "('있다면', '컴퓨터는', '지능적인')\n",
      "('컴퓨터는', '지능적인', '것으로')\n",
      "('지능적인', '것으로', '간주될')\n",
      "('것으로', '간주될', '수')\n",
      "('간주될', '수', '있습니다')\n",
      "('수', '있습니다', '.')\n"
     ]
    }
   ],
   "source": [
    "trigram = ngrams(tokens, 3)\n",
    "for t in trigram:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-burst",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
