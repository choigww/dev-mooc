{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Project Walkthrough:\n",
    "# Preparing the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In the past mission, you removed all of the columns that contained redundant information, weren't useful for modeling, required too much processing to make useful, or leaked information from the future. We've exported the Dataframe from the end of the last mission to a CSV file named `filtered_loans_2007.csv` to differentiate the file with the `loans_2007.csv` we used in the last mission. In this mission, we'll prepare the data for machine learning by focusing on handling missing values, converting categorical columns to numeric columns, and removing any other extraneous columns we encounter throughout this process.<br>\n",
    "\n",
    "This is because the mathematics underlying most machine learning models assumes that the data is numerical and contains no missing values. To reinforce this requirement, scikit-learn will return an error if you try to train a model using data that contain missing values or non-numeric values when working with models like linear regression and logistic regression.<br>\n",
    "\n",
    "Let's start by computing the number of missing values and come up with a strategy for handling them. Then, we'll focus on the categorical columns.<br>\n",
    "\n",
    "We can return the number of missing values across the Dataframe by:\n",
    "\n",
    "* first using the Pandas Dataframe method [isnull](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html) to return a Dataframe containing Boolean values:\n",
    "  * `True` if the original value is null,\n",
    "  * `False` if the original value isn't null.\n",
    "* then using the Pandas Dataframe method [sum](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html) to calculate the number of null values in each column.\n",
    "\n",
    "```python\n",
    "null_counts = df.isnull().sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read in `filtered_loans_2007.csv` as a Dataframe and assign it to `loans`.\n",
    "* Use the `isnull` and `sum` methods to return the number of null values in each column. Assign the resulting Series object to `null_counts`.\n",
    "* Use the `print` function to display `null_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt                  0\n",
      "term                       0\n",
      "int_rate                   0\n",
      "installment                0\n",
      "emp_length              1036\n",
      "home_ownership             0\n",
      "annual_inc                 0\n",
      "verification_status        0\n",
      "loan_status                0\n",
      "purpose                    0\n",
      "title                     11\n",
      "addr_state                 0\n",
      "dti                        0\n",
      "delinq_2yrs                0\n",
      "earliest_cr_line           0\n",
      "inq_last_6mths             0\n",
      "open_acc                   0\n",
      "pub_rec                    0\n",
      "revol_bal                  0\n",
      "revol_util                50\n",
      "total_acc                  0\n",
      "last_credit_pull_d         2\n",
      "pub_rec_bankruptcies     697\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('data/filtered_loans_2007.csv')\n",
    "null_counts = loans.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values\n",
    "\n",
    "While most of the columns have 0 missing values, 2 columns have 50 or less rows with missing values, and 1 column, `pub_rec_bankruptcies`, contains 697 rows with missing values. Let's remove columns entirely where more than 1% of the rows for that column contain a null value. In addition, we'll remove the remaining rows containing null values.<br>\n",
    "\n",
    "This means that we'll keep the following columns and just remove rows containing missing values for them:\n",
    "\n",
    "* title\n",
    "* revol_util\n",
    "* last_credit_pull_d\n",
    "\n",
    "and drop the `pub_rec_bankruptcies` column entirely since more than 1% of the rows have a missing value for this column.<br>\n",
    "\n",
    "Let's use the strategy of removing the `pub_rec_bankruptcies` column first then removing all rows containing any missing values at all to cover both of these cases. This way, we only remove the rows containing missing values for the `title` and `revol_util` columns but not the `pub_rec_bankruptcies` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the [drop method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) to remove the `pub_rec_bankruptcies` column from `loans`.\n",
    "* Use the [dropna method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) to remove all rows from `loans` containing any missing values.\n",
    "* Use the `dtypes` attribute followed by the `value_counts()` method to return the counts for each column data type. Use the `print` function to display these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     11\n",
      "float64    10\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "loans = loans.drop(['pub_rec_bankruptcies'], axis=1)\n",
    "loans = loans.dropna(how='any', axis=0)\n",
    "print(loans.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text columns\n",
    "\n",
    "While the numerical columns can be used natively with scikit-learn, the object columns that contain text need to be converted to numerical data types. Let's return a new Dataframe containing just the object columns so we can explore them in more depth. You can use the Dataframe method [select_dtypes](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html) to select only the columns of a certain data type:\n",
    "\n",
    "```python\n",
    "float_df = df.select_dtypes(include=['float'])\n",
    "```\n",
    "\n",
    "Let's select just the object columns then display a sample row to get a better sense of how the values in each column are formatted.\n",
    "\n",
    "* Use the Dataframe method `select_dtypes` to select only the columns of `object` type from `loans` and assign the resulting Dataframe `object_columns_df`.\n",
    "* Display the first row in `object_columns_df` using the `print` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                     36 months\n",
      "int_rate                    10.65%\n",
      "emp_length               10+ years\n",
      "home_ownership                RENT\n",
      "verification_status       Verified\n",
      "purpose                credit_card\n",
      "title                     Computer\n",
      "addr_state                      AZ\n",
      "earliest_cr_line          Jan-1985\n",
      "revol_util                   83.7%\n",
      "last_credit_pull_d        Jun-2016\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "object_columns_df = loans.select_dtypes(include=['object'])\n",
    "print(object_columns_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text columns\n",
    "\n",
    "Some of the columns seem like they represent categorical values, but we should confirm by checking the number of unique values in those columns:\n",
    "\n",
    "* `home_ownership`: home ownership status, can only be 1 of 4 categorical values according to the data dictionary,\n",
    "* `verification_status`: indicates if income was verified by Lending Club,\n",
    "* `emp_length`: number of years the borrower was employed upon time of application,\n",
    "* `term`: number of payments on the loan, either 36 or 60,\n",
    "* `addr_state`: borrower's state of residence,\n",
    "* `purpose`: a category provided by the borrower for the loan request,\n",
    "* `title`: loan title provided the borrower,\n",
    "\n",
    "There are also some columns that represent numeric values, that need to be converted:\n",
    "\n",
    "* `int_rate`: interest rate of the loan in %,\n",
    "* `revol_util`: revolving line utilization rate or the amount of credit the borrower is using relative to all available credit, read more [here](http://blog.credit.com/2013/04/what-is-revolving-utilization-65530/).\n",
    "\n",
    "Based on the first row's values for `purpose` and `title`, it seems like these columns could reflect the same information. Let's explore the unique value counts separately to confirm if this is true.<br>\n",
    "\n",
    "Lastly, some of the columns contain date values that would require a good amount of feature engineering for them to be potentially useful:<br>\n",
    "\n",
    "* `earliest_cr_line`: The month the borrower's earliest reported credit line was opened,\n",
    "* `last_credit_pull_d`: The most recent month Lending Club pulled credit for this loan.\n",
    "\n",
    "Since these date features require some feature engineering for modeling purposes, let's remove these date columns from the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 5 categorical columns\n",
    "\n",
    "Let's explore the unique value counts of the columnns that seem like they contain categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display the unique value counts for the following columns: `home_ownership`, `verification_status`, `emp_length`, `term`, `addr_state` columns:\n",
    "  * Store these column names in a list named `cols`.\n",
    "  * Use a for loop to iterate over `cols`:\n",
    "    * Use the `print` function combined with the Series method `value_counts` to display each column's unique value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RENT        18112\n",
      "MORTGAGE    16686\n",
      "OWN          2778\n",
      "OTHER          96\n",
      "NONE            3\n",
      "Name: home_ownership, dtype: int64\n",
      "Not Verified       16281\n",
      "Verified           11856\n",
      "Source Verified     9538\n",
      "Name: verification_status, dtype: int64\n",
      "10+ years    8545\n",
      "< 1 year     4513\n",
      "2 years      4303\n",
      "3 years      4022\n",
      "4 years      3353\n",
      "5 years      3202\n",
      "1 year       3176\n",
      "6 years      2177\n",
      "7 years      1714\n",
      "8 years      1442\n",
      "9 years      1228\n",
      "Name: emp_length, dtype: int64\n",
      " 36 months    28234\n",
      " 60 months     9441\n",
      "Name: term, dtype: int64\n",
      "CA    6776\n",
      "NY    3614\n",
      "FL    2704\n",
      "TX    2613\n",
      "NJ    1776\n",
      "IL    1447\n",
      "PA    1442\n",
      "VA    1347\n",
      "GA    1323\n",
      "MA    1272\n",
      "OH    1149\n",
      "MD    1008\n",
      "AZ     807\n",
      "WA     788\n",
      "CO     748\n",
      "NC     729\n",
      "CT     711\n",
      "MI     678\n",
      "MO     648\n",
      "MN     581\n",
      "NV     466\n",
      "SC     454\n",
      "WI     427\n",
      "OR     422\n",
      "AL     420\n",
      "LA     420\n",
      "KY     311\n",
      "OK     285\n",
      "UT     249\n",
      "KS     249\n",
      "AR     229\n",
      "DC     209\n",
      "RI     194\n",
      "NM     180\n",
      "WV     164\n",
      "HI     162\n",
      "NH     157\n",
      "DE     110\n",
      "MT      77\n",
      "WY      76\n",
      "AK      76\n",
      "SD      60\n",
      "VT      53\n",
      "MS      19\n",
      "TN      17\n",
      "IN       9\n",
      "ID       6\n",
      "IA       5\n",
      "NE       5\n",
      "ME       3\n",
      "Name: addr_state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols = ['home_ownership', 'verification_status', \n",
    "        'emp_length', 'term', 'addr_state']\n",
    "\n",
    "for col in cols:\n",
    "    print(loans[col].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reason for the loan\n",
    "\n",
    "The `home_ownership`, `verification_status`, `emp_length`, `term`, and `addr_state` columns all contain multiple discrete values. We should clean the `emp_length` column and treat it as a numerical one since the values have ordering (2 years of employment is less than 8 years).<br>\n",
    "\n",
    "First, let's look at the unique value counts for the `purpose` and `title` columns to understand which column we want to keep.\n",
    "\n",
    "* Use the `value_counts` method and the `print` function to display the unique values in the following columns:\n",
    "  * `purpose`\n",
    "  * `title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debt_consolidation    17751\n",
      "credit_card            4911\n",
      "other                  3711\n",
      "home_improvement       2808\n",
      "major_purchase         2083\n",
      "small_business         1719\n",
      "car                    1459\n",
      "wedding                 916\n",
      "medical                 655\n",
      "moving                  552\n",
      "house                   356\n",
      "vacation                348\n",
      "educational             312\n",
      "renewable_energy         94\n",
      "Name: purpose, dtype: int64\n",
      "Debt Consolidation                            2068\n",
      "Debt Consolidation Loan                       1599\n",
      "Personal Loan                                  624\n",
      "Consolidation                                  488\n",
      "debt consolidation                             466\n",
      "Credit Card Consolidation                      345\n",
      "Home Improvement                               336\n",
      "Debt consolidation                             314\n",
      "Small Business Loan                            298\n",
      "Credit Card Loan                               294\n",
      "Personal                                       290\n",
      "Consolidation Loan                             250\n",
      "Home Improvement Loan                          228\n",
      "personal loan                                  219\n",
      "Loan                                           202\n",
      "Wedding Loan                                   199\n",
      "personal                                       198\n",
      "Car Loan                                       188\n",
      "consolidation                                  186\n",
      "Other Loan                                     168\n",
      "Wedding                                        148\n",
      "Credit Card Payoff                             144\n",
      "Credit Card Refinance                          140\n",
      "Major Purchase Loan                            131\n",
      "Consolidate                                    124\n",
      "Medical                                        111\n",
      "Credit Card                                    110\n",
      "home improvement                               101\n",
      "Credit Cards                                    91\n",
      "My Loan                                         90\n",
      "                                              ... \n",
      "Business 10 Yrs in biz - happyballs.com          1\n",
      "credit card /taxes                               1\n",
      "Debt consolidation 9.13.10                       1\n",
      "Bridge loan to cover loss on home sale.          1\n",
      "Pay Down High Interest Credit Card Balance       1\n",
      "bills                                            1\n",
      "Relocatioin                                      1\n",
      "Student Teaching                                 1\n",
      "Consolidate -Lower Payment- Save Money           1\n",
      "Pay off debt and start buisness                  1\n",
      "Debt Consolidation Loan $9,500.00                1\n",
      "Paying off Mum's surgery                         1\n",
      "ReFi CC                                          1\n",
      "Caretaker                                        1\n",
      "Get the heck out of debt loan                    1\n",
      "Loan Funds1                                      1\n",
      "for excellent credit                             1\n",
      "No more WellFargo Visa                           1\n",
      "Money for land purchase                          1\n",
      "Bye-bye Chase!                                   1\n",
      "Refinancing an old closed Cedit Card             1\n",
      "Pay off High Interest Car Repair Bill            1\n",
      "CCard payments                                   1\n",
      "getting rid of credit cards                      1\n",
      "Guaranteed Full Return Plus Interest             1\n",
      "Pay off Divorce Settlement                       1\n",
      "PayThatCreditCardOFF!!!                          1\n",
      "Special Day Expenses                             1\n",
      "HOME REPAIRS                                     1\n",
      "Cons_Loan                                        1\n",
      "Name: title, Length: 18881, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loans['purpose'].value_counts())\n",
    "print(loans['title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns\n",
    "\n",
    "The `home_ownership`, `verification_status`, `emp_length`, and `term` columns each contain a few discrete categorical values. We should encode these columns as dummy variables and keep them.<br>\n",
    "\n",
    "It seems like the `purpose` and `title` columns do contain overlapping information but we'll keep the `purpose` column since it contains a few discrete values. In addition, the `title` column has data quality issues since many of the values are repeated with slight modifications (e.g. `Debt Consolidation` and `Debt Consolidation Loan` and `debt consolidation`).<br>\n",
    "\n",
    "We can use the following mapping to clean the `emp_length` column:\n",
    "\n",
    "* \"10+ years\": 10\n",
    "* \"9 years\": 9\n",
    "* \"8 years\": 8\n",
    "* \"7 years\": 7\n",
    "* \"6 years\": 6\n",
    "* \"5 years\": 5\n",
    "* \"4 years\": 4\n",
    "* \"3 years\": 3\n",
    "* \"2 years\": 2\n",
    "* \"1 year\": 1\n",
    "* \"< 1 year\": 0\n",
    "* \"n/a\": 0\n",
    "\n",
    "We erred on the side of being conservative with the `10+ years`, `< 1 year` and `n/a` mappings. We assume that people who may have been working more than 10 years have only really worked for 10 years. We also assume that people who've worked less than a year or if the information is not available that they've worked for 0. This is a general heuristic but it's not perfect.<br>\n",
    "\n",
    "Lastly, the `addr_state` column contains many discrete values and we'd need to add 49 dummy variable columns to use it for classification. This would make our Dataframe much larger and could slow down how quickly the code runs. Let's remove this column from consideration.\n",
    "\n",
    "\n",
    "* Remove the `last_credit_pull_d`, `addr_state`, `title`, and `earliest_cr_line` columns from `loans`.\n",
    "* Convert the `int_rate` and `revol_util` columns to float columns by:\n",
    "  * Using the `str` acessor followed by the `rstrip` string method to strip the right trailing percent sign (%):\n",
    "    * `loans['int_rate'].str.rstrip('%')` returns a new Series with % stripped from the right side of each value.\n",
    "  * On the resulting Series object, use the `astype` method to convert to the float type.\n",
    "  * Assign the new Series of float values back to the respective columns in the Dataframe.\n",
    "* Use the `replace` method to clean the `emp_length` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['last_credit_pull_d',\n",
    "               'addr_state', 'title',\n",
    "               'earliest_cr_line']\n",
    "loans = loans.drop(cols_to_drop, axis=1)\n",
    "\n",
    "loans['int_rate'] = loans['int_rate'].str.rstrip('%').astype('float64')\n",
    "\n",
    "#print(loans['revol_util'].value_counts())\n",
    "\n",
    "loans['revol_util'] = loans['revol_util'].str.rstrip('%').astype('float64')\n",
    "\n",
    "loans = loans.replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "\n",
    "Let's now encode the `home_ownership`, `verification_status`, `title`, and `term` columns as dummy variables so we can use them in our model. We first need to use the Pandas [get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) method to return a new Dataframe containing a new column for each dummy variable:\n",
    "\n",
    "```python\n",
    "# Returns a new Dataframe containing 1 column for each dummy variable.\n",
    "dummy_df = pd.get_dummies(loans[\"term\", \"verification_status\"])\n",
    "```\n",
    "\n",
    "We can then use the [concat](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) method to add these dummy columns back to the original Dataframe:\n",
    "\n",
    "```python\n",
    "loans = pd.concat([loans, dummy_df], axis=1]\n",
    "```\n",
    "                  \n",
    "and then drop the original columns entirely using the `drop` method:\n",
    "\n",
    "```python\n",
    "loans = loans.drop([\"verification_status\", \"term\"], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encode the home_ownership, verification_status, purpose, and term columns as integer values:\n",
    "  * Use the Series method astype to convert each column to the category data type.\n",
    "  * Use the get_dummies function to return a Dataframe containing the dummy columns.\n",
    "  * Use the concat method to add these dummy columns back to loans.\n",
    "  * Remove the original, non-dummy columns (home_ownership, verification_status, purpose, and term) from loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['home_ownership', 'verification_status', \n",
    "                  'purpose','term']\n",
    "\n",
    "dummy_df = pd.get_dummies(loans[cols_to_encode])\n",
    "loans = pd.concat([loans, dummy_df], axis=1)\n",
    "    \n",
    "loans = loans.drop(cols_to_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this mission, we performed the last amount of data preparation necessary to start training machine learning models. We converted all of the columns to numerical values because those are the only type of value scikit-learn can work with. In the next mission, we'll experiment with training models and evaluating accuracy using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
