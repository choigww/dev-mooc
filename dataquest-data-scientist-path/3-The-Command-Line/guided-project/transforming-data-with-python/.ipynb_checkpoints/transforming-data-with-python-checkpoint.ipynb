{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Transforming data with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this project, you'll be working with a dataset of submissions to [Hacker News](http://news.ycombinator.com/) from 2006 to 2015. \n",
    "Hacker News is a site where users can submit articles from across the internet (usually about technology and startups), and others can \"upvote\" the articles, signifying that they like them. The more upvotes a submission gets, the more popular it was in the community. Popular articles get to the \"front page\" of Hacker News, where they're more likely to be seen by others.<br>\n",
    "\n",
    "The dataset you'll be using was compiled by Arnaud Drizard using the Hacker News API, and can be found [here](https://github.com/arnauddri/hn). We've sampled 10000 rows from the data randomly, and removed all extraneous columns. **Our dataset only has four columns**:\n",
    "\n",
    "* submission_time -- when the story was submitted.\n",
    "* upvotes -- number of upvotes the submission got.\n",
    "* url -- the base domain of the submission.\n",
    "* headline -- the headline of the submission. Users can edit this, and it doesn't have to match the headline of the original article.\n",
    "\n",
    "### You'll be writing scripts to answer some main questions:\n",
    "\n",
    "What words appear most often in the headlines?\n",
    "What domains were submitted most often to Hacker News?\n",
    "At what times are the most articles submitted?\n",
    "You'll be answering these questions by writing command line scripts, instead of using IPython notebook. IPython notebooks are great for quick data visualization and exploration, but Python scripts are the way to put anything we learn into production. Let's say you want to make a website to help people write headlines that get as many upvotes as possible, and submit articles at the right time. To do this, you'll need scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the read.py file, read the hn_stories.csv file into a Pandas Dataframe.\n",
    "* There is no header row in the data, so the columns don't have names. See this [stackoverflow thread for how to add column names](http://stackoverflow.com/questions/11346283/renaming-columns-in-pandas). Add the column names from the last screen (submission_time, upvotes, url, and headline) to the Dataframe.\n",
    "* Create a function called load_data that takes no inputs, but contains the code to read in and process the dataset. load_data should return a Pandas Dataframe with the column names set correctly.\n",
    "\n",
    "As you work on these steps, you should be running your script on the command line every so often and verifying that things are working. You can run read.py from the command line by calling python read.py. \n",
    "* The first verification is to make sure that you don't see any errors. \n",
    "* The second one is to call print at key points in your code, and make sure that the output looks like what you expect. \n",
    "* You might want to do this after each step above. This is a good general rule of thumb to follow when writing new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "/home/dq/scripts$ ls -la\n",
    "total 840                                   \n",
    "drwxr-xr-x 2 dq dq   4096 Nov 13 07:36 .    \n",
    "drwxr-xr-x 1 dq dq   4096 Nov 13 09:04 ..   \n",
    "-rwxr-xr-x 1 dq dq 851754 Nov 13 07:36 hn_stories.csv\n",
    "-rwxrwxrwx 1 dq dq      0 Nov 13 09:11 read.py\n",
    "/home/dq/scripts$ nano read.py\n",
    "\n",
    "--- read.py\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    stories = pd.read_csv('hn_stories.csv')\n",
    "    stories.columns = ['submission_time', 'upvotes', 'url', 'headline']\n",
    "    return stories\n",
    "---\n",
    "\n",
    "/home/dq/scripts$ \n",
    "/home/dq/scripts$ \n",
    "/home/dq/scripts$ \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to figure out which words appear most often in the headlines. We'll be developing another script, called `count.py` to accomplish this. We'll need to import our load_data function from `read.py` into `count.py` so we can use it.\n",
    "\n",
    "You'll recall that if you have a folder with two files, `read.py` and `count.py`, you can use the function `load_data` in `read.py` from `count.py` by writing the following code in count.py:\n",
    "\n",
    "```bash\n",
    "import read\n",
    "df = read.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "--- count.py\n",
    "\n",
    "import read\n",
    "from collections import Counter\n",
    "\n",
    "data = read.load_data()\n",
    "data_ = data[data['headline'].notnull()]\n",
    "\n",
    "headlines = ' '.join(data_['headline'])\n",
    "headlines = headlines.lower()\\\n",
    ".replace('(','')\\\n",
    ".replace(')','')\\\n",
    ".replace('?','')\n",
    "\n",
    "head_split = headlines.split(' ')\n",
    "\n",
    "c = Counter(head_split)\n",
    "print(c.most_common(100))\n",
    "\n",
    "---\n",
    "\n",
    "[('the', 2051), ('to', 1643), ('a', 1279), ('of', 1174), ('for', 1143)\n",
    ", ('in', 1042), ('and', 960), ('', 740), ('is', 621), ('on', 573), ('w\n",
    "ith', 541), ('hn:', 537), ('how', 529), ('-', 487), ('your', 480), ('y\n",
    "ou', 401), ('ask', 371), ('from', 314), ('google', 308), ('new', 305),\n",
    " ('why', 266), ('what', 262), ('an', 245), ('are', 223), ('by', 222), ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move on to our second question, and explore which domains were submitted most often. We'll want to make a separate script, called `domains.py`, for this.\n",
    "\n",
    "### Here are the steps:\n",
    "* Make a file called domains.py, using the file browser, or the command line.\n",
    "* Add in the code to read the file hn_stories.csv, and add column names.\n",
    "* You can think of each domain name as a \"word\". A domain will look like scala-lang.org, or blog.iweb.com.\n",
    "  * You can use the value_counts method in pandas to count the number of occurrences of each value in a column. [Here](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.Series.value_counts.html) are the docs.\n",
    "* Print the 100 most submitted domains.\n",
    "\n",
    "By default, Pandas only prints 10 rows of a Dataframe or Series. There is a pandas option to make it print more rows (see [this thread on Stackoverflow](http://stackoverflow.com/questions/19124601/is-there-a-way-to-pretty-print-the-entire-pandas-series-dataframe)), but there are bugs with it and Series. Instead, just loop through the series and print the index value, and the total. Here's some sample code:\n",
    "\n",
    "```bash\n",
    "for name, row in domains.items():\n",
    "    print(\"{0}: {1}\".format(name, row))\n",
    "```\n",
    "The above code assumes that the results of running `value_counts` is assigned to `domains`.\n",
    "\n",
    "* You can extend this analysis and make it a bit more robust by removing subdomains. For example, `blog.iweb.com `and `iweb.com` would be separate domains at the moment, but they are the same. By removing the subdomain, you can turn `blog.iweb.com` into `iweb.com`. You can remove the subdomain using the `apply` method on Pandas Series and Dataframes. [Here's the documentation](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.apply.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "--- domain.py\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "import read\n",
    "\n",
    "stories_df = read.load_data()\n",
    "print(stories_df['url'].value_counts()[:100])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming timestamp format using `dateutil` parser\n",
    "**We want to know when the most articles are submitted.** One easy way to reframe this is to look at what hour articles are submitted. To figure this out, we'll need to use the `submission_time` column.\n",
    "\n",
    "* The `submission_time` column contains timestamps, which look like this: `2011-11-09T21:56:22Z`. These times are expressed in UTC, which is a universal time zone used by most software for consistency (imagine a database populated with times all having different timezones; it would be a huge pain to work with).\n",
    "\n",
    "* To get hour from a timestamp, we can use the `dateutil` library. The `parser` module in `dateutil` contains the `parse` function, which can take in a timestamp, and return a datetime object. [Here](https://dateutil.readthedocs.org/en/latest/parser.html)'s a link to the documentation. After parsing the timestamp, the `hour` property of the resulting date object will tell you the hour the article was submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "--- times.py\n",
    "\n",
    "from dateutil.parser import parse as dateparser\n",
    "import domains\n",
    "\n",
    "stories_df = domains.stories_df\n",
    "\n",
    "stories_df['submission_hour'] = stories_df['submission_time'].apply(dateparser).apply(lambda x: x.hour)\n",
    "\n",
    "print(stories_df['submission_hour'].value_counts)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# update read.py\n",
    "\n",
    "--- read.py\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as dateparser\n",
    "\n",
    "def load_data():\n",
    "    stories = pd.read_csv('hn_stories.csv')\n",
    "    stories.columns = ['submission_time', 'upvotes', 'url', 'headline']\n",
    "    \n",
    "    stories['submission_hour'] = stories['submission_time'].apply(dateparser)\\\n",
    ".apply(lambda x: x.hour)\n",
    "    return stories\n",
    "\n",
    "---\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't think of any questions, some interesting ones are:\n",
    "\n",
    "* What headline length leads to the most upvotes?\n",
    "* What submission time leads to the most upvotes?\n",
    "* How are the total numbers of upvotes changing over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What headline length leads to the most upvotes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "--- popular_head_len.py\n",
    "\n",
    "import pandas as pd\n",
    "import read\n",
    "\n",
    "stories_df = read.load_data()\n",
    "\n",
    "stories_df['head_len'] = stories_df['headline'].apply(lambda x: len(str(x)))\n",
    "\n",
    "#print(stories_df[['headline','head_len']].iloc[:5])\n",
    "\n",
    "stories_sorted_upvote = stories_df.sort_values(by='upvotes', ascending=False)\n",
    "\n",
    "print(stories_sorted_upvote['head_len'][:10]\\\n",
    "      .value_counts())\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What submission time leads to the most upvotes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "--- hot_submission_time.py\n",
    "\n",
    "import pandas as pd\n",
    "import read\n",
    "\n",
    "stories_df = read.load_data()\n",
    "\n",
    "#print(stories_df['submission_hour'].dtype)\n",
    "#print(stories_df['upvotes'].dtype)\n",
    "\n",
    "stories_df['upvotes_int'] = stories_df['upvotes'].apply(int)\n",
    "\n",
    "hot_submission_time = stories_df.groupby('submission_hour')['upvotes_int'].mean()\n",
    "\n",
    "print(hot_submission_time.sort_values(ascending=False))\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "\n",
    "# update read.py\n",
    "--- read.py\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as dateparser\n",
    "\n",
    "def load_data():\n",
    "    stories = pd.read_csv('hn_stories.csv')\n",
    "    stories.columns = ['submission_time', 'upvotes', 'url', 'headline']\n",
    "\n",
    "    stories['submission_time'] = stories['submission_time'].apply(dateparser)\n",
    "    \n",
    "    stories['submission_hour'] = stories['submission_time'].apply(lambda x: x.hour)\n",
    "\n",
    "    stories['upvotes_int'] = stories['upvotes'].apply(int)\n",
    "\n",
    "    return stories\n",
    "---\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are the total numbers of upvotes changing over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "\n",
    "--- upvotes_over_time.py\n",
    "\n",
    "import pandas as pd\n",
    "import read\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "stories_df = read.load_data()\n",
    "\n",
    "tot_upvotes_over_time = stories_df.groupby('submission_time')['upvotes_int'].sum()\n",
    "\n",
    "print(tot_upvotes_over_time)\n",
    "\n",
    "tot_upvotes_over_time.plot()\n",
    "plt.show()\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
