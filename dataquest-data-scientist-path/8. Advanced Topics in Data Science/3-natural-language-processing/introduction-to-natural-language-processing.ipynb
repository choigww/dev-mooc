{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When you hear your native language, you intuitively know the meaning of what you heard. However, many people who've tried to learn a second or third language find the process to be much more painful. They have to break the language down into components like tenses in order to understand it better. Many have to take years of language lessons to get to the point where they can have a conversation.<br>\n",
    "\n",
    "**Learning a language is difficult because language has many complex rules**. If we want computers to be able to understand language, we either need to explicitly teach computers the rules, or enable the computers to intuit the rules themselves. The former is a lot like learning a second language, and the latter is a lot like learning your native language.<br>\n",
    "\n",
    "Broadly speakingly, **natural language processing is the study of enabling computers to understand human languages**. This field may involve teaching computers to automatically score essays, infer grammatical rules, or determine the emotions associated with text.<br>\n",
    "\n",
    "In this mission, we'll learn some of the basic building blocks of natural langage processing. When we feed a computer written text, it has no idea what that text means. In order for a computer to begin making inferences from it, we'll need to convert the text to a numerical representation. This process will enable the computer to intuit grammatical rules, which is more akin to learning a first language.<br>\n",
    "\n",
    "We'll explore how to get from written text to a numerical representation, and how we can use that representation to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data\n",
    "\n",
    "[Hacker News](http://news.ycombinator.com/) is a community where users can submit articles, and other users can upvote those articles. The articles with the most upvotes make it to the front page, where they're more visible to the community.<br>\n",
    "\n",
    "Our data set consists of submissions users made to Hacker News from 2006 to 2015. Developer Arnaud Drizard used the Hacker News API to scrape the data, which you can find in [one of his GitHub repositories](https://github.com/arnauddri/hn). We've sampled `3000` rows from the data randomly, and removed all of the extraneous columns. Our data only has four columns:\n",
    "* `submission_time` - When the article was submitted\n",
    "* `upvotes` - The number of upvotes the article received\n",
    "* `url` - The base URL of the article\n",
    "* `headline` - The article's headline\n",
    "\n",
    "In this mission, we'll be predicting the number of upvotes the articles received, based on their headlines. Because upvotes are an indicator of popularity, we'll discover which types of articles tend to be the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_csv('data/sel_hn_stories.csv')\n",
    "submissions.columns = ['submission_time', 'upvotes', 'url',\n",
    "                     'headline']\n",
    "submissions = submissions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-17T16:57:59Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blog.jonasbandi.net</td>\n",
       "      <td>Software: Sadly we did adopt from the construc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-02-04T02:36:30Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blogs.wsj.com</td>\n",
       "      <td>Google’s Stock Split Means More Control for L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-26T07:11:29Z</td>\n",
       "      <td>1</td>\n",
       "      <td>threatpost.com</td>\n",
       "      <td>SSL DOS attack tool released exploiting negoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-03T15:43:44Z</td>\n",
       "      <td>67</td>\n",
       "      <td>algorithm.com.au</td>\n",
       "      <td>Immutability and Blocks Lambdas and Closures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-13T16:49:20Z</td>\n",
       "      <td>1</td>\n",
       "      <td>winmacsofts.com</td>\n",
       "      <td>Comment optimiser la vitesse de Wordpress?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        submission_time  upvotes                  url  \\\n",
       "0  2010-02-17T16:57:59Z        1  blog.jonasbandi.net   \n",
       "1  2014-02-04T02:36:30Z        1        blogs.wsj.com   \n",
       "2  2011-10-26T07:11:29Z        1       threatpost.com   \n",
       "3  2011-04-03T15:43:44Z       67     algorithm.com.au   \n",
       "4  2013-01-13T16:49:20Z        1      winmacsofts.com   \n",
       "\n",
       "                                            headline  \n",
       "0  Software: Sadly we did adopt from the construc...  \n",
       "1   Google’s Stock Split Means More Control for L...  \n",
       "2  SSL DOS attack tool released exploiting negoti...  \n",
       "3       Immutability and Blocks Lambdas and Closures  \n",
       "4         Comment optimiser la vitesse de Wordpress?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Headlines\n",
    "\n",
    "Our goal is to train a linear regression algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation.<br>\n",
    "\n",
    "While there are several ways to accomplish this, we'll use [a bag of words model](https://en.wikipedia.org/wiki/Bag-of-words_model). A bag of words model represents each piece of text as a numerical vector.<br>\n",
    "\n",
    "We'll examine each step in the bag of words process in this mission. For now, here's a high-level diagram showing how two sentences, `I rode my horse to Berlin.` and `You rode my horse to Berlin in the winter.`, convert to a bag of words:\n",
    "\n",
    "![](img/1.png)\n",
    "\n",
    "The first step in creating a bag of words model is [tokenization](https://en.wikipedia.org/wiki/Tokenization). In tokenization, we break a sentence up into disconnected words.<br>\n",
    "\n",
    "Here's a diagram in which we tokenize the two sentences we mentioned above:\n",
    "\n",
    "![](img/2.png)\n",
    "\n",
    "As you can see, all we're doing is splitting each sentence into a list of individual words, or tokens. The split occurs on the space character (`\" \"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split each headline into individual words on the space character(`\" \"`), and append the resulting list to `tokenized_headlines`.\n",
    "* When you're finished, `tokenized_headlines` should be a list of lists. Each list should contain the tokens for the `headline` located at the corresponding position in the `submissions` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headlines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    tokenized_headlines.append(row['headline'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "5       None\n",
       "6       None\n",
       "7       None\n",
       "8       None\n",
       "9       None\n",
       "11      None\n",
       "12      None\n",
       "13      None\n",
       "14      None\n",
       "15      None\n",
       "16      None\n",
       "17      None\n",
       "18      None\n",
       "19      None\n",
       "20      None\n",
       "21      None\n",
       "22      None\n",
       "23      None\n",
       "24      None\n",
       "25      None\n",
       "26      None\n",
       "27      None\n",
       "28      None\n",
       "29      None\n",
       "30      None\n",
       "        ... \n",
       "2969    None\n",
       "2970    None\n",
       "2971    None\n",
       "2972    None\n",
       "2973    None\n",
       "2974    None\n",
       "2975    None\n",
       "2976    None\n",
       "2977    None\n",
       "2978    None\n",
       "2979    None\n",
       "2980    None\n",
       "2981    None\n",
       "2982    None\n",
       "2983    None\n",
       "2984    None\n",
       "2985    None\n",
       "2986    None\n",
       "2987    None\n",
       "2988    None\n",
       "2989    None\n",
       "2990    None\n",
       "2991    None\n",
       "2992    None\n",
       "2993    None\n",
       "2994    None\n",
       "2995    None\n",
       "2996    None\n",
       "2997    None\n",
       "2998    None\n",
       "Length: 2800, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.apply(tokenize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Google’s',\n",
       " 'Stock',\n",
       " 'Split',\n",
       " 'Means',\n",
       " 'More',\n",
       " 'Control',\n",
       " 'for',\n",
       " 'Larry',\n",
       " 'and',\n",
       " 'Sergey',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_headlines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tokens to Increase Accuracy\n",
    "\n",
    "We now have tokens, but we need to process them a bit to make our predictions more accurate. We know that `Berlin`, `Berlin.`, and `berlin` all refer to the same word, but the computer doesn't know that. We'll need to convert those variations so that they're consistent.<br>\n",
    "\n",
    "We can do this by lowercasing (which will convert `Berlin` to `berlin`), and also by removing punctuation (so `Berlin.` becomes `Berlin`).\n",
    "\n",
    "![](img/3.png)\n",
    "\n",
    "Preprocessing doesn't have to be perfect, but the more we can help the computer group the same word together, the higher our prediction accuracy will be. Take a look through your tokens, and see if there are any instances of the same word that you haven't grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loop through each item in `tokenized_headlines`, which is a list of lists.\n",
    "  * For each list of tokens:\n",
    "      * Convert each individual token to lowercase\n",
    "      * Remove all of the items in the `punctuation` list from each individual token\n",
    "      * Append the clean list to `clean_tokenized`\n",
    "* `clean_tokenized` should now be a list of lists. Each list should contain the preprocessed tokens associated with the `headline` in the corresponding position of the `submissions` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tokenized_headlines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'googles',\n",
       " 'stock',\n",
       " 'split',\n",
       " 'means',\n",
       " 'more',\n",
       " 'control',\n",
       " 'for',\n",
       " 'larry',\n",
       " 'and',\n",
       " 'sergey',\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokenized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling a Matrix of Unique Words\n",
    "\n",
    "Now that we have our tokens, we can begin converting the sentences to their numerical representations. First, we'll retrieve all of the unique words from all of the headlines. Then, we'll create a matrix, and assign those words as the column headers. We'll initialize all of the values in the matrix to `0`.\n",
    "\n",
    "![](img/4.png)\n",
    "\n",
    "We'll use a pandas dataframe instead of a NumPy matrix. We can create a dataframe with all zero values using this syntax:\n",
    "\n",
    "```python\n",
    "pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)\n",
    "```\n",
    "\n",
    "The code above will create a dataframe with as many rows as the number of items in `clean_tokenized`. Each column name will be a word from `unique_tokens`. This assumes that we already assigned the unique tokens to `unique_tokens`. Each cell in the dataframe will have the value `0`. You can find more documentation on initializing a dataframe [in the pandas documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find all of the unique tokens in `clean_tokenized`, and assign the result to `unique_tokens`.\n",
    "  * Only add tokens that occur more than once (across all of the headlines). Tokens that only occur once don't add anything to the model's prediction power, and removing them will make our algorithm run much more quickly.\n",
    "  * To do this, you can keep a list of the tokens that occur once in the data, and a different list of the tokens that occur more than once. If a token is already in the first list when you encounter it and it's not in the second list, you should add it to the second list.\n",
    "  * When you're finished, `unique_tokens` should contain any tokens that occur more than once across all of the headlines.\n",
    "  * Each token in `unique_tokens` should only appear in the list a single time.\n",
    "* Create a dataframe with as many rows as there are items in the `clean_tokenized` list. Each column name should be a token in `unique_tokens`. Initialize all of the cells to the value `0`. Assign the dataframe to the variable `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tokens = []\n",
    "unique_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clean_item in clean_tokenized:\n",
    "    \n",
    "    for term in clean_item:\n",
    "        \n",
    "        if term not in single_tokens:\n",
    "            single_tokens.append(term)\n",
    "        else:\n",
    "            if term not in unique_tokens:\n",
    "                unique_tokens.append(term)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6918 2309\n"
     ]
    }
   ],
   "source": [
    "print(len(single_tokens), len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>as</th>\n",
       "      <th>you</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>split</th>\n",
       "      <th>good</th>\n",
       "      <th>how</th>\n",
       "      <th>...</th>\n",
       "      <th>frameworks</th>\n",
       "      <th>animated</th>\n",
       "      <th>walks</th>\n",
       "      <th>auctions</th>\n",
       "      <th>clouds</th>\n",
       "      <th>hammer</th>\n",
       "      <th>autonomous</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>crowdsourcing</th>\n",
       "      <th>disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  for   as  you   is  the  split  good  how    ...     frameworks  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "\n",
       "   animated  walks  auctions  clouds  hammer  autonomous  vehicle  \\\n",
       "0       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "1       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "2       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "3       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "4       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "\n",
       "   crowdsourcing  disaster  \n",
       "0            0.0       0.0  \n",
       "1            0.0       0.0  \n",
       "2            0.0       0.0  \n",
       "3            0.0       0.0  \n",
       "4            0.0       0.0  \n",
       "\n",
       "[5 rows x 2309 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = np.zeros((len(clean_tokenized), len(unique_tokens)))\n",
    "counts = pd.DataFrame(zeros, columns=unique_tokens)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Token Occurrences\n",
    "\n",
    "Now that we have a matrix where all values are `0`, we need to fill in the correct counts for each cell. This involves going through each set of tokens, and incrementing the column counters in the appropriate row.\n",
    "\n",
    "![](img/5.png)\n",
    "\n",
    "When we're finished, we'll have a row vector for each headline that tells us how many times each token occured in that headline.<br>\n",
    "\n",
    "To accomplish this, we can loop through each list of tokens in `clean_tokenized`, then loop through each token in the list and increment the proper cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loop through each list of tokens in `clean_tokenized`.\n",
    "  * You should use the `enumerate()` function when writing the loop to get an index along with the list of tokens.\n",
    "* Loop through each token in the list of tokens.\n",
    "* Check whether the token is in `unique_tokens`. If not, it isn't a column in the dataframe, and you should ignore it.\n",
    "* Increment the appropriate cell by indexing the row of `counts`, and finding the right column for the token. Add 1 to the cell to indicate that you found the token once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clean_item in enumerate(clean_tokenized):\n",
    "    for token in clean_item:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[i][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>as</th>\n",
       "      <th>you</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>split</th>\n",
       "      <th>good</th>\n",
       "      <th>how</th>\n",
       "      <th>...</th>\n",
       "      <th>frameworks</th>\n",
       "      <th>animated</th>\n",
       "      <th>walks</th>\n",
       "      <th>auctions</th>\n",
       "      <th>clouds</th>\n",
       "      <th>hammer</th>\n",
       "      <th>autonomous</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>crowdsourcing</th>\n",
       "      <th>disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  for   as  you   is  the  split  good  how    ...     frameworks  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  1.0    0.0   0.0  0.0    ...            0.0   \n",
       "1  2.0  1.0  1.0  0.0  0.0  0.0  0.0    1.0   0.0  0.0    ...            0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "3  0.0  2.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0  0.0    ...            0.0   \n",
       "\n",
       "   animated  walks  auctions  clouds  hammer  autonomous  vehicle  \\\n",
       "0       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "1       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "2       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "3       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "4       0.0    0.0       0.0     0.0     0.0         0.0      0.0   \n",
       "\n",
       "   crowdsourcing  disaster  \n",
       "0            0.0       0.0  \n",
       "1            0.0       0.0  \n",
       "2            0.0       0.0  \n",
       "3            0.0       0.0  \n",
       "4            0.0       0.0  \n",
       "\n",
       "[5 rows x 2309 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Columns to Increase Accuracy\n",
    "\n",
    "We have over `2000` columns in our matrix. This can make it very hard for a linear regression model to make good predictions. Too many columns will cause the model to fit to noise instead of the signal in the data.<br>\n",
    "\n",
    "There are two kinds of features that will reduce prediction accuracy. Features that occur only a few times will cause overfitting, because the model doesn't have enough information to accurately decide whether they're important. These features will probably correlate differently with upvotes in the test set and the training set.<br>\n",
    "\n",
    "Features that occur too many times can also cause issues. These are words like and and to, which occur in nearly every headline. These words don't add any information, because they don't necessarily correlate with upvotes. These types of words are sometimes called stopwords.<br>\n",
    "\n",
    "To reduce the number of features and enable the linear regression model to make better predictions, we'll remove any words that occur fewer than `5` times or more than `100` times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate a vector that contains the sum of each column in `counts`. This data will indicate how many times each word occurs in the headlines. You can use the `sum()` method on pandas dataframes to accomplish this. Assign this vector to `word_counts`.\n",
    "* Use the vector to filter `counts` to remove any columns that occur less than `5` times, or more than `100` times. You can use the `loc` method on dataframes to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = counts.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.loc[:, (word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>as</th>\n",
       "      <th>you</th>\n",
       "      <th>good</th>\n",
       "      <th>what</th>\n",
       "      <th>de</th>\n",
       "      <th>amazon</th>\n",
       "      <th>cloud</th>\n",
       "      <th>at</th>\n",
       "      <th>google</th>\n",
       "      <th>back</th>\n",
       "      <th>...</th>\n",
       "      <th>uk</th>\n",
       "      <th>preview</th>\n",
       "      <th>compiler</th>\n",
       "      <th>manager</th>\n",
       "      <th>sharing</th>\n",
       "      <th>sale</th>\n",
       "      <th>competition</th>\n",
       "      <th>diet</th>\n",
       "      <th>reasons</th>\n",
       "      <th>nike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 661 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    as  you  good  what   de  amazon  cloud   at  google  back  ...    uk  \\\n",
       "0  0.0  0.0   0.0   0.0  0.0     0.0    0.0  0.0     0.0   0.0  ...   0.0   \n",
       "1  0.0  0.0   0.0   0.0  0.0     0.0    0.0  0.0     0.0   0.0  ...   0.0   \n",
       "2  0.0  0.0   0.0   0.0  0.0     0.0    0.0  0.0     0.0   0.0  ...   0.0   \n",
       "3  0.0  0.0   0.0   0.0  0.0     0.0    0.0  0.0     0.0   0.0  ...   0.0   \n",
       "4  0.0  0.0   0.0   0.0  1.0     0.0    0.0  0.0     0.0   0.0  ...   0.0   \n",
       "\n",
       "   preview  compiler  manager  sharing  sale  competition  diet  reasons  nike  \n",
       "0      0.0       0.0      0.0      0.0   0.0          0.0   0.0      0.0   0.0  \n",
       "1      0.0       0.0      0.0      0.0   0.0          0.0   0.0      0.0   0.0  \n",
       "2      0.0       0.0      0.0      0.0   0.0          0.0   0.0      0.0   0.0  \n",
       "3      0.0       0.0      0.0      0.0   0.0          0.0   0.0      0.0   0.0  \n",
       "4      0.0       0.0      0.0      0.0   0.0          0.0   0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 661 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data Into Train and Test Sets\n",
    "\n",
    "Now we'll need to split the data into two sets so that we can evaluate our algorithm effectively. We'll train our algorithm on a training set, then test its performance on a test set.<br>\n",
    "\n",
    "The [train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function from scikit-learn will help us accomplish this.<br>\n",
    "\n",
    "We'll pass in `.2` for the `test_size` parameter to randomly select `20%` of the rows for our test set, and `80%` for our training set.<br>\n",
    "\n",
    "`X_train` and `X_test` contain the predictors, and `y_train` and `y_test` contain the value we're trying to predict (upvotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions With `fit()`\n",
    "\n",
    "Now that we have a training set and a test set, let's train a model and make test predictions. We'll use a linear regression algorithm from scikit-learn, which you can read more about in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).<br>\n",
    "\n",
    "First we'll initialize the model using the `LinearRegression` class. Then, we'll use the `fit()` method on the model to train with `X_train` and `y_train`. Finally, we'll make predictions with `X_test`.<br>\n",
    "\n",
    "When we make predictions with a linear regression model, the model assigns coefficients to each column. Essentially, the model is determining which words correlate with more upvotes, and which with less. By finding these correlations, the model will be able to predict which headlines will be highly upvoted in the future. While the algorithm won't have a high level of understanding of the text, linear regression can generate surprisingly good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train `clf` using the `fit()` method.\n",
    "* Use the `predict()` method on `clf` to make predictions on `X_test`. Assign the result to `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Prediction Error\n",
    "\n",
    "Now that we have predictions, we can calculate our prediction error. We'll need to select an error metric first, though. We'll use [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE), which is a common error metric.<br>\n",
    "\n",
    "Here's the formula for MSE:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{Y_{i}} - Y_{i})^{2}$$\n",
    "\n",
    "With MSE, we subtract the predictions from the actual values, square the results, and find the mean. Because the errors are squared, MSE penalizes errors further away from the actual value more than those close to the actual value. We want to use MSE because we'd like all of our predictions to be relatively close to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the mean squared error associated with our predictions.\n",
    "  * Subtract `y_test` from `predictions`.\n",
    "  * Square each of the differences.\n",
    "  * Add all of the squared differences together, and divide by the number of differences to get the mean.\n",
    "  * Assign the result to `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((predictions - y_test)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2652.6082512522867"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Our MSE is `2181`, which is a fairly large value. There's no hard and fast rule about what a \"good\" error rate is, because it depends on the problem we're solving and our error tolerance.<br>\n",
    "\n",
    "In this case, the mean number of upvotes is `10`, and the standard deviation is `39.5`. If we take the square root of our MSE to calculate error in terms of upvotes, we get `46.7`. This means that our average error is `46.7` upvotes away from the true value. \n",
    "### This is higher than the standard deviation, so our predictions are often far off-base.<br>\n",
    "\n",
    "We can take several steps to reduce the error and explore natural language processing further. Here are some ideas for your next steps:\n",
    "\n",
    "* Use the entire data set. While we used samples in this mission, you could download the entire data set from [this GitHub repository](https://github.com/arnauddri/hn). This approach will reduce the error rate dramatically. There are many features in natural language processing. Using more data will ensure that the model will find more occurrences of the same features in the test and training sets, which will help the model make better predictions.\n",
    "* Add \"meta\" features like headline length and average word length.\n",
    "* Use a random forest, or another more powerful machine learning technique.\n",
    "* Explore different thresholds for removing extraneous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
