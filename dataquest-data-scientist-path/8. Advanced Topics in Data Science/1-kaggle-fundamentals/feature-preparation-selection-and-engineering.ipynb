{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparation, Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last mission, we made our first submission to Kaggle, getting an accuracy score of 75.6%. While this is a good start, there is definitely room for improvement. There are two main areas we can focus on to boost the accuracy of our predictions:\n",
    "\n",
    "* Improving the features we train our model on\n",
    "* Improving the model itself\n",
    "\n",
    "In this mission, we're going to focus working with the features used in our model.<br>\n",
    "\n",
    "We'll start by looking at **feature selection**. Feature selection is important because it helps to exclude features which are not good predictors, or features that are closely related to each other. Both of these will cause our model to be less accurate, particularly on previously unseen data.<br>\n",
    "\n",
    "The diagram below illustrates this. The red dots represent the data we are trying to predict, and each of the blue lines represents a different model.\n",
    "\n",
    "![https://s3.amazonaws.com/dq-content/186/overfitting.svg](https://s3.amazonaws.com/dq-content/186/overfitting.svg)\n",
    "\n",
    "The model on the left is **overfitting**, which means the model represents the training data too closely, and is unlikely to predict well on unseen data, like the holdout data for our Kaggle competition.<br>\n",
    "\n",
    "The model on the right is **well-fit**. It captures the underlying pattern in the data without the detailed noise found just in the training set. A well fit model is likely to make accurate predictions on previously unseen data. The key to creating a well-fit model is to select the right balance of features, and to create new features to train your model.<br>\n",
    "\n",
    "In the previous mission, we trained our model using data about the age, sex and class of the passengers on the Titanic. Let's start by using the functions we created in that mission to add the columns we had at the end of the first mission.<br>\n",
    "\n",
    "Remember that any modifications we make to our training data (`train.csv`) we also have to make to our holdout data (`test.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `process_age()` function:\n",
    "* To convert the `Age` column in `train`, assigning the result to `train`.\n",
    "* To convert the `Age` column in `holdout`, assigning the result to `holdout`.\n",
    "* Create a for loop which iterates over the column names `\"Age_categories\"`, `\"Pclass\"`, and `\"Sex\"`. In each iteration:\n",
    "  * Use the `create_dummies()` function to process the `train` dataframe for the given column, assigning the result to `train`.\n",
    "  * Use the `create_dummies()` function to process the `holdout` dataframe for the given column, assigning the result to `holdout`.\n",
    "* Use the `print()` function to display the columns in `train` using `train.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "holdout = pd.read_csv('data/test.csv')\n",
    "\n",
    "def process_age(df):\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
    "    cut_points = [-1,0,5,12,18,35,60,100]\n",
    "    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n",
    "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "    return df\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Age_categories',\n",
      "       'Age_categories_Missing', 'Age_categories_Infant',\n",
      "       'Age_categories_Child', 'Age_categories_Teenager',\n",
      "       'Age_categories_Young Adult', 'Age_categories_Adult',\n",
      "       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
      "       'Sex_female', 'Sex_male'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train = process_age(train)\n",
    "holdout = process_age(holdout)\n",
    "\n",
    "for col in ['Age_categories', 'Pclass', 'Sex']:\n",
    "    \n",
    "    train = create_dummies(train, col)\n",
    "    holdout = create_dummies(holdout, col)\n",
    "    \n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing More Features\n",
    "\n",
    "Our model in the previous mission was based on three columns from the original data: `Age`, `Sex`, and `Pclass`. As you saw when you printed the column names in the previous screen, there are a number of other columns that we haven't yet used. To make it easier to reference, the output from the previous screen is copied below:\n",
    "\n",
    "```python\n",
    "Index(['PassengerId', 'Survived', 'Pclass', 'Name',\n",
    "       'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
    "       'Fare', 'Cabin', 'Embarked', 'Age_categories',\n",
    "       'Age_categories_Missing',\n",
    "       'Age_categories_Infant',\n",
    "       'Age_categories_Child',\n",
    "       'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult',\n",
    "       'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1',\n",
    "       'Pclass_2', 'Pclass_3','Sex_female',\n",
    "       'Sex_male'], dtype='object')\n",
    "```\n",
    "\n",
    "The last nine rows of the output are dummy columns we created, but in the first three rows we can see there are a number of features we haven't yet utilized. We can ignore `PassengerId`, since this is just a column Kaggle have added to identify each passenger and calculate scores. We can also ignore `Survived`, as this is what we're predicting, as well as the three columns we've already used.<br>\n",
    "\n",
    "Here is a list of the remaining columns (with a brief description), followed by 10 randomly selected passengers from and their data from those columns, so we can refamiliarize ourselves with the data.\n",
    "\n",
    "* `SibSp` - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "* `Parch` - The number of parents or children the passenger had aboard the Titanic\n",
    "* `Ticket` - The passenger's ticket number\n",
    "* `Fare` - The fair the passenger paid\n",
    "* `Cabin` - The passengers cabin number\n",
    "* `Embarked` - The port where the passenger embarked (*C=Cherbourg, Q=Queenstown, S=Southampton*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      1      0   \n",
       "2                             Heikkinen, Miss. Laina      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      1      0   \n",
       "4                           Allen, Mr. William Henry      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, both the `Name` and `Ticket` columns look to be unique to each passenger. We will come back to these columns later, but for now we'll focus on the other columns.<br>\n",
    "\n",
    "We can use the [`Dataframe.describe()` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) to give us some more information on the values within each remaining column.\n",
    "\n",
    "```python\n",
    ">>> columns = ['SibSp','Parch','Fare','Cabin','Embarked']\n",
    ">>> train[columns].describe(include='all',percentiles=[])\n",
    "                 SibSp       Parch        Fare Cabin Embarked\n",
    "    count   891.000000  891.000000  891.000000   204      889\n",
    "    unique         NaN         NaN         NaN   147        3\n",
    "    top            NaN         NaN         NaN    G6        S\n",
    "    freq           NaN         NaN         NaN     4      644\n",
    "    mean      0.523008    0.381594   32.204208   NaN      NaN\n",
    "    std       1.102743    0.806057   49.693429   NaN      NaN\n",
    "    min       0.000000    0.000000    0.000000   NaN      NaN\n",
    "    50%       0.000000    0.000000   14.454200   NaN      NaN\n",
    "    max       8.000000    6.000000  512.329200   NaN      NaN\n",
    "```\n",
    "\n",
    "Of these, `SibSp`, `Parch` and `Fare` look to be standard numeric columns with no missing values. `Cabin` has values for only 204 of the 891 rows, and even then most of the values are unique, so for now we will leave this column also. `Embarked` looks to be a standard categorical column with 3 unique values, much like `PClass` was, except that there are two missing values. We can easily fill these two missing values with the most common value, \"S\" which occurs 644 times.<br>\n",
    "\n",
    "Looking at our numeric columns, we can see a big difference between the range of each. `SibSp` has values between 0-8, `Parch` between 0-6, and `Fare` is on a dramatically different scale, with values ranging from 0-512. In order to make sure these values are equally weighted within our model, we'll need to **rescale** the data.<br>\n",
    "\n",
    "Rescaling simply stretches or shrinks the data as needed to be on the same scale, in our case between 0 and 1.\n",
    "\n",
    "![https://s3.amazonaws.com/dq-content/186/rescaling.svg](https://s3.amazonaws.com/dq-content/186/rescaling.svg)\n",
    "\n",
    "In the diagram above, the three columns have different minimum and maximum values before rescaling.<br>\n",
    "\n",
    "After rescaling, the values in each feature has been compressed or stretched so that they are all on the same scale - they have the same minimum and maximum, and the relationship between each point is still the same relative other points in that feature. You can now easily see that the data represented in each column is identical.<br>\n",
    "\n",
    "Within scikit-learn, the [`preprocessing.minmax_scale()` function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) allows us to quickly and easily rescale our data:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "columns = [\"column one\", \"column two\"]\n",
    "data[columns] = min_max_scale(data[columns])\n",
    "```\n",
    "\n",
    "Let's process the `Embarked`, `SibSp`, `Parch` and `Fare` columns in both our `train` and `holdout` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For both the `train` and `holdout` dataframes:\n",
    "  * Use the [`Series.fillna()` method](https://www.dataquest.io/m/186/feature-preparation%2C-selection-and-engineering/2/pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html) to replace any missing values in the `Embarked` column with `\"S\"`\n",
    "  * Use our `create_dummies()` function to create dummy columns for the `Embarked` column.\n",
    "  * Use `minmax_scale()` to rescale the `SibSp`, `Parch`, and `Fare` columns, assigning the results back to new columns `SibSp_scaled`, `Parch_scaled`. and `Fare_scaled` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "# The holdout set has a missing value in the Fare column which\n",
    "# we'll fill with the mean.\n",
    "holdout[\"Fare\"] = holdout[\"Fare\"].fillna(train[\"Fare\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train['Embarked'] = train['Embarked'].fillna(\"S\")\n",
    "holdout['Embarked'] = holdout['Embarked'].fillna(\"S\")\n",
    "\n",
    "train = create_dummies(train, 'Embarked')\n",
    "holdout = create_dummies(holdout, 'Embarked')\n",
    "\n",
    "for df in [train, holdout]:\n",
    "    for newcol, toscale in zip(['SibSp_scaled', 'Parch_scaled', 'Fare_scaled'],\n",
    "                               ['SibSp', 'Parch', 'Fare']):\n",
    "        df[newcol] = minmax_scale(df[toscale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Most Relevant Features\n",
    "\n",
    "In order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.<br>\n",
    "\n",
    "The scikit-learn [LogisticRegression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) has an attribute in which coefficients are stored after the model is fit, `LogisticRegression.coef_`. We first need to train our model, after which we can access this attribute.\n",
    "\n",
    "```python\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_X,train_y)\n",
    "coefficients = lr.coef_\n",
    "```\n",
    "\n",
    "The `coef()` method returns a NumPy array of coefficients, in the same order as the features that were used to fit the model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:\n",
    "\n",
    "```python\n",
    "feature_importance = pd.Series(coefficients[0],\n",
    "                               index=train_X.columns)\n",
    "```\n",
    "\n",
    "We'll now fit a model and plot the coefficients for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instantiate a `LogisticRegression()` object.\n",
    "* Fit the `LogisticRegression` object using the `columns` from the list columns from the `train` dataframe and the `target` column `Survived`.\n",
    "* Use the `coef_` attribute to retrieve the coefficients of the features, and assign the results to `coefficients`.\n",
    "* Create a series object using `coefficients`, with the feature column names as the index and assign it to `feature_importance`.\n",
    "* Use the [`Series.plot.barh()` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.plot.barh.html) to plot the `feature_importance` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "columns = ['Age_categories_Missing', 'Age_categories_Infant',\n",
    "       'Age_categories_Child', 'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult', 'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
    "       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34723567,  1.86060592,  0.34493188, -0.18273418, -0.0110893 ,\n",
       "        -0.52620202, -0.90049959,  1.04515623,  0.13729476, -0.94467395,\n",
       "         1.45610934, -1.2183323 ,  0.25010253,  0.24374319, -0.25606868,\n",
       "        -1.74775712, -0.77650208,  0.54308487]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train[columns], train['Survived'])\n",
    "coefficients = lr.coef_\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAD8CAYAAABqxe1QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1082c30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = pd.Series(coefficients[0], index=columns)\n",
    "feature_importance.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model using relevant features\n",
    "\n",
    "The plot we generated in the last screen showed a range of both positive and negative values. Whether the value is positive or negative isn't as important in this case, relative to the magnitude of the value. If you think about it, this makes sense. A feature that indicates strongly whether a passenger died is just as useful as a feature that indicates strongly that a passenger survived, given they are mutually exclusive outcomes.<br>\n",
    "\n",
    "To make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in order of size:\n",
    "\n",
    "```python\n",
    "ordered_feature_importance = feature_importance.abs().sort_values()\n",
    "ordered_feature_importance.plot.barh()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![https://s3.amazonaws.com/dq-content/186/feature_importance.png](https://s3.amazonaws.com/dq-content/186/feature_importance.png)\n",
    "\n",
    "We'll train a new model with the top 8 scores and check our accuracy using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instantiate a `LogisticRegression()` object.\n",
    "* Use the [`model_selection.cross_val_score()` function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) and assign the returned object to `scores`, using:\n",
    "  * The columns specified in `columns` and all rows from the `train` dataframe.\n",
    "  * A `cv` parameter of 10.\n",
    "* Calculate the mean of the cross validation scores and assign the results to `accuracy`.\n",
    "* Use the `print()` function to display the variable `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', \n",
    "           'Sex_male', 'Pclass_1', 'Pclass_3', \n",
    "           'Age_categories_Senior', 'Parch_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814801952105\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "scores = cross_val_score(lr, train[columns], train['Survived'], cv=10)\n",
    "accuracy = scores.mean()\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting our Improved Model to Kaggle\n",
    "\n",
    "The cross validation score of 81.48% is marginally higher than the cross validation score for the model we created in the previous mission, which had a score of 80.2%.<br>\n",
    "\n",
    "Hopefully, this improvement will translate to previously unseen data. Let's train a model using the columns from the previous step, make some predictions on the holdout data and submit it to Kaggle for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instantiate a `LogisticRegression()` object and fit it using `all_X` and `all_y`.\n",
    "* Use the `predict()` method to make predictions using the same columns in the `holdout` dataframe, and assign the result to `holdout_predictions`\n",
    "* Create a dataframe `submission` with two columns:\n",
    "  * `PassengerId`, with the values from the `PassengerId` column of the `holdout` dataframe.\n",
    "  * `Survived`, with the values from `holdout_predictions`.\n",
    "* Use the `DataFrame.to_csv` method to save the `submission` dataframe to the filename `submission_1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n",
    "       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n",
    "\n",
    "all_X = train[columns]\n",
    "all_y = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(all_X, all_y)\n",
    "holdout_predictions = lr.predict(holdout[columns])\n",
    "\n",
    "submission_df = {'PassengerId':holdout['PassengerId'],\n",
    "                'Survived':holdout_predictions}\n",
    "\n",
    "pd.DataFrame(submission_df).to_csv('submission_1.csv',\n",
    "                                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering a New Feature Using Binning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
